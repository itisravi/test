<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - btrfstest.info - include/linux/pagemap.h</title>
  <link rel="stylesheet" type="text/css" href="../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../index.html">top level</a> - <a href="index.html">include/linux</a> - pagemap.h<span style="font-size: 80%;"> (source / <a href="pagemap.h.func.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">btrfstest.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">23</td>
            <td class="headerCovTableEntry">29</td>
            <td class="headerCovTableEntryMed">79.3 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2014-11-28</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">3</td>
            <td class="headerCovTableEntry">5</td>
            <td class="headerCovTableEntryLo">60.0 %</td>
          </tr>
          <tr><td><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : #ifndef _LINUX_PAGEMAP_H</a>
<span class="lineNum">       2 </span>            : #define _LINUX_PAGEMAP_H
<span class="lineNum">       3 </span>            : 
<span class="lineNum">       4 </span>            : /*
<span class="lineNum">       5 </span>            :  * Copyright 1995 Linus Torvalds
<span class="lineNum">       6 </span>            :  */
<span class="lineNum">       7 </span>            : #include &lt;linux/mm.h&gt;
<span class="lineNum">       8 </span>            : #include &lt;linux/fs.h&gt;
<span class="lineNum">       9 </span>            : #include &lt;linux/list.h&gt;
<span class="lineNum">      10 </span>            : #include &lt;linux/highmem.h&gt;
<span class="lineNum">      11 </span>            : #include &lt;linux/compiler.h&gt;
<span class="lineNum">      12 </span>            : #include &lt;asm/uaccess.h&gt;
<span class="lineNum">      13 </span>            : #include &lt;linux/gfp.h&gt;
<span class="lineNum">      14 </span>            : #include &lt;linux/bitops.h&gt;
<span class="lineNum">      15 </span>            : #include &lt;linux/hardirq.h&gt; /* for in_interrupt() */
<span class="lineNum">      16 </span>            : #include &lt;linux/hugetlb_inline.h&gt;
<span class="lineNum">      17 </span>            : 
<span class="lineNum">      18 </span>            : /*
<span class="lineNum">      19 </span>            :  * Bits in mapping-&gt;flags.  The lower __GFP_BITS_SHIFT bits are the page
<span class="lineNum">      20 </span>            :  * allocation mode flags.
<span class="lineNum">      21 </span>            :  */
<span class="lineNum">      22 </span>            : enum mapping_flags {
<span class="lineNum">      23 </span>            :         AS_EIO          = __GFP_BITS_SHIFT + 0, /* IO error on async write */
<span class="lineNum">      24 </span>            :         AS_ENOSPC       = __GFP_BITS_SHIFT + 1, /* ENOSPC on async write */
<span class="lineNum">      25 </span>            :         AS_MM_ALL_LOCKS = __GFP_BITS_SHIFT + 2, /* under mm_take_all_locks() */
<span class="lineNum">      26 </span>            :         AS_UNEVICTABLE  = __GFP_BITS_SHIFT + 3, /* e.g., ramdisk, SHM_LOCK */
<span class="lineNum">      27 </span>            :         AS_BALLOON_MAP  = __GFP_BITS_SHIFT + 4, /* balloon page special map */
<span class="lineNum">      28 </span>            :         AS_EXITING      = __GFP_BITS_SHIFT + 5, /* final truncate in progress */
<a name="29"><span class="lineNum">      29 </span>            : };</a>
<span class="lineNum">      30 </span>            : 
<span class="lineNum">      31 </span><span class="lineNoCov">          0 : static inline void mapping_set_error(struct address_space *mapping, int error)</span>
<span class="lineNum">      32 </span>            : {
<span class="lineNum">      33 </span><span class="lineNoCov">          0 :         if (unlikely(error)) {</span>
<span class="lineNum">      34 </span><span class="lineNoCov">          0 :                 if (error == -ENOSPC)</span>
<span class="lineNum">      35 </span>            :                         set_bit(AS_ENOSPC, &amp;mapping-&gt;flags);
<span class="lineNum">      36 </span>            :                 else
<span class="lineNum">      37 </span>            :                         set_bit(AS_EIO, &amp;mapping-&gt;flags);
<span class="lineNum">      38 </span>            :         }
<span class="lineNum">      39 </span><span class="lineNoCov">          0 : }</span>
<span class="lineNum">      40 </span>            : 
<span class="lineNum">      41 </span>            : static inline void mapping_set_unevictable(struct address_space *mapping)
<span class="lineNum">      42 </span>            : {
<span class="lineNum">      43 </span>            :         set_bit(AS_UNEVICTABLE, &amp;mapping-&gt;flags);
<span class="lineNum">      44 </span>            : }
<span class="lineNum">      45 </span>            : 
<span class="lineNum">      46 </span>            : static inline void mapping_clear_unevictable(struct address_space *mapping)
<span class="lineNum">      47 </span>            : {
<span class="lineNum">      48 </span>            :         clear_bit(AS_UNEVICTABLE, &amp;mapping-&gt;flags);
<span class="lineNum">      49 </span>            : }
<span class="lineNum">      50 </span>            : 
<span class="lineNum">      51 </span>            : static inline int mapping_unevictable(struct address_space *mapping)
<span class="lineNum">      52 </span>            : {
<span class="lineNum">      53 </span>            :         if (mapping)
<span class="lineNum">      54 </span>            :                 return test_bit(AS_UNEVICTABLE, &amp;mapping-&gt;flags);
<span class="lineNum">      55 </span>            :         return !!mapping;
<span class="lineNum">      56 </span>            : }
<span class="lineNum">      57 </span>            : 
<span class="lineNum">      58 </span>            : static inline void mapping_set_balloon(struct address_space *mapping)
<span class="lineNum">      59 </span>            : {
<span class="lineNum">      60 </span>            :         set_bit(AS_BALLOON_MAP, &amp;mapping-&gt;flags);
<span class="lineNum">      61 </span>            : }
<span class="lineNum">      62 </span>            : 
<span class="lineNum">      63 </span>            : static inline void mapping_clear_balloon(struct address_space *mapping)
<span class="lineNum">      64 </span>            : {
<span class="lineNum">      65 </span>            :         clear_bit(AS_BALLOON_MAP, &amp;mapping-&gt;flags);
<span class="lineNum">      66 </span>            : }
<span class="lineNum">      67 </span>            : 
<span class="lineNum">      68 </span>            : static inline int mapping_balloon(struct address_space *mapping)
<span class="lineNum">      69 </span>            : {
<span class="lineNum">      70 </span>            :         return mapping &amp;&amp; test_bit(AS_BALLOON_MAP, &amp;mapping-&gt;flags);
<span class="lineNum">      71 </span>            : }
<span class="lineNum">      72 </span>            : 
<span class="lineNum">      73 </span>            : static inline void mapping_set_exiting(struct address_space *mapping)
<span class="lineNum">      74 </span>            : {
<span class="lineNum">      75 </span>            :         set_bit(AS_EXITING, &amp;mapping-&gt;flags);
<span class="lineNum">      76 </span>            : }
<span class="lineNum">      77 </span>            : 
<span class="lineNum">      78 </span>            : static inline int mapping_exiting(struct address_space *mapping)
<span class="lineNum">      79 </span>            : {
<span class="lineNum">      80 </span>            :         return test_bit(AS_EXITING, &amp;mapping-&gt;flags);
<span class="lineNum">      81 </span>            : }
<span class="lineNum">      82 </span>            : 
<a name="83"><span class="lineNum">      83 </span>            : static inline gfp_t mapping_gfp_mask(struct address_space * mapping)</a>
<span class="lineNum">      84 </span>            : {
<span class="lineNum">      85 </span><span class="lineCov">     130439 :         return (__force gfp_t)mapping-&gt;flags &amp; __GFP_BITS_MASK;</span>
<span class="lineNum">      86 </span>            : }
<span class="lineNum">      87 </span>            : 
<span class="lineNum">      88 </span>            : /*
<span class="lineNum">      89 </span>            :  * This is non-atomic.  Only to be used before the mapping is activated.
<span class="lineNum">      90 </span>            :  * Probably needs a barrier...
<span class="lineNum">      91 </span>            :  */
<span class="lineNum">      92 </span>            : static inline void mapping_set_gfp_mask(struct address_space *m, gfp_t mask)
<span class="lineNum">      93 </span>            : {
<span class="lineNum">      94 </span><span class="lineCov">        851 :         m-&gt;flags = (m-&gt;flags &amp; ~(__force unsigned long)__GFP_BITS_MASK) |</span>
<span class="lineNum">      95 </span><span class="lineCov">        315 :                                 (__force unsigned long)mask;</span>
<span class="lineNum">      96 </span>            : }
<span class="lineNum">      97 </span>            : 
<span class="lineNum">      98 </span>            : /*
<span class="lineNum">      99 </span>            :  * The page cache can done in larger chunks than
<span class="lineNum">     100 </span>            :  * one page, because it allows for more efficient
<span class="lineNum">     101 </span>            :  * throughput (it can then be mapped into user
<span class="lineNum">     102 </span>            :  * space in smaller chunks for same flexibility).
<span class="lineNum">     103 </span>            :  *
<span class="lineNum">     104 </span>            :  * Or rather, it _will_ be done in larger chunks.
<span class="lineNum">     105 </span>            :  */
<span class="lineNum">     106 </span>            : #define PAGE_CACHE_SHIFT        PAGE_SHIFT
<span class="lineNum">     107 </span>            : #define PAGE_CACHE_SIZE         PAGE_SIZE
<span class="lineNum">     108 </span>            : #define PAGE_CACHE_MASK         PAGE_MASK
<span class="lineNum">     109 </span>            : #define PAGE_CACHE_ALIGN(addr)  (((addr)+PAGE_CACHE_SIZE-1)&amp;PAGE_CACHE_MASK)
<span class="lineNum">     110 </span>            : 
<span class="lineNum">     111 </span>            : #define page_cache_get(page)            get_page(page)
<span class="lineNum">     112 </span>            : #define page_cache_release(page)        put_page(page)
<span class="lineNum">     113 </span>            : void release_pages(struct page **pages, int nr, bool cold);
<span class="lineNum">     114 </span>            : 
<span class="lineNum">     115 </span>            : /*
<span class="lineNum">     116 </span>            :  * speculatively take a reference to a page.
<span class="lineNum">     117 </span>            :  * If the page is free (_count == 0), then _count is untouched, and 0
<span class="lineNum">     118 </span>            :  * is returned. Otherwise, _count is incremented by 1 and 1 is returned.
<span class="lineNum">     119 </span>            :  *
<span class="lineNum">     120 </span>            :  * This function must be called inside the same rcu_read_lock() section as has
<span class="lineNum">     121 </span>            :  * been used to lookup the page in the pagecache radix-tree (or page table):
<span class="lineNum">     122 </span>            :  * this allows allocators to use a synchronize_rcu() to stabilize _count.
<span class="lineNum">     123 </span>            :  *
<span class="lineNum">     124 </span>            :  * Unless an RCU grace period has passed, the count of all pages coming out
<span class="lineNum">     125 </span>            :  * of the allocator must be considered unstable. page_count may return higher
<span class="lineNum">     126 </span>            :  * than expected, and put_page must be able to do the right thing when the
<span class="lineNum">     127 </span>            :  * page has been finished with, no matter what it is subsequently allocated
<span class="lineNum">     128 </span>            :  * for (because put_page is what is used here to drop an invalid speculative
<span class="lineNum">     129 </span>            :  * reference).
<span class="lineNum">     130 </span>            :  *
<span class="lineNum">     131 </span>            :  * This is the interesting part of the lockless pagecache (and lockless
<span class="lineNum">     132 </span>            :  * get_user_pages) locking protocol, where the lookup-side (eg. find_get_page)
<span class="lineNum">     133 </span>            :  * has the following pattern:
<span class="lineNum">     134 </span>            :  * 1. find page in radix tree
<span class="lineNum">     135 </span>            :  * 2. conditionally increment refcount
<span class="lineNum">     136 </span>            :  * 3. check the page is still in pagecache (if no, goto 1)
<span class="lineNum">     137 </span>            :  *
<span class="lineNum">     138 </span>            :  * Remove-side that cares about stability of _count (eg. reclaim) has the
<span class="lineNum">     139 </span>            :  * following (with tree_lock held for write):
<span class="lineNum">     140 </span>            :  * A. atomically check refcount is correct and set it to 0 (atomic_cmpxchg)
<span class="lineNum">     141 </span>            :  * B. remove page from pagecache
<span class="lineNum">     142 </span>            :  * C. free the page
<span class="lineNum">     143 </span>            :  *
<span class="lineNum">     144 </span>            :  * There are 2 critical interleavings that matter:
<span class="lineNum">     145 </span>            :  * - 2 runs before A: in this case, A sees elevated refcount and bails out
<span class="lineNum">     146 </span>            :  * - A runs before 2: in this case, 2 sees zero refcount and retries;
<span class="lineNum">     147 </span>            :  *   subsequently, B will complete and 1 will find no page, causing the
<span class="lineNum">     148 </span>            :  *   lookup to return NULL.
<span class="lineNum">     149 </span>            :  *
<span class="lineNum">     150 </span>            :  * It is possible that between 1 and 2, the page is removed then the exact same
<span class="lineNum">     151 </span>            :  * page is inserted into the same position in pagecache. That's OK: the
<span class="lineNum">     152 </span>            :  * old find_get_page using tree_lock could equally have run before or after
<span class="lineNum">     153 </span>            :  * such a re-insertion, depending on order that locks are granted.
<span class="lineNum">     154 </span>            :  *
<span class="lineNum">     155 </span>            :  * Lookups racing against pagecache insertion isn't a big problem: either 1
<span class="lineNum">     156 </span>            :  * will find the page or it will not. Likewise, the old find_get_page could run
<a name="157"><span class="lineNum">     157 </span>            :  * either before the insertion or afterwards, depending on timing.</a>
<span class="lineNum">     158 </span>            :  */
<span class="lineNum">     159 </span><span class="lineCov">         34 : static inline int page_cache_get_speculative(struct page *page)</span>
<span class="lineNum">     160 </span>            : {
<span class="lineNum">     161 </span><span class="lineCov">         34 :         VM_BUG_ON(in_interrupt());</span>
<span class="lineNum">     162 </span>            : 
<span class="lineNum">     163 </span>            : #ifdef CONFIG_TINY_RCU
<span class="lineNum">     164 </span>            : # ifdef CONFIG_PREEMPT_COUNT
<span class="lineNum">     165 </span>            :         VM_BUG_ON(!in_atomic());
<span class="lineNum">     166 </span>            : # endif
<span class="lineNum">     167 </span>            :         /*
<span class="lineNum">     168 </span>            :          * Preempt must be disabled here - we rely on rcu_read_lock doing
<span class="lineNum">     169 </span>            :          * this for us.
<span class="lineNum">     170 </span>            :          *
<span class="lineNum">     171 </span>            :          * Pagecache won't be truncated from interrupt context, so if we have
<span class="lineNum">     172 </span>            :          * found a page in the radix tree here, we have pinned its refcount by
<span class="lineNum">     173 </span>            :          * disabling preempt, and hence no need for the &quot;speculative get&quot; that
<span class="lineNum">     174 </span>            :          * SMP requires.
<span class="lineNum">     175 </span>            :          */
<span class="lineNum">     176 </span>            :         VM_BUG_ON_PAGE(page_count(page) == 0, page);
<span class="lineNum">     177 </span>            :         atomic_inc(&amp;page-&gt;_count);
<span class="lineNum">     178 </span>            : 
<span class="lineNum">     179 </span>            : #else
<span class="lineNum">     180 </span><span class="lineCov">         34 :         if (unlikely(!get_page_unless_zero(page))) {</span>
<span class="lineNum">     181 </span>            :                 /*
<span class="lineNum">     182 </span>            :                  * Either the page has been freed, or will be freed.
<span class="lineNum">     183 </span>            :                  * In either case, retry here and the caller should
<span class="lineNum">     184 </span>            :                  * do the right thing (see comments above).
<span class="lineNum">     185 </span>            :                  */
<span class="lineNum">     186 </span>            :                 return 0;
<span class="lineNum">     187 </span>            :         }
<span class="lineNum">     188 </span>            : #endif
<span class="lineNum">     189 </span><span class="lineCov">         34 :         VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="lineNum">     190 </span>            : 
<span class="lineNum">     191 </span>            :         return 1;
<span class="lineNum">     192 </span>            : }
<span class="lineNum">     193 </span>            : 
<span class="lineNum">     194 </span>            : /*
<span class="lineNum">     195 </span>            :  * Same as above, but add instead of inc (could just be merged)
<span class="lineNum">     196 </span>            :  */
<span class="lineNum">     197 </span>            : static inline int page_cache_add_speculative(struct page *page, int count)
<span class="lineNum">     198 </span>            : {
<span class="lineNum">     199 </span>            :         VM_BUG_ON(in_interrupt());
<span class="lineNum">     200 </span>            : 
<span class="lineNum">     201 </span>            : #if !defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_TREE_RCU)
<span class="lineNum">     202 </span>            : # ifdef CONFIG_PREEMPT_COUNT
<span class="lineNum">     203 </span>            :         VM_BUG_ON(!in_atomic());
<span class="lineNum">     204 </span>            : # endif
<span class="lineNum">     205 </span>            :         VM_BUG_ON_PAGE(page_count(page) == 0, page);
<span class="lineNum">     206 </span>            :         atomic_add(count, &amp;page-&gt;_count);
<span class="lineNum">     207 </span>            : 
<span class="lineNum">     208 </span>            : #else
<span class="lineNum">     209 </span>            :         if (unlikely(!atomic_add_unless(&amp;page-&gt;_count, count, 0)))
<span class="lineNum">     210 </span>            :                 return 0;
<span class="lineNum">     211 </span>            : #endif
<span class="lineNum">     212 </span>            :         VM_BUG_ON_PAGE(PageCompound(page) &amp;&amp; page != compound_head(page), page);
<span class="lineNum">     213 </span>            : 
<span class="lineNum">     214 </span>            :         return 1;
<span class="lineNum">     215 </span>            : }
<span class="lineNum">     216 </span>            : 
<span class="lineNum">     217 </span>            : static inline int page_freeze_refs(struct page *page, int count)
<span class="lineNum">     218 </span>            : {
<span class="lineNum">     219 </span>            :         return likely(atomic_cmpxchg(&amp;page-&gt;_count, count, 0) == count);
<span class="lineNum">     220 </span>            : }
<span class="lineNum">     221 </span>            : 
<span class="lineNum">     222 </span>            : static inline void page_unfreeze_refs(struct page *page, int count)
<span class="lineNum">     223 </span>            : {
<span class="lineNum">     224 </span>            :         VM_BUG_ON_PAGE(page_count(page) != 0, page);
<span class="lineNum">     225 </span>            :         VM_BUG_ON(count == 0);
<span class="lineNum">     226 </span>            : 
<span class="lineNum">     227 </span>            :         atomic_set(&amp;page-&gt;_count, count);
<span class="lineNum">     228 </span>            : }
<span class="lineNum">     229 </span>            : 
<span class="lineNum">     230 </span>            : #ifdef CONFIG_NUMA
<span class="lineNum">     231 </span>            : extern struct page *__page_cache_alloc(gfp_t gfp);
<span class="lineNum">     232 </span>            : #else
<span class="lineNum">     233 </span>            : static inline struct page *__page_cache_alloc(gfp_t gfp)
<span class="lineNum">     234 </span>            : {
<span class="lineNum">     235 </span>            :         return alloc_pages(gfp, 0);
<span class="lineNum">     236 </span>            : }
<span class="lineNum">     237 </span>            : #endif
<span class="lineNum">     238 </span>            : 
<span class="lineNum">     239 </span>            : static inline struct page *page_cache_alloc(struct address_space *x)
<span class="lineNum">     240 </span>            : {
<span class="lineNum">     241 </span>            :         return __page_cache_alloc(mapping_gfp_mask(x));
<span class="lineNum">     242 </span>            : }
<span class="lineNum">     243 </span>            : 
<span class="lineNum">     244 </span>            : static inline struct page *page_cache_alloc_cold(struct address_space *x)
<span class="lineNum">     245 </span>            : {
<span class="lineNum">     246 </span>            :         return __page_cache_alloc(mapping_gfp_mask(x)|__GFP_COLD);
<span class="lineNum">     247 </span>            : }
<span class="lineNum">     248 </span>            : 
<span class="lineNum">     249 </span>            : static inline struct page *page_cache_alloc_readahead(struct address_space *x)
<span class="lineNum">     250 </span>            : {
<span class="lineNum">     251 </span>            :         return __page_cache_alloc(mapping_gfp_mask(x) |
<span class="lineNum">     252 </span>            :                                   __GFP_COLD | __GFP_NORETRY | __GFP_NOWARN);
<span class="lineNum">     253 </span>            : }
<span class="lineNum">     254 </span>            : 
<span class="lineNum">     255 </span>            : typedef int filler_t(void *, struct page *);
<span class="lineNum">     256 </span>            : 
<span class="lineNum">     257 </span>            : pgoff_t page_cache_next_hole(struct address_space *mapping,
<span class="lineNum">     258 </span>            :                              pgoff_t index, unsigned long max_scan);
<span class="lineNum">     259 </span>            : pgoff_t page_cache_prev_hole(struct address_space *mapping,
<span class="lineNum">     260 </span>            :                              pgoff_t index, unsigned long max_scan);
<span class="lineNum">     261 </span>            : 
<span class="lineNum">     262 </span>            : #define FGP_ACCESSED            0x00000001
<span class="lineNum">     263 </span>            : #define FGP_LOCK                0x00000002
<span class="lineNum">     264 </span>            : #define FGP_CREAT               0x00000004
<span class="lineNum">     265 </span>            : #define FGP_WRITE               0x00000008
<span class="lineNum">     266 </span>            : #define FGP_NOFS                0x00000010
<span class="lineNum">     267 </span>            : #define FGP_NOWAIT              0x00000020
<span class="lineNum">     268 </span>            : 
<span class="lineNum">     269 </span>            : struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
<span class="lineNum">     270 </span>            :                 int fgp_flags, gfp_t cache_gfp_mask, gfp_t radix_gfp_mask);
<span class="lineNum">     271 </span>            : 
<span class="lineNum">     272 </span>            : /**
<span class="lineNum">     273 </span>            :  * find_get_page - find and get a page reference
<span class="lineNum">     274 </span>            :  * @mapping: the address_space to search
<span class="lineNum">     275 </span>            :  * @offset: the page index
<span class="lineNum">     276 </span>            :  *
<span class="lineNum">     277 </span>            :  * Looks up the page cache slot at @mapping &amp; @offset.  If there is a
<span class="lineNum">     278 </span>            :  * page cache page, it is returned with an increased refcount.
<span class="lineNum">     279 </span>            :  *
<span class="lineNum">     280 </span>            :  * Otherwise, %NULL is returned.
<span class="lineNum">     281 </span>            :  */
<span class="lineNum">     282 </span>            : static inline struct page *find_get_page(struct address_space *mapping,
<span class="lineNum">     283 </span>            :                                         pgoff_t offset)
<span class="lineNum">     284 </span>            : {
<span class="lineNum">     285 </span><span class="lineCov">    1320080 :         return pagecache_get_page(mapping, offset, 0, 0, 0);</span>
<span class="lineNum">     286 </span>            : }
<span class="lineNum">     287 </span>            : 
<span class="lineNum">     288 </span>            : static inline struct page *find_get_page_flags(struct address_space *mapping,
<span class="lineNum">     289 </span>            :                                         pgoff_t offset, int fgp_flags)
<span class="lineNum">     290 </span>            : {
<span class="lineNum">     291 </span>            :         return pagecache_get_page(mapping, offset, fgp_flags, 0, 0);
<span class="lineNum">     292 </span>            : }
<span class="lineNum">     293 </span>            : 
<span class="lineNum">     294 </span>            : /**
<span class="lineNum">     295 </span>            :  * find_lock_page - locate, pin and lock a pagecache page
<span class="lineNum">     296 </span>            :  * pagecache_get_page - find and get a page reference
<span class="lineNum">     297 </span>            :  * @mapping: the address_space to search
<span class="lineNum">     298 </span>            :  * @offset: the page index
<span class="lineNum">     299 </span>            :  *
<span class="lineNum">     300 </span>            :  * Looks up the page cache slot at @mapping &amp; @offset.  If there is a
<span class="lineNum">     301 </span>            :  * page cache page, it is returned locked and with an increased
<span class="lineNum">     302 </span>            :  * refcount.
<span class="lineNum">     303 </span>            :  *
<span class="lineNum">     304 </span>            :  * Otherwise, %NULL is returned.
<span class="lineNum">     305 </span>            :  *
<span class="lineNum">     306 </span>            :  * find_lock_page() may sleep.
<span class="lineNum">     307 </span>            :  */
<span class="lineNum">     308 </span>            : static inline struct page *find_lock_page(struct address_space *mapping,
<span class="lineNum">     309 </span>            :                                         pgoff_t offset)
<span class="lineNum">     310 </span>            : {
<span class="lineNum">     311 </span><span class="lineCov">       1276 :         return pagecache_get_page(mapping, offset, FGP_LOCK, 0, 0);</span>
<span class="lineNum">     312 </span>            : }
<span class="lineNum">     313 </span>            : 
<span class="lineNum">     314 </span>            : /**
<span class="lineNum">     315 </span>            :  * find_or_create_page - locate or add a pagecache page
<span class="lineNum">     316 </span>            :  * @mapping: the page's address_space
<span class="lineNum">     317 </span>            :  * @index: the page's index into the mapping
<span class="lineNum">     318 </span>            :  * @gfp_mask: page allocation mode
<span class="lineNum">     319 </span>            :  *
<span class="lineNum">     320 </span>            :  * Looks up the page cache slot at @mapping &amp; @offset.  If there is a
<span class="lineNum">     321 </span>            :  * page cache page, it is returned locked and with an increased
<span class="lineNum">     322 </span>            :  * refcount.
<span class="lineNum">     323 </span>            :  *
<span class="lineNum">     324 </span>            :  * If the page is not present, a new page is allocated using @gfp_mask
<span class="lineNum">     325 </span>            :  * and added to the page cache and the VM's LRU list.  The page is
<span class="lineNum">     326 </span>            :  * returned locked and with an increased refcount.
<span class="lineNum">     327 </span>            :  *
<span class="lineNum">     328 </span>            :  * On memory exhaustion, %NULL is returned.
<span class="lineNum">     329 </span>            :  *
<span class="lineNum">     330 </span>            :  * find_or_create_page() may sleep, even if @gfp_flags specifies an
<span class="lineNum">     331 </span>            :  * atomic allocation!
<span class="lineNum">     332 </span>            :  */
<span class="lineNum">     333 </span>            : static inline struct page *find_or_create_page(struct address_space *mapping,
<span class="lineNum">     334 </span>            :                                         pgoff_t offset, gfp_t gfp_mask)
<span class="lineNum">     335 </span>            : {
<span class="lineNum">     336 </span><span class="lineCov">    1568484 :         return pagecache_get_page(mapping, offset,</span>
<span class="lineNum">     337 </span>            :                                         FGP_LOCK|FGP_ACCESSED|FGP_CREAT,
<span class="lineNum">     338 </span>            :                                         gfp_mask, gfp_mask &amp; GFP_RECLAIM_MASK);
<span class="lineNum">     339 </span>            : }
<span class="lineNum">     340 </span>            : 
<span class="lineNum">     341 </span>            : /**
<span class="lineNum">     342 </span>            :  * grab_cache_page_nowait - returns locked page at given index in given cache
<span class="lineNum">     343 </span>            :  * @mapping: target address_space
<span class="lineNum">     344 </span>            :  * @index: the page index
<span class="lineNum">     345 </span>            :  *
<span class="lineNum">     346 </span>            :  * Same as grab_cache_page(), but do not wait if the page is unavailable.
<span class="lineNum">     347 </span>            :  * This is intended for speculative data generators, where the data can
<span class="lineNum">     348 </span>            :  * be regenerated if the page couldn't be grabbed.  This routine should
<span class="lineNum">     349 </span>            :  * be safe to call while holding the lock for another page.
<span class="lineNum">     350 </span>            :  *
<span class="lineNum">     351 </span>            :  * Clear __GFP_FS when allocating the page to avoid recursion into the fs
<span class="lineNum">     352 </span>            :  * and deadlock against the caller's locked page.
<span class="lineNum">     353 </span>            :  */
<span class="lineNum">     354 </span>            : static inline struct page *grab_cache_page_nowait(struct address_space *mapping,
<span class="lineNum">     355 </span>            :                                 pgoff_t index)
<span class="lineNum">     356 </span>            : {
<span class="lineNum">     357 </span>            :         return pagecache_get_page(mapping, index,
<span class="lineNum">     358 </span>            :                         FGP_LOCK|FGP_CREAT|FGP_NOFS|FGP_NOWAIT,
<span class="lineNum">     359 </span>            :                         mapping_gfp_mask(mapping),
<span class="lineNum">     360 </span>            :                         GFP_NOFS);
<span class="lineNum">     361 </span>            : }
<span class="lineNum">     362 </span>            : 
<span class="lineNum">     363 </span>            : struct page *find_get_entry(struct address_space *mapping, pgoff_t offset);
<span class="lineNum">     364 </span>            : struct page *find_lock_entry(struct address_space *mapping, pgoff_t offset);
<span class="lineNum">     365 </span>            : unsigned find_get_entries(struct address_space *mapping, pgoff_t start,
<span class="lineNum">     366 </span>            :                           unsigned int nr_entries, struct page **entries,
<span class="lineNum">     367 </span>            :                           pgoff_t *indices);
<span class="lineNum">     368 </span>            : unsigned find_get_pages(struct address_space *mapping, pgoff_t start,
<span class="lineNum">     369 </span>            :                         unsigned int nr_pages, struct page **pages);
<span class="lineNum">     370 </span>            : unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t start,
<span class="lineNum">     371 </span>            :                                unsigned int nr_pages, struct page **pages);
<span class="lineNum">     372 </span>            : unsigned find_get_pages_tag(struct address_space *mapping, pgoff_t *index,
<span class="lineNum">     373 </span>            :                         int tag, unsigned int nr_pages, struct page **pages);
<span class="lineNum">     374 </span>            : 
<span class="lineNum">     375 </span>            : struct page *grab_cache_page_write_begin(struct address_space *mapping,
<span class="lineNum">     376 </span>            :                         pgoff_t index, unsigned flags);
<span class="lineNum">     377 </span>            : 
<span class="lineNum">     378 </span>            : /*
<span class="lineNum">     379 </span>            :  * Returns locked page at given index in given cache, creating it if needed.
<span class="lineNum">     380 </span>            :  */
<span class="lineNum">     381 </span><span class="lineNoCov">          0 : static inline struct page *grab_cache_page(struct address_space *mapping,</span>
<span class="lineNum">     382 </span>            :                                                                 pgoff_t index)
<span class="lineNum">     383 </span>            : {
<span class="lineNum">     384 </span><span class="lineNoCov">          0 :         return find_or_create_page(mapping, index, mapping_gfp_mask(mapping));</span>
<span class="lineNum">     385 </span>            : }
<span class="lineNum">     386 </span>            : 
<span class="lineNum">     387 </span>            : extern struct page * read_cache_page(struct address_space *mapping,
<span class="lineNum">     388 </span>            :                                 pgoff_t index, filler_t *filler, void *data);
<span class="lineNum">     389 </span>            : extern struct page * read_cache_page_gfp(struct address_space *mapping,
<span class="lineNum">     390 </span>            :                                 pgoff_t index, gfp_t gfp_mask);
<span class="lineNum">     391 </span>            : extern int read_cache_pages(struct address_space *mapping,
<span class="lineNum">     392 </span>            :                 struct list_head *pages, filler_t *filler, void *data);
<span class="lineNum">     393 </span>            : 
<span class="lineNum">     394 </span>            : static inline struct page *read_mapping_page(struct address_space *mapping,
<span class="lineNum">     395 </span>            :                                 pgoff_t index, void *data)
<span class="lineNum">     396 </span>            : {
<span class="lineNum">     397 </span>            :         filler_t *filler = (filler_t *)mapping-&gt;a_ops-&gt;readpage;
<span class="lineNum">     398 </span>            :         return read_cache_page(mapping, index, filler, data);
<span class="lineNum">     399 </span>            : }
<span class="lineNum">     400 </span>            : 
<span class="lineNum">     401 </span>            : /*
<span class="lineNum">     402 </span>            :  * Get the offset in PAGE_SIZE.
<span class="lineNum">     403 </span>            :  * (TODO: hugepage should have -&gt;index in PAGE_SIZE)
<span class="lineNum">     404 </span>            :  */
<span class="lineNum">     405 </span>            : static inline pgoff_t page_to_pgoff(struct page *page)
<span class="lineNum">     406 </span>            : {
<span class="lineNum">     407 </span>            :         if (unlikely(PageHeadHuge(page)))
<span class="lineNum">     408 </span>            :                 return page-&gt;index &lt;&lt; compound_order(page);
<span class="lineNum">     409 </span>            :         else
<span class="lineNum">     410 </span>            :                 return page-&gt;index &lt;&lt; (PAGE_CACHE_SHIFT - PAGE_SHIFT);
<span class="lineNum">     411 </span>            : }
<span class="lineNum">     412 </span>            : 
<span class="lineNum">     413 </span>            : /*
<span class="lineNum">     414 </span>            :  * Return byte-offset into filesystem object for page.
<span class="lineNum">     415 </span>            :  */
<span class="lineNum">     416 </span>            : static inline loff_t page_offset(struct page *page)
<span class="lineNum">     417 </span>            : {
<span class="lineNum">     418 </span><span class="lineCov">    8692269 :         return ((loff_t)page-&gt;index) &lt;&lt; PAGE_CACHE_SHIFT;</span>
<span class="lineNum">     419 </span>            : }
<span class="lineNum">     420 </span>            : 
<span class="lineNum">     421 </span>            : static inline loff_t page_file_offset(struct page *page)
<span class="lineNum">     422 </span>            : {
<span class="lineNum">     423 </span>            :         return ((loff_t)page_file_index(page)) &lt;&lt; PAGE_CACHE_SHIFT;
<span class="lineNum">     424 </span>            : }
<span class="lineNum">     425 </span>            : 
<span class="lineNum">     426 </span>            : extern pgoff_t linear_hugepage_index(struct vm_area_struct *vma,
<span class="lineNum">     427 </span>            :                                      unsigned long address);
<span class="lineNum">     428 </span>            : 
<span class="lineNum">     429 </span>            : static inline pgoff_t linear_page_index(struct vm_area_struct *vma,
<span class="lineNum">     430 </span>            :                                         unsigned long address)
<span class="lineNum">     431 </span>            : {
<span class="lineNum">     432 </span>            :         pgoff_t pgoff;
<span class="lineNum">     433 </span>            :         if (unlikely(is_vm_hugetlb_page(vma)))
<span class="lineNum">     434 </span>            :                 return linear_hugepage_index(vma, address);
<span class="lineNum">     435 </span>            :         pgoff = (address - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;
<span class="lineNum">     436 </span>            :         pgoff += vma-&gt;vm_pgoff;
<span class="lineNum">     437 </span>            :         return pgoff &gt;&gt; (PAGE_CACHE_SHIFT - PAGE_SHIFT);
<span class="lineNum">     438 </span>            : }
<span class="lineNum">     439 </span>            : 
<span class="lineNum">     440 </span>            : extern void __lock_page(struct page *page);
<span class="lineNum">     441 </span>            : extern int __lock_page_killable(struct page *page);
<span class="lineNum">     442 </span>            : extern int __lock_page_or_retry(struct page *page, struct mm_struct *mm,
<span class="lineNum">     443 </span>            :                                 unsigned int flags);
<span class="lineNum">     444 </span>            : extern void unlock_page(struct page *page);
<span class="lineNum">     445 </span>            : 
<span class="lineNum">     446 </span>            : static inline void __set_page_locked(struct page *page)
<span class="lineNum">     447 </span>            : {
<span class="lineNum">     448 </span>            :         __set_bit(PG_locked, &amp;page-&gt;flags);
<span class="lineNum">     449 </span>            : }
<span class="lineNum">     450 </span>            : 
<span class="lineNum">     451 </span>            : static inline void __clear_page_locked(struct page *page)
<span class="lineNum">     452 </span>            : {
<span class="lineNum">     453 </span>            :         __clear_bit(PG_locked, &amp;page-&gt;flags);
<span class="lineNum">     454 </span>            : }
<span class="lineNum">     455 </span>            : 
<a name="456"><span class="lineNum">     456 </span>            : static inline int trylock_page(struct page *page)</a>
<span class="lineNum">     457 </span>            : {
<span class="lineNum">     458 </span><span class="lineCov">    5647383 :         return (likely(!test_and_set_bit_lock(PG_locked, &amp;page-&gt;flags)));</span>
<span class="lineNum">     459 </span>            : }
<span class="lineNum">     460 </span>            : 
<span class="lineNum">     461 </span>            : /*
<span class="lineNum">     462 </span>            :  * lock_page may only be called if we have the page's inode pinned.
<span class="lineNum">     463 </span>            :  */
<span class="lineNum">     464 </span><span class="lineCov">    1332190 : static inline void lock_page(struct page *page)</span>
<span class="lineNum">     465 </span>            : {
<span class="lineNum">     466 </span><span class="lineCov">    1332190 :         might_sleep();</span>
<span class="lineNum">     467 </span><span class="lineCov">    1332195 :         if (!trylock_page(page))</span>
<span class="lineNum">     468 </span><span class="lineCov">       1176 :                 __lock_page(page);</span>
<span class="lineNum">     469 </span><span class="lineCov">    1332195 : }</span>
<span class="lineNum">     470 </span>            : 
<span class="lineNum">     471 </span>            : /*
<span class="lineNum">     472 </span>            :  * lock_page_killable is like lock_page but can be interrupted by fatal
<span class="lineNum">     473 </span>            :  * signals.  It returns 0 if it locked the page and -EINTR if it was
<span class="lineNum">     474 </span>            :  * killed while waiting.
<span class="lineNum">     475 </span>            :  */
<span class="lineNum">     476 </span>            : static inline int lock_page_killable(struct page *page)
<span class="lineNum">     477 </span>            : {
<span class="lineNum">     478 </span>            :         might_sleep();
<span class="lineNum">     479 </span>            :         if (!trylock_page(page))
<span class="lineNum">     480 </span>            :                 return __lock_page_killable(page);
<span class="lineNum">     481 </span>            :         return 0;
<span class="lineNum">     482 </span>            : }
<span class="lineNum">     483 </span>            : 
<span class="lineNum">     484 </span>            : /*
<span class="lineNum">     485 </span>            :  * lock_page_or_retry - Lock the page, unless this would block and the
<span class="lineNum">     486 </span>            :  * caller indicated that it can handle a retry.
<span class="lineNum">     487 </span>            :  *
<span class="lineNum">     488 </span>            :  * Return value and mmap_sem implications depend on flags; see
<span class="lineNum">     489 </span>            :  * __lock_page_or_retry().
<span class="lineNum">     490 </span>            :  */
<span class="lineNum">     491 </span>            : static inline int lock_page_or_retry(struct page *page, struct mm_struct *mm,
<span class="lineNum">     492 </span>            :                                      unsigned int flags)
<span class="lineNum">     493 </span>            : {
<span class="lineNum">     494 </span>            :         might_sleep();
<span class="lineNum">     495 </span>            :         return trylock_page(page) || __lock_page_or_retry(page, mm, flags);
<span class="lineNum">     496 </span>            : }
<span class="lineNum">     497 </span>            : 
<span class="lineNum">     498 </span>            : /*
<span class="lineNum">     499 </span>            :  * This is exported only for wait_on_page_locked/wait_on_page_writeback.
<span class="lineNum">     500 </span>            :  * Never use this directly!
<span class="lineNum">     501 </span>            :  */
<span class="lineNum">     502 </span>            : extern void wait_on_page_bit(struct page *page, int bit_nr);
<span class="lineNum">     503 </span>            : 
<span class="lineNum">     504 </span>            : extern int wait_on_page_bit_killable(struct page *page, int bit_nr);
<span class="lineNum">     505 </span>            : 
<span class="lineNum">     506 </span>            : static inline int wait_on_page_locked_killable(struct page *page)
<span class="lineNum">     507 </span>            : {
<span class="lineNum">     508 </span>            :         if (PageLocked(page))
<span class="lineNum">     509 </span>            :                 return wait_on_page_bit_killable(page, PG_locked);
<span class="lineNum">     510 </span>            :         return 0;
<span class="lineNum">     511 </span>            : }
<span class="lineNum">     512 </span>            : 
<span class="lineNum">     513 </span>            : /* 
<span class="lineNum">     514 </span>            :  * Wait for a page to be unlocked.
<span class="lineNum">     515 </span>            :  *
<span class="lineNum">     516 </span>            :  * This must be called with the caller &quot;holding&quot; the page,
<span class="lineNum">     517 </span>            :  * ie with increased &quot;page-&gt;count&quot; so that the page won't
<span class="lineNum">     518 </span>            :  * go away during the wait..
<span class="lineNum">     519 </span>            :  */
<span class="lineNum">     520 </span>            : static inline void wait_on_page_locked(struct page *page)
<span class="lineNum">     521 </span>            : {
<span class="lineNum">     522 </span><span class="lineCov">       7485 :         if (PageLocked(page))</span>
<span class="lineNum">     523 </span><span class="lineCov">       2518 :                 wait_on_page_bit(page, PG_locked);</span>
<span class="lineNum">     524 </span>            : }
<span class="lineNum">     525 </span>            : 
<span class="lineNum">     526 </span>            : /* 
<a name="527"><span class="lineNum">     527 </span>            :  * Wait for a page to complete writeback</a>
<span class="lineNum">     528 </span>            :  */
<span class="lineNum">     529 </span><span class="lineCov">    3116974 : static inline void wait_on_page_writeback(struct page *page)</span>
<span class="lineNum">     530 </span>            : {
<span class="lineNum">     531 </span><span class="lineCov">    3116974 :         if (PageWriteback(page))</span>
<span class="lineNum">     532 </span><span class="lineCov">         62 :                 wait_on_page_bit(page, PG_writeback);</span>
<span class="lineNum">     533 </span><span class="lineCov">    3116974 : }</span>
<span class="lineNum">     534 </span>            : 
<span class="lineNum">     535 </span>            : extern void end_page_writeback(struct page *page);
<span class="lineNum">     536 </span>            : void wait_for_stable_page(struct page *page);
<span class="lineNum">     537 </span>            : 
<span class="lineNum">     538 </span>            : void page_endio(struct page *page, int rw, int err);
<span class="lineNum">     539 </span>            : 
<span class="lineNum">     540 </span>            : /*
<span class="lineNum">     541 </span>            :  * Add an arbitrary waiter to a page's wait queue
<span class="lineNum">     542 </span>            :  */
<span class="lineNum">     543 </span>            : extern void add_page_wait_queue(struct page *page, wait_queue_t *waiter);
<span class="lineNum">     544 </span>            : 
<span class="lineNum">     545 </span>            : /*
<span class="lineNum">     546 </span>            :  * Fault a userspace page into pagetables.  Return non-zero on a fault.
<span class="lineNum">     547 </span>            :  *
<span class="lineNum">     548 </span>            :  * This assumes that two userspace pages are always sufficient.  That's
<span class="lineNum">     549 </span>            :  * not true if PAGE_CACHE_SIZE &gt; PAGE_SIZE.
<span class="lineNum">     550 </span>            :  */
<span class="lineNum">     551 </span>            : static inline int fault_in_pages_writeable(char __user *uaddr, int size)
<span class="lineNum">     552 </span>            : {
<span class="lineNum">     553 </span>            :         int ret;
<span class="lineNum">     554 </span>            : 
<span class="lineNum">     555 </span>            :         if (unlikely(size == 0))
<span class="lineNum">     556 </span>            :                 return 0;
<span class="lineNum">     557 </span>            : 
<span class="lineNum">     558 </span>            :         /*
<span class="lineNum">     559 </span>            :          * Writing zeroes into userspace here is OK, because we know that if
<span class="lineNum">     560 </span>            :          * the zero gets there, we'll be overwriting it.
<span class="lineNum">     561 </span>            :          */
<span class="lineNum">     562 </span>            :         ret = __put_user(0, uaddr);
<span class="lineNum">     563 </span>            :         if (ret == 0) {
<span class="lineNum">     564 </span>            :                 char __user *end = uaddr + size - 1;
<span class="lineNum">     565 </span>            : 
<span class="lineNum">     566 </span>            :                 /*
<span class="lineNum">     567 </span>            :                  * If the page was already mapped, this will get a cache miss
<span class="lineNum">     568 </span>            :                  * for sure, so try to avoid doing it.
<span class="lineNum">     569 </span>            :                  */
<span class="lineNum">     570 </span>            :                 if (((unsigned long)uaddr &amp; PAGE_MASK) !=
<span class="lineNum">     571 </span>            :                                 ((unsigned long)end &amp; PAGE_MASK))
<span class="lineNum">     572 </span>            :                         ret = __put_user(0, end);
<span class="lineNum">     573 </span>            :         }
<span class="lineNum">     574 </span>            :         return ret;
<span class="lineNum">     575 </span>            : }
<span class="lineNum">     576 </span>            : 
<span class="lineNum">     577 </span>            : static inline int fault_in_pages_readable(const char __user *uaddr, int size)
<span class="lineNum">     578 </span>            : {
<span class="lineNum">     579 </span>            :         volatile char c;
<span class="lineNum">     580 </span>            :         int ret;
<span class="lineNum">     581 </span>            : 
<span class="lineNum">     582 </span>            :         if (unlikely(size == 0))
<span class="lineNum">     583 </span>            :                 return 0;
<span class="lineNum">     584 </span>            : 
<span class="lineNum">     585 </span>            :         ret = __get_user(c, uaddr);
<span class="lineNum">     586 </span>            :         if (ret == 0) {
<span class="lineNum">     587 </span>            :                 const char __user *end = uaddr + size - 1;
<span class="lineNum">     588 </span>            : 
<span class="lineNum">     589 </span>            :                 if (((unsigned long)uaddr &amp; PAGE_MASK) !=
<span class="lineNum">     590 </span>            :                                 ((unsigned long)end &amp; PAGE_MASK)) {
<span class="lineNum">     591 </span>            :                         ret = __get_user(c, end);
<span class="lineNum">     592 </span>            :                         (void)c;
<span class="lineNum">     593 </span>            :                 }
<span class="lineNum">     594 </span>            :         }
<span class="lineNum">     595 </span>            :         return ret;
<span class="lineNum">     596 </span>            : }
<span class="lineNum">     597 </span>            : 
<span class="lineNum">     598 </span>            : /*
<span class="lineNum">     599 </span>            :  * Multipage variants of the above prefault helpers, useful if more than
<span class="lineNum">     600 </span>            :  * PAGE_SIZE of data needs to be prefaulted. These are separate from the above
<span class="lineNum">     601 </span>            :  * functions (which only handle up to PAGE_SIZE) to avoid clobbering the
<span class="lineNum">     602 </span>            :  * filemap.c hotpaths.
<span class="lineNum">     603 </span>            :  */
<span class="lineNum">     604 </span>            : static inline int fault_in_multipages_writeable(char __user *uaddr, int size)
<span class="lineNum">     605 </span>            : {
<span class="lineNum">     606 </span>            :         int ret = 0;
<span class="lineNum">     607 </span>            :         char __user *end = uaddr + size - 1;
<span class="lineNum">     608 </span>            : 
<span class="lineNum">     609 </span>            :         if (unlikely(size == 0))
<span class="lineNum">     610 </span>            :                 return ret;
<span class="lineNum">     611 </span>            : 
<span class="lineNum">     612 </span>            :         /*
<span class="lineNum">     613 </span>            :          * Writing zeroes into userspace here is OK, because we know that if
<span class="lineNum">     614 </span>            :          * the zero gets there, we'll be overwriting it.
<span class="lineNum">     615 </span>            :          */
<span class="lineNum">     616 </span>            :         while (uaddr &lt;= end) {
<span class="lineNum">     617 </span>            :                 ret = __put_user(0, uaddr);
<span class="lineNum">     618 </span>            :                 if (ret != 0)
<span class="lineNum">     619 </span>            :                         return ret;
<span class="lineNum">     620 </span>            :                 uaddr += PAGE_SIZE;
<span class="lineNum">     621 </span>            :         }
<span class="lineNum">     622 </span>            : 
<span class="lineNum">     623 </span>            :         /* Check whether the range spilled into the next page. */
<span class="lineNum">     624 </span>            :         if (((unsigned long)uaddr &amp; PAGE_MASK) ==
<span class="lineNum">     625 </span>            :                         ((unsigned long)end &amp; PAGE_MASK))
<span class="lineNum">     626 </span>            :                 ret = __put_user(0, end);
<span class="lineNum">     627 </span>            : 
<span class="lineNum">     628 </span>            :         return ret;
<span class="lineNum">     629 </span>            : }
<span class="lineNum">     630 </span>            : 
<span class="lineNum">     631 </span>            : static inline int fault_in_multipages_readable(const char __user *uaddr,
<span class="lineNum">     632 </span>            :                                                int size)
<span class="lineNum">     633 </span>            : {
<span class="lineNum">     634 </span>            :         volatile char c;
<span class="lineNum">     635 </span>            :         int ret = 0;
<span class="lineNum">     636 </span>            :         const char __user *end = uaddr + size - 1;
<span class="lineNum">     637 </span>            : 
<span class="lineNum">     638 </span>            :         if (unlikely(size == 0))
<span class="lineNum">     639 </span>            :                 return ret;
<span class="lineNum">     640 </span>            : 
<span class="lineNum">     641 </span>            :         while (uaddr &lt;= end) {
<span class="lineNum">     642 </span>            :                 ret = __get_user(c, uaddr);
<span class="lineNum">     643 </span>            :                 if (ret != 0)
<span class="lineNum">     644 </span>            :                         return ret;
<span class="lineNum">     645 </span>            :                 uaddr += PAGE_SIZE;
<span class="lineNum">     646 </span>            :         }
<span class="lineNum">     647 </span>            : 
<span class="lineNum">     648 </span>            :         /* Check whether the range spilled into the next page. */
<span class="lineNum">     649 </span>            :         if (((unsigned long)uaddr &amp; PAGE_MASK) ==
<span class="lineNum">     650 </span>            :                         ((unsigned long)end &amp; PAGE_MASK)) {
<span class="lineNum">     651 </span>            :                 ret = __get_user(c, end);
<span class="lineNum">     652 </span>            :                 (void)c;
<span class="lineNum">     653 </span>            :         }
<span class="lineNum">     654 </span>            : 
<span class="lineNum">     655 </span>            :         return ret;
<span class="lineNum">     656 </span>            : }
<span class="lineNum">     657 </span>            : 
<span class="lineNum">     658 </span>            : int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
<span class="lineNum">     659 </span>            :                                 pgoff_t index, gfp_t gfp_mask);
<span class="lineNum">     660 </span>            : int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
<span class="lineNum">     661 </span>            :                                 pgoff_t index, gfp_t gfp_mask);
<span class="lineNum">     662 </span>            : extern void delete_from_page_cache(struct page *page);
<span class="lineNum">     663 </span>            : extern void __delete_from_page_cache(struct page *page, void *shadow);
<span class="lineNum">     664 </span>            : int replace_page_cache_page(struct page *old, struct page *new, gfp_t gfp_mask);
<span class="lineNum">     665 </span>            : 
<span class="lineNum">     666 </span>            : /*
<span class="lineNum">     667 </span>            :  * Like add_to_page_cache_locked, but used to add newly allocated pages:
<span class="lineNum">     668 </span>            :  * the page is new, so we can just run __set_page_locked() against it.
<span class="lineNum">     669 </span>            :  */
<span class="lineNum">     670 </span>            : static inline int add_to_page_cache(struct page *page,
<span class="lineNum">     671 </span>            :                 struct address_space *mapping, pgoff_t offset, gfp_t gfp_mask)
<span class="lineNum">     672 </span>            : {
<span class="lineNum">     673 </span>            :         int error;
<span class="lineNum">     674 </span>            : 
<span class="lineNum">     675 </span>            :         __set_page_locked(page);
<span class="lineNum">     676 </span>            :         error = add_to_page_cache_locked(page, mapping, offset, gfp_mask);
<span class="lineNum">     677 </span>            :         if (unlikely(error))
<span class="lineNum">     678 </span>            :                 __clear_page_locked(page);
<span class="lineNum">     679 </span>            :         return error;
<span class="lineNum">     680 </span>            : }
<span class="lineNum">     681 </span>            : 
<span class="lineNum">     682 </span>            : #endif /* _LINUX_PAGEMAP_H */
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.10</a></td></tr>
  </table>
  <br>

</body>
</html>

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - btrfstest.info - include/linux/mm.h</title>
  <link rel="stylesheet" type="text/css" href="../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../index.html">top level</a> - <a href="index.html">include/linux</a> - mm.h<span style="font-size: 80%;"> (source / <a href="mm.h.func.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">btrfstest.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">12</td>
            <td class="headerCovTableEntry">14</td>
            <td class="headerCovTableEntryMed">85.7 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2014-11-28</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">2</td>
            <td class="headerCovTableEntry">2</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr><td><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : #ifndef _LINUX_MM_H</a>
<span class="lineNum">       2 </span>            : #define _LINUX_MM_H
<span class="lineNum">       3 </span>            : 
<span class="lineNum">       4 </span>            : #include &lt;linux/errno.h&gt;
<span class="lineNum">       5 </span>            : 
<span class="lineNum">       6 </span>            : #ifdef __KERNEL__
<span class="lineNum">       7 </span>            : 
<span class="lineNum">       8 </span>            : #include &lt;linux/mmdebug.h&gt;
<span class="lineNum">       9 </span>            : #include &lt;linux/gfp.h&gt;
<span class="lineNum">      10 </span>            : #include &lt;linux/bug.h&gt;
<span class="lineNum">      11 </span>            : #include &lt;linux/list.h&gt;
<span class="lineNum">      12 </span>            : #include &lt;linux/mmzone.h&gt;
<span class="lineNum">      13 </span>            : #include &lt;linux/rbtree.h&gt;
<span class="lineNum">      14 </span>            : #include &lt;linux/atomic.h&gt;
<span class="lineNum">      15 </span>            : #include &lt;linux/debug_locks.h&gt;
<span class="lineNum">      16 </span>            : #include &lt;linux/mm_types.h&gt;
<span class="lineNum">      17 </span>            : #include &lt;linux/range.h&gt;
<span class="lineNum">      18 </span>            : #include &lt;linux/pfn.h&gt;
<span class="lineNum">      19 </span>            : #include &lt;linux/bit_spinlock.h&gt;
<span class="lineNum">      20 </span>            : #include &lt;linux/shrinker.h&gt;
<span class="lineNum">      21 </span>            : 
<span class="lineNum">      22 </span>            : struct mempolicy;
<span class="lineNum">      23 </span>            : struct anon_vma;
<span class="lineNum">      24 </span>            : struct anon_vma_chain;
<span class="lineNum">      25 </span>            : struct file_ra_state;
<span class="lineNum">      26 </span>            : struct user_struct;
<span class="lineNum">      27 </span>            : struct writeback_control;
<span class="lineNum">      28 </span>            : 
<span class="lineNum">      29 </span>            : #ifndef CONFIG_NEED_MULTIPLE_NODES      /* Don't use mapnrs, do it properly */
<span class="lineNum">      30 </span>            : extern unsigned long max_mapnr;
<span class="lineNum">      31 </span>            : 
<span class="lineNum">      32 </span>            : static inline void set_max_mapnr(unsigned long limit)
<span class="lineNum">      33 </span>            : {
<span class="lineNum">      34 </span>            :         max_mapnr = limit;
<span class="lineNum">      35 </span>            : }
<span class="lineNum">      36 </span>            : #else
<span class="lineNum">      37 </span>            : static inline void set_max_mapnr(unsigned long limit) { }
<span class="lineNum">      38 </span>            : #endif
<span class="lineNum">      39 </span>            : 
<span class="lineNum">      40 </span>            : extern unsigned long totalram_pages;
<span class="lineNum">      41 </span>            : extern void * high_memory;
<span class="lineNum">      42 </span>            : extern int page_cluster;
<span class="lineNum">      43 </span>            : 
<span class="lineNum">      44 </span>            : #ifdef CONFIG_SYSCTL
<span class="lineNum">      45 </span>            : extern int sysctl_legacy_va_layout;
<span class="lineNum">      46 </span>            : #else
<span class="lineNum">      47 </span>            : #define sysctl_legacy_va_layout 0
<span class="lineNum">      48 </span>            : #endif
<span class="lineNum">      49 </span>            : 
<span class="lineNum">      50 </span>            : #include &lt;asm/page.h&gt;
<span class="lineNum">      51 </span>            : #include &lt;asm/pgtable.h&gt;
<span class="lineNum">      52 </span>            : #include &lt;asm/processor.h&gt;
<span class="lineNum">      53 </span>            : 
<span class="lineNum">      54 </span>            : #ifndef __pa_symbol
<span class="lineNum">      55 </span>            : #define __pa_symbol(x)  __pa(RELOC_HIDE((unsigned long)(x), 0))
<span class="lineNum">      56 </span>            : #endif
<span class="lineNum">      57 </span>            : 
<span class="lineNum">      58 </span>            : extern unsigned long sysctl_user_reserve_kbytes;
<span class="lineNum">      59 </span>            : extern unsigned long sysctl_admin_reserve_kbytes;
<span class="lineNum">      60 </span>            : 
<span class="lineNum">      61 </span>            : extern int sysctl_overcommit_memory;
<span class="lineNum">      62 </span>            : extern int sysctl_overcommit_ratio;
<span class="lineNum">      63 </span>            : extern unsigned long sysctl_overcommit_kbytes;
<span class="lineNum">      64 </span>            : 
<span class="lineNum">      65 </span>            : extern int overcommit_ratio_handler(struct ctl_table *, int, void __user *,
<span class="lineNum">      66 </span>            :                                     size_t *, loff_t *);
<span class="lineNum">      67 </span>            : extern int overcommit_kbytes_handler(struct ctl_table *, int, void __user *,
<span class="lineNum">      68 </span>            :                                     size_t *, loff_t *);
<span class="lineNum">      69 </span>            : 
<span class="lineNum">      70 </span>            : #define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))
<span class="lineNum">      71 </span>            : 
<span class="lineNum">      72 </span>            : /* to align the pointer to the (next) page boundary */
<span class="lineNum">      73 </span>            : #define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)
<span class="lineNum">      74 </span>            : 
<span class="lineNum">      75 </span>            : /* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */
<span class="lineNum">      76 </span>            : #define PAGE_ALIGNED(addr)      IS_ALIGNED((unsigned long)addr, PAGE_SIZE)
<span class="lineNum">      77 </span>            : 
<span class="lineNum">      78 </span>            : /*
<span class="lineNum">      79 </span>            :  * Linux kernel virtual memory manager primitives.
<span class="lineNum">      80 </span>            :  * The idea being to have a &quot;virtual&quot; mm in the same way
<span class="lineNum">      81 </span>            :  * we have a virtual fs - giving a cleaner interface to the
<span class="lineNum">      82 </span>            :  * mm details, and allowing different kinds of memory mappings
<span class="lineNum">      83 </span>            :  * (from shared memory to executable loading to arbitrary
<span class="lineNum">      84 </span>            :  * mmap() functions).
<span class="lineNum">      85 </span>            :  */
<span class="lineNum">      86 </span>            : 
<span class="lineNum">      87 </span>            : extern struct kmem_cache *vm_area_cachep;
<span class="lineNum">      88 </span>            : 
<span class="lineNum">      89 </span>            : #ifndef CONFIG_MMU
<span class="lineNum">      90 </span>            : extern struct rb_root nommu_region_tree;
<span class="lineNum">      91 </span>            : extern struct rw_semaphore nommu_region_sem;
<span class="lineNum">      92 </span>            : 
<span class="lineNum">      93 </span>            : extern unsigned int kobjsize(const void *objp);
<span class="lineNum">      94 </span>            : #endif
<span class="lineNum">      95 </span>            : 
<span class="lineNum">      96 </span>            : /*
<span class="lineNum">      97 </span>            :  * vm_flags in vm_area_struct, see mm_types.h.
<span class="lineNum">      98 </span>            :  */
<span class="lineNum">      99 </span>            : #define VM_NONE         0x00000000
<span class="lineNum">     100 </span>            : 
<span class="lineNum">     101 </span>            : #define VM_READ         0x00000001      /* currently active flags */
<span class="lineNum">     102 </span>            : #define VM_WRITE        0x00000002
<span class="lineNum">     103 </span>            : #define VM_EXEC         0x00000004
<span class="lineNum">     104 </span>            : #define VM_SHARED       0x00000008
<span class="lineNum">     105 </span>            : 
<span class="lineNum">     106 </span>            : /* mprotect() hardcodes VM_MAYREAD &gt;&gt; 4 == VM_READ, and so for r/w/x bits. */
<span class="lineNum">     107 </span>            : #define VM_MAYREAD      0x00000010      /* limits for mprotect() etc */
<span class="lineNum">     108 </span>            : #define VM_MAYWRITE     0x00000020
<span class="lineNum">     109 </span>            : #define VM_MAYEXEC      0x00000040
<span class="lineNum">     110 </span>            : #define VM_MAYSHARE     0x00000080
<span class="lineNum">     111 </span>            : 
<span class="lineNum">     112 </span>            : #define VM_GROWSDOWN    0x00000100      /* general info on the segment */
<span class="lineNum">     113 </span>            : #define VM_PFNMAP       0x00000400      /* Page-ranges managed without &quot;struct page&quot;, just pure PFN */
<span class="lineNum">     114 </span>            : #define VM_DENYWRITE    0x00000800      /* ETXTBSY on write attempts.. */
<span class="lineNum">     115 </span>            : 
<span class="lineNum">     116 </span>            : #define VM_LOCKED       0x00002000
<span class="lineNum">     117 </span>            : #define VM_IO           0x00004000      /* Memory mapped I/O or similar */
<span class="lineNum">     118 </span>            : 
<span class="lineNum">     119 </span>            :                                         /* Used by sys_madvise() */
<span class="lineNum">     120 </span>            : #define VM_SEQ_READ     0x00008000      /* App will access data sequentially */
<span class="lineNum">     121 </span>            : #define VM_RAND_READ    0x00010000      /* App will not benefit from clustered reads */
<span class="lineNum">     122 </span>            : 
<span class="lineNum">     123 </span>            : #define VM_DONTCOPY     0x00020000      /* Do not copy this vma on fork */
<span class="lineNum">     124 </span>            : #define VM_DONTEXPAND   0x00040000      /* Cannot expand with mremap() */
<span class="lineNum">     125 </span>            : #define VM_ACCOUNT      0x00100000      /* Is a VM accounted object */
<span class="lineNum">     126 </span>            : #define VM_NORESERVE    0x00200000      /* should the VM suppress accounting */
<span class="lineNum">     127 </span>            : #define VM_HUGETLB      0x00400000      /* Huge TLB Page VM */
<span class="lineNum">     128 </span>            : #define VM_NONLINEAR    0x00800000      /* Is non-linear (remap_file_pages) */
<span class="lineNum">     129 </span>            : #define VM_ARCH_1       0x01000000      /* Architecture-specific flag */
<span class="lineNum">     130 </span>            : #define VM_DONTDUMP     0x04000000      /* Do not include in the core dump */
<span class="lineNum">     131 </span>            : 
<span class="lineNum">     132 </span>            : #ifdef CONFIG_MEM_SOFT_DIRTY
<span class="lineNum">     133 </span>            : # define VM_SOFTDIRTY   0x08000000      /* Not soft dirty clean area */
<span class="lineNum">     134 </span>            : #else
<span class="lineNum">     135 </span>            : # define VM_SOFTDIRTY   0
<span class="lineNum">     136 </span>            : #endif
<span class="lineNum">     137 </span>            : 
<span class="lineNum">     138 </span>            : #define VM_MIXEDMAP     0x10000000      /* Can contain &quot;struct page&quot; and pure PFN pages */
<span class="lineNum">     139 </span>            : #define VM_HUGEPAGE     0x20000000      /* MADV_HUGEPAGE marked this vma */
<span class="lineNum">     140 </span>            : #define VM_NOHUGEPAGE   0x40000000      /* MADV_NOHUGEPAGE marked this vma */
<span class="lineNum">     141 </span>            : #define VM_MERGEABLE    0x80000000      /* KSM may merge identical pages */
<span class="lineNum">     142 </span>            : 
<span class="lineNum">     143 </span>            : #if defined(CONFIG_X86)
<span class="lineNum">     144 </span>            : # define VM_PAT         VM_ARCH_1       /* PAT reserves whole VMA at once (x86) */
<span class="lineNum">     145 </span>            : #elif defined(CONFIG_PPC)
<span class="lineNum">     146 </span>            : # define VM_SAO         VM_ARCH_1       /* Strong Access Ordering (powerpc) */
<span class="lineNum">     147 </span>            : #elif defined(CONFIG_PARISC)
<span class="lineNum">     148 </span>            : # define VM_GROWSUP     VM_ARCH_1
<span class="lineNum">     149 </span>            : #elif defined(CONFIG_METAG)
<span class="lineNum">     150 </span>            : # define VM_GROWSUP     VM_ARCH_1
<span class="lineNum">     151 </span>            : #elif defined(CONFIG_IA64)
<span class="lineNum">     152 </span>            : # define VM_GROWSUP     VM_ARCH_1
<span class="lineNum">     153 </span>            : #elif !defined(CONFIG_MMU)
<span class="lineNum">     154 </span>            : # define VM_MAPPED_COPY VM_ARCH_1       /* T if mapped copy of data (nommu mmap) */
<span class="lineNum">     155 </span>            : #endif
<span class="lineNum">     156 </span>            : 
<span class="lineNum">     157 </span>            : #ifndef VM_GROWSUP
<span class="lineNum">     158 </span>            : # define VM_GROWSUP     VM_NONE
<span class="lineNum">     159 </span>            : #endif
<span class="lineNum">     160 </span>            : 
<span class="lineNum">     161 </span>            : /* Bits set in the VMA until the stack is in its final location */
<span class="lineNum">     162 </span>            : #define VM_STACK_INCOMPLETE_SETUP       (VM_RAND_READ | VM_SEQ_READ)
<span class="lineNum">     163 </span>            : 
<span class="lineNum">     164 </span>            : #ifndef VM_STACK_DEFAULT_FLAGS          /* arch can override this */
<span class="lineNum">     165 </span>            : #define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS
<span class="lineNum">     166 </span>            : #endif
<span class="lineNum">     167 </span>            : 
<span class="lineNum">     168 </span>            : #ifdef CONFIG_STACK_GROWSUP
<span class="lineNum">     169 </span>            : #define VM_STACK_FLAGS  (VM_GROWSUP | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)
<span class="lineNum">     170 </span>            : #else
<span class="lineNum">     171 </span>            : #define VM_STACK_FLAGS  (VM_GROWSDOWN | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)
<span class="lineNum">     172 </span>            : #endif
<span class="lineNum">     173 </span>            : 
<span class="lineNum">     174 </span>            : /*
<span class="lineNum">     175 </span>            :  * Special vmas that are non-mergable, non-mlock()able.
<span class="lineNum">     176 </span>            :  * Note: mm/huge_memory.c VM_NO_THP depends on this definition.
<span class="lineNum">     177 </span>            :  */
<span class="lineNum">     178 </span>            : #define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_PFNMAP | VM_MIXEDMAP)
<span class="lineNum">     179 </span>            : 
<span class="lineNum">     180 </span>            : /* This mask defines which mm-&gt;def_flags a process can inherit its parent */
<span class="lineNum">     181 </span>            : #define VM_INIT_DEF_MASK        VM_NOHUGEPAGE
<span class="lineNum">     182 </span>            : 
<span class="lineNum">     183 </span>            : /*
<span class="lineNum">     184 </span>            :  * mapping from the currently active vm_flags protection bits (the
<span class="lineNum">     185 </span>            :  * low four bits) to a page protection mask..
<span class="lineNum">     186 </span>            :  */
<span class="lineNum">     187 </span>            : extern pgprot_t protection_map[16];
<span class="lineNum">     188 </span>            : 
<span class="lineNum">     189 </span>            : #define FAULT_FLAG_WRITE        0x01    /* Fault was a write access */
<span class="lineNum">     190 </span>            : #define FAULT_FLAG_NONLINEAR    0x02    /* Fault was via a nonlinear mapping */
<span class="lineNum">     191 </span>            : #define FAULT_FLAG_MKWRITE      0x04    /* Fault was mkwrite of existing pte */
<span class="lineNum">     192 </span>            : #define FAULT_FLAG_ALLOW_RETRY  0x08    /* Retry fault if blocking */
<span class="lineNum">     193 </span>            : #define FAULT_FLAG_RETRY_NOWAIT 0x10    /* Don't drop mmap_sem and wait when retrying */
<span class="lineNum">     194 </span>            : #define FAULT_FLAG_KILLABLE     0x20    /* The fault task is in SIGKILL killable region */
<span class="lineNum">     195 </span>            : #define FAULT_FLAG_TRIED        0x40    /* second try */
<span class="lineNum">     196 </span>            : #define FAULT_FLAG_USER         0x80    /* The fault originated in userspace */
<span class="lineNum">     197 </span>            : 
<span class="lineNum">     198 </span>            : /*
<span class="lineNum">     199 </span>            :  * vm_fault is filled by the the pagefault handler and passed to the vma's
<span class="lineNum">     200 </span>            :  * -&gt;fault function. The vma's -&gt;fault is responsible for returning a bitmask
<span class="lineNum">     201 </span>            :  * of VM_FAULT_xxx flags that give details about how the fault was handled.
<span class="lineNum">     202 </span>            :  *
<span class="lineNum">     203 </span>            :  * pgoff should be used in favour of virtual_address, if possible. If pgoff
<span class="lineNum">     204 </span>            :  * is used, one may implement -&gt;remap_pages to get nonlinear mapping support.
<span class="lineNum">     205 </span>            :  */
<span class="lineNum">     206 </span>            : struct vm_fault {
<span class="lineNum">     207 </span>            :         unsigned int flags;             /* FAULT_FLAG_xxx flags */
<span class="lineNum">     208 </span>            :         pgoff_t pgoff;                  /* Logical page offset based on vma */
<span class="lineNum">     209 </span>            :         void __user *virtual_address;   /* Faulting virtual address */
<span class="lineNum">     210 </span>            : 
<span class="lineNum">     211 </span>            :         struct page *page;              /* -&gt;fault handlers should return a
<span class="lineNum">     212 </span>            :                                          * page here, unless VM_FAULT_NOPAGE
<span class="lineNum">     213 </span>            :                                          * is set (which is also implied by
<span class="lineNum">     214 </span>            :                                          * VM_FAULT_ERROR).
<span class="lineNum">     215 </span>            :                                          */
<span class="lineNum">     216 </span>            :         /* for -&gt;map_pages() only */
<span class="lineNum">     217 </span>            :         pgoff_t max_pgoff;              /* map pages for offset from pgoff till
<span class="lineNum">     218 </span>            :                                          * max_pgoff inclusive */
<span class="lineNum">     219 </span>            :         pte_t *pte;                     /* pte entry associated with -&gt;pgoff */
<span class="lineNum">     220 </span>            : };
<span class="lineNum">     221 </span>            : 
<span class="lineNum">     222 </span>            : /*
<span class="lineNum">     223 </span>            :  * These are the virtual MM functions - opening of an area, closing and
<span class="lineNum">     224 </span>            :  * unmapping it (needed to keep files on disk up-to-date etc), pointer
<span class="lineNum">     225 </span>            :  * to the functions called when a no-page or a wp-page exception occurs. 
<span class="lineNum">     226 </span>            :  */
<span class="lineNum">     227 </span>            : struct vm_operations_struct {
<span class="lineNum">     228 </span>            :         void (*open)(struct vm_area_struct * area);
<span class="lineNum">     229 </span>            :         void (*close)(struct vm_area_struct * area);
<span class="lineNum">     230 </span>            :         int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);
<span class="lineNum">     231 </span>            :         void (*map_pages)(struct vm_area_struct *vma, struct vm_fault *vmf);
<span class="lineNum">     232 </span>            : 
<span class="lineNum">     233 </span>            :         /* notification that a previously read-only page is about to become
<span class="lineNum">     234 </span>            :          * writable, if an error is returned it will cause a SIGBUS */
<span class="lineNum">     235 </span>            :         int (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);
<span class="lineNum">     236 </span>            : 
<span class="lineNum">     237 </span>            :         /* called by access_process_vm when get_user_pages() fails, typically
<span class="lineNum">     238 </span>            :          * for use by special VMAs that can switch between memory and hardware
<span class="lineNum">     239 </span>            :          */
<span class="lineNum">     240 </span>            :         int (*access)(struct vm_area_struct *vma, unsigned long addr,
<span class="lineNum">     241 </span>            :                       void *buf, int len, int write);
<span class="lineNum">     242 </span>            : 
<span class="lineNum">     243 </span>            :         /* Called by the /proc/PID/maps code to ask the vma whether it
<span class="lineNum">     244 </span>            :          * has a special name.  Returning non-NULL will also cause this
<span class="lineNum">     245 </span>            :          * vma to be dumped unconditionally. */
<span class="lineNum">     246 </span>            :         const char *(*name)(struct vm_area_struct *vma);
<span class="lineNum">     247 </span>            : 
<span class="lineNum">     248 </span>            : #ifdef CONFIG_NUMA
<span class="lineNum">     249 </span>            :         /*
<span class="lineNum">     250 </span>            :          * set_policy() op must add a reference to any non-NULL @new mempolicy
<span class="lineNum">     251 </span>            :          * to hold the policy upon return.  Caller should pass NULL @new to
<span class="lineNum">     252 </span>            :          * remove a policy and fall back to surrounding context--i.e. do not
<span class="lineNum">     253 </span>            :          * install a MPOL_DEFAULT policy, nor the task or system default
<span class="lineNum">     254 </span>            :          * mempolicy.
<span class="lineNum">     255 </span>            :          */
<span class="lineNum">     256 </span>            :         int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);
<span class="lineNum">     257 </span>            : 
<span class="lineNum">     258 </span>            :         /*
<span class="lineNum">     259 </span>            :          * get_policy() op must add reference [mpol_get()] to any policy at
<span class="lineNum">     260 </span>            :          * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure
<span class="lineNum">     261 </span>            :          * in mm/mempolicy.c will do this automatically.
<span class="lineNum">     262 </span>            :          * get_policy() must NOT add a ref if the policy at (vma,addr) is not
<span class="lineNum">     263 </span>            :          * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.
<span class="lineNum">     264 </span>            :          * If no [shared/vma] mempolicy exists at the addr, get_policy() op
<span class="lineNum">     265 </span>            :          * must return NULL--i.e., do not &quot;fallback&quot; to task or system default
<span class="lineNum">     266 </span>            :          * policy.
<span class="lineNum">     267 </span>            :          */
<span class="lineNum">     268 </span>            :         struct mempolicy *(*get_policy)(struct vm_area_struct *vma,
<span class="lineNum">     269 </span>            :                                         unsigned long addr);
<span class="lineNum">     270 </span>            :         int (*migrate)(struct vm_area_struct *vma, const nodemask_t *from,
<span class="lineNum">     271 </span>            :                 const nodemask_t *to, unsigned long flags);
<span class="lineNum">     272 </span>            : #endif
<span class="lineNum">     273 </span>            :         /* called by sys_remap_file_pages() to populate non-linear mapping */
<span class="lineNum">     274 </span>            :         int (*remap_pages)(struct vm_area_struct *vma, unsigned long addr,
<span class="lineNum">     275 </span>            :                            unsigned long size, pgoff_t pgoff);
<span class="lineNum">     276 </span>            : };
<span class="lineNum">     277 </span>            : 
<span class="lineNum">     278 </span>            : struct mmu_gather;
<span class="lineNum">     279 </span>            : struct inode;
<span class="lineNum">     280 </span>            : 
<span class="lineNum">     281 </span>            : #define page_private(page)              ((page)-&gt;private)
<span class="lineNum">     282 </span>            : #define set_page_private(page, v)       ((page)-&gt;private = (v))
<span class="lineNum">     283 </span>            : 
<span class="lineNum">     284 </span>            : /* It's valid only if the page is free path or free_list */
<span class="lineNum">     285 </span>            : static inline void set_freepage_migratetype(struct page *page, int migratetype)
<span class="lineNum">     286 </span>            : {
<span class="lineNum">     287 </span>            :         page-&gt;index = migratetype;
<span class="lineNum">     288 </span>            : }
<span class="lineNum">     289 </span>            : 
<span class="lineNum">     290 </span>            : /* It's valid only if the page is free path or free_list */
<span class="lineNum">     291 </span>            : static inline int get_freepage_migratetype(struct page *page)
<span class="lineNum">     292 </span>            : {
<span class="lineNum">     293 </span>            :         return page-&gt;index;
<span class="lineNum">     294 </span>            : }
<span class="lineNum">     295 </span>            : 
<span class="lineNum">     296 </span>            : /*
<span class="lineNum">     297 </span>            :  * FIXME: take this include out, include page-flags.h in
<span class="lineNum">     298 </span>            :  * files which need it (119 of them)
<span class="lineNum">     299 </span>            :  */
<span class="lineNum">     300 </span>            : #include &lt;linux/page-flags.h&gt;
<span class="lineNum">     301 </span>            : #include &lt;linux/huge_mm.h&gt;
<span class="lineNum">     302 </span>            : 
<span class="lineNum">     303 </span>            : /*
<span class="lineNum">     304 </span>            :  * Methods to modify the page usage count.
<span class="lineNum">     305 </span>            :  *
<span class="lineNum">     306 </span>            :  * What counts for a page usage:
<span class="lineNum">     307 </span>            :  * - cache mapping   (page-&gt;mapping)
<span class="lineNum">     308 </span>            :  * - private data    (page-&gt;private)
<span class="lineNum">     309 </span>            :  * - page mapped in a task's page tables, each mapping
<span class="lineNum">     310 </span>            :  *   is counted separately
<span class="lineNum">     311 </span>            :  *
<span class="lineNum">     312 </span>            :  * Also, many kernel routines increase the page count before a critical
<span class="lineNum">     313 </span>            :  * routine so they can be sure the page doesn't go away from under them.
<span class="lineNum">     314 </span>            :  */
<span class="lineNum">     315 </span>            : 
<span class="lineNum">     316 </span>            : /*
<span class="lineNum">     317 </span>            :  * Drop a ref, return true if the refcount fell to zero (the page has no users)
<span class="lineNum">     318 </span>            :  */
<span class="lineNum">     319 </span>            : static inline int put_page_testzero(struct page *page)
<span class="lineNum">     320 </span>            : {
<span class="lineNum">     321 </span>            :         VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) == 0, page);
<span class="lineNum">     322 </span>            :         return atomic_dec_and_test(&amp;page-&gt;_count);
<span class="lineNum">     323 </span>            : }
<span class="lineNum">     324 </span>            : 
<span class="lineNum">     325 </span>            : /*
<span class="lineNum">     326 </span>            :  * Try to grab a ref unless the page has a refcount of zero, return false if
<span class="lineNum">     327 </span>            :  * that is the case.
<span class="lineNum">     328 </span>            :  * This can be called when MMU is off so it must not access
<a name="329"><span class="lineNum">     329 </span>            :  * any of the virtual mappings.</a>
<span class="lineNum">     330 </span>            :  */
<span class="lineNum">     331 </span><span class="lineCov">         34 : static inline int get_page_unless_zero(struct page *page)</span>
<span class="lineNum">     332 </span>            : {
<span class="lineNum">     333 </span><span class="lineCov">         34 :         return atomic_inc_not_zero(&amp;page-&gt;_count);</span>
<span class="lineNum">     334 </span>            : }
<span class="lineNum">     335 </span>            : 
<span class="lineNum">     336 </span>            : /*
<span class="lineNum">     337 </span>            :  * Try to drop a ref unless the page has a refcount of one, return false if
<span class="lineNum">     338 </span>            :  * that is the case.
<span class="lineNum">     339 </span>            :  * This is to make sure that the refcount won't become zero after this drop.
<span class="lineNum">     340 </span>            :  * This can be called when MMU is off so it must not access
<span class="lineNum">     341 </span>            :  * any of the virtual mappings.
<span class="lineNum">     342 </span>            :  */
<span class="lineNum">     343 </span>            : static inline int put_page_unless_one(struct page *page)
<span class="lineNum">     344 </span>            : {
<span class="lineNum">     345 </span>            :         return atomic_add_unless(&amp;page-&gt;_count, -1, 1);
<span class="lineNum">     346 </span>            : }
<span class="lineNum">     347 </span>            : 
<span class="lineNum">     348 </span>            : extern int page_is_ram(unsigned long pfn);
<span class="lineNum">     349 </span>            : 
<span class="lineNum">     350 </span>            : /* Support for virtually mapped pages */
<span class="lineNum">     351 </span>            : struct page *vmalloc_to_page(const void *addr);
<span class="lineNum">     352 </span>            : unsigned long vmalloc_to_pfn(const void *addr);
<span class="lineNum">     353 </span>            : 
<span class="lineNum">     354 </span>            : /*
<span class="lineNum">     355 </span>            :  * Determine if an address is within the vmalloc range
<span class="lineNum">     356 </span>            :  *
<span class="lineNum">     357 </span>            :  * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there
<span class="lineNum">     358 </span>            :  * is no special casing required.
<span class="lineNum">     359 </span>            :  */
<span class="lineNum">     360 </span>            : static inline int is_vmalloc_addr(const void *x)
<span class="lineNum">     361 </span>            : {
<span class="lineNum">     362 </span>            : #ifdef CONFIG_MMU
<span class="lineNum">     363 </span><span class="lineCov">        221 :         unsigned long addr = (unsigned long)x;</span>
<span class="lineNum">     364 </span>            : 
<span class="lineNum">     365 </span><span class="lineCov">        221 :         return addr &gt;= VMALLOC_START &amp;&amp; addr &lt; VMALLOC_END;</span>
<span class="lineNum">     366 </span>            : #else
<span class="lineNum">     367 </span>            :         return 0;
<span class="lineNum">     368 </span>            : #endif
<span class="lineNum">     369 </span>            : }
<span class="lineNum">     370 </span>            : #ifdef CONFIG_MMU
<span class="lineNum">     371 </span>            : extern int is_vmalloc_or_module_addr(const void *x);
<span class="lineNum">     372 </span>            : #else
<span class="lineNum">     373 </span>            : static inline int is_vmalloc_or_module_addr(const void *x)
<span class="lineNum">     374 </span>            : {
<span class="lineNum">     375 </span>            :         return 0;
<span class="lineNum">     376 </span>            : }
<span class="lineNum">     377 </span>            : #endif
<span class="lineNum">     378 </span>            : 
<span class="lineNum">     379 </span>            : extern void kvfree(const void *addr);
<span class="lineNum">     380 </span>            : 
<span class="lineNum">     381 </span>            : static inline void compound_lock(struct page *page)
<span class="lineNum">     382 </span>            : {
<span class="lineNum">     383 </span>            : #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="lineNum">     384 </span>            :         VM_BUG_ON_PAGE(PageSlab(page), page);
<span class="lineNum">     385 </span>            :         bit_spin_lock(PG_compound_lock, &amp;page-&gt;flags);
<span class="lineNum">     386 </span>            : #endif
<span class="lineNum">     387 </span>            : }
<span class="lineNum">     388 </span>            : 
<span class="lineNum">     389 </span>            : static inline void compound_unlock(struct page *page)
<span class="lineNum">     390 </span>            : {
<span class="lineNum">     391 </span>            : #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="lineNum">     392 </span>            :         VM_BUG_ON_PAGE(PageSlab(page), page);
<span class="lineNum">     393 </span>            :         bit_spin_unlock(PG_compound_lock, &amp;page-&gt;flags);
<span class="lineNum">     394 </span>            : #endif
<span class="lineNum">     395 </span>            : }
<span class="lineNum">     396 </span>            : 
<span class="lineNum">     397 </span>            : static inline unsigned long compound_lock_irqsave(struct page *page)
<span class="lineNum">     398 </span>            : {
<span class="lineNum">     399 </span>            :         unsigned long uninitialized_var(flags);
<span class="lineNum">     400 </span>            : #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="lineNum">     401 </span>            :         local_irq_save(flags);
<span class="lineNum">     402 </span>            :         compound_lock(page);
<span class="lineNum">     403 </span>            : #endif
<span class="lineNum">     404 </span>            :         return flags;
<span class="lineNum">     405 </span>            : }
<span class="lineNum">     406 </span>            : 
<span class="lineNum">     407 </span>            : static inline void compound_unlock_irqrestore(struct page *page,
<span class="lineNum">     408 </span>            :                                               unsigned long flags)
<span class="lineNum">     409 </span>            : {
<span class="lineNum">     410 </span>            : #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="lineNum">     411 </span>            :         compound_unlock(page);
<span class="lineNum">     412 </span>            :         local_irq_restore(flags);
<span class="lineNum">     413 </span>            : #endif
<span class="lineNum">     414 </span>            : }
<span class="lineNum">     415 </span>            : 
<span class="lineNum">     416 </span>            : static inline struct page *compound_head_by_tail(struct page *tail)
<span class="lineNum">     417 </span>            : {
<span class="lineNum">     418 </span>            :         struct page *head = tail-&gt;first_page;
<span class="lineNum">     419 </span>            : 
<span class="lineNum">     420 </span>            :         /*
<span class="lineNum">     421 </span>            :          * page-&gt;first_page may be a dangling pointer to an old
<span class="lineNum">     422 </span>            :          * compound page, so recheck that it is still a tail
<span class="lineNum">     423 </span>            :          * page before returning.
<span class="lineNum">     424 </span>            :          */
<span class="lineNum">     425 </span>            :         smp_rmb();
<span class="lineNum">     426 </span>            :         if (likely(PageTail(tail)))
<span class="lineNum">     427 </span>            :                 return head;
<span class="lineNum">     428 </span>            :         return tail;
<span class="lineNum">     429 </span>            : }
<span class="lineNum">     430 </span>            : 
<span class="lineNum">     431 </span>            : static inline struct page *compound_head(struct page *page)
<span class="lineNum">     432 </span>            : {
<span class="lineNum">     433 </span>            :         if (unlikely(PageTail(page)))
<span class="lineNum">     434 </span>            :                 return compound_head_by_tail(page);
<span class="lineNum">     435 </span>            :         return page;
<span class="lineNum">     436 </span>            : }
<span class="lineNum">     437 </span>            : 
<span class="lineNum">     438 </span>            : /*
<span class="lineNum">     439 </span>            :  * The atomic page-&gt;_mapcount, starts from -1: so that transitions
<span class="lineNum">     440 </span>            :  * both from it and to it can be tracked, using atomic_inc_and_test
<span class="lineNum">     441 </span>            :  * and atomic_add_negative(-1).
<span class="lineNum">     442 </span>            :  */
<span class="lineNum">     443 </span>            : static inline void page_mapcount_reset(struct page *page)
<span class="lineNum">     444 </span>            : {
<span class="lineNum">     445 </span>            :         atomic_set(&amp;(page)-&gt;_mapcount, -1);
<span class="lineNum">     446 </span>            : }
<span class="lineNum">     447 </span>            : 
<span class="lineNum">     448 </span>            : static inline int page_mapcount(struct page *page)
<span class="lineNum">     449 </span>            : {
<span class="lineNum">     450 </span>            :         return atomic_read(&amp;(page)-&gt;_mapcount) + 1;
<span class="lineNum">     451 </span>            : }
<span class="lineNum">     452 </span>            : 
<span class="lineNum">     453 </span>            : static inline int page_count(struct page *page)
<span class="lineNum">     454 </span>            : {
<span class="lineNum">     455 </span>            :         return atomic_read(&amp;compound_head(page)-&gt;_count);
<span class="lineNum">     456 </span>            : }
<span class="lineNum">     457 </span>            : 
<span class="lineNum">     458 </span>            : #ifdef CONFIG_HUGETLB_PAGE
<span class="lineNum">     459 </span>            : extern int PageHeadHuge(struct page *page_head);
<span class="lineNum">     460 </span>            : #else /* CONFIG_HUGETLB_PAGE */
<span class="lineNum">     461 </span>            : static inline int PageHeadHuge(struct page *page_head)
<span class="lineNum">     462 </span>            : {
<span class="lineNum">     463 </span>            :         return 0;
<span class="lineNum">     464 </span>            : }
<span class="lineNum">     465 </span>            : #endif /* CONFIG_HUGETLB_PAGE */
<span class="lineNum">     466 </span>            : 
<span class="lineNum">     467 </span>            : static inline bool __compound_tail_refcounted(struct page *page)
<span class="lineNum">     468 </span>            : {
<span class="lineNum">     469 </span>            :         return !PageSlab(page) &amp;&amp; !PageHeadHuge(page);
<span class="lineNum">     470 </span>            : }
<span class="lineNum">     471 </span>            : 
<span class="lineNum">     472 </span>            : /*
<span class="lineNum">     473 </span>            :  * This takes a head page as parameter and tells if the
<span class="lineNum">     474 </span>            :  * tail page reference counting can be skipped.
<span class="lineNum">     475 </span>            :  *
<span class="lineNum">     476 </span>            :  * For this to be safe, PageSlab and PageHeadHuge must remain true on
<span class="lineNum">     477 </span>            :  * any given page where they return true here, until all tail pins
<span class="lineNum">     478 </span>            :  * have been released.
<span class="lineNum">     479 </span>            :  */
<span class="lineNum">     480 </span>            : static inline bool compound_tail_refcounted(struct page *page)
<span class="lineNum">     481 </span>            : {
<span class="lineNum">     482 </span>            :         VM_BUG_ON_PAGE(!PageHead(page), page);
<span class="lineNum">     483 </span>            :         return __compound_tail_refcounted(page);
<span class="lineNum">     484 </span>            : }
<span class="lineNum">     485 </span>            : 
<span class="lineNum">     486 </span>            : static inline void get_huge_page_tail(struct page *page)
<span class="lineNum">     487 </span>            : {
<span class="lineNum">     488 </span>            :         /*
<span class="lineNum">     489 </span>            :          * __split_huge_page_refcount() cannot run from under us.
<span class="lineNum">     490 </span>            :          */
<span class="lineNum">     491 </span>            :         VM_BUG_ON_PAGE(!PageTail(page), page);
<span class="lineNum">     492 </span>            :         VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);
<span class="lineNum">     493 </span>            :         VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) != 0, page);
<span class="lineNum">     494 </span>            :         if (compound_tail_refcounted(page-&gt;first_page))
<span class="lineNum">     495 </span>            :                 atomic_inc(&amp;page-&gt;_mapcount);
<span class="lineNum">     496 </span>            : }
<span class="lineNum">     497 </span>            : 
<a name="498"><span class="lineNum">     498 </span>            : extern bool __get_page_tail(struct page *page);</a>
<span class="lineNum">     499 </span>            : 
<span class="lineNum">     500 </span><span class="lineCov">    1949891 : static inline void get_page(struct page *page)</span>
<span class="lineNum">     501 </span>            : {
<span class="lineNum">     502 </span><span class="lineCov">    1949891 :         if (unlikely(PageTail(page)))</span>
<span class="lineNum">     503 </span><span class="lineNoCov">          0 :                 if (likely(__get_page_tail(page)))</span>
<span class="lineNum">     504 </span><span class="lineCov">    1949897 :                         return;</span>
<span class="lineNum">     505 </span>            :         /*
<span class="lineNum">     506 </span>            :          * Getting a normal page or the head of a compound page
<span class="lineNum">     507 </span>            :          * requires to already have an elevated page-&gt;_count.
<span class="lineNum">     508 </span>            :          */
<span class="lineNum">     509 </span><span class="lineCov">    1949890 :         VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) &lt;= 0, page);</span>
<span class="lineNum">     510 </span><span class="lineCov">    1949890 :         atomic_inc(&amp;page-&gt;_count);</span>
<span class="lineNum">     511 </span>            : }
<span class="lineNum">     512 </span>            : 
<span class="lineNum">     513 </span>            : static inline struct page *virt_to_head_page(const void *x)
<span class="lineNum">     514 </span>            : {
<span class="lineNum">     515 </span>            :         struct page *page = virt_to_page(x);
<span class="lineNum">     516 </span>            :         return compound_head(page);
<span class="lineNum">     517 </span>            : }
<span class="lineNum">     518 </span>            : 
<span class="lineNum">     519 </span>            : /*
<span class="lineNum">     520 </span>            :  * Setup the page count before being freed into the page allocator for
<span class="lineNum">     521 </span>            :  * the first time (boot or memory hotplug)
<span class="lineNum">     522 </span>            :  */
<span class="lineNum">     523 </span>            : static inline void init_page_count(struct page *page)
<span class="lineNum">     524 </span>            : {
<span class="lineNum">     525 </span>            :         atomic_set(&amp;page-&gt;_count, 1);
<span class="lineNum">     526 </span>            : }
<span class="lineNum">     527 </span>            : 
<span class="lineNum">     528 </span>            : /*
<span class="lineNum">     529 </span>            :  * PageBuddy() indicate that the page is free and in the buddy system
<span class="lineNum">     530 </span>            :  * (see mm/page_alloc.c).
<span class="lineNum">     531 </span>            :  *
<span class="lineNum">     532 </span>            :  * PAGE_BUDDY_MAPCOUNT_VALUE must be &lt;= -2 but better not too close to
<span class="lineNum">     533 </span>            :  * -2 so that an underflow of the page_mapcount() won't be mistaken
<span class="lineNum">     534 </span>            :  * for a genuine PAGE_BUDDY_MAPCOUNT_VALUE. -128 can be created very
<span class="lineNum">     535 </span>            :  * efficiently by most CPU architectures.
<span class="lineNum">     536 </span>            :  */
<span class="lineNum">     537 </span>            : #define PAGE_BUDDY_MAPCOUNT_VALUE (-128)
<span class="lineNum">     538 </span>            : 
<span class="lineNum">     539 </span>            : static inline int PageBuddy(struct page *page)
<span class="lineNum">     540 </span>            : {
<span class="lineNum">     541 </span>            :         return atomic_read(&amp;page-&gt;_mapcount) == PAGE_BUDDY_MAPCOUNT_VALUE;
<span class="lineNum">     542 </span>            : }
<span class="lineNum">     543 </span>            : 
<span class="lineNum">     544 </span>            : static inline void __SetPageBuddy(struct page *page)
<span class="lineNum">     545 </span>            : {
<span class="lineNum">     546 </span>            :         VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_mapcount) != -1, page);
<span class="lineNum">     547 </span>            :         atomic_set(&amp;page-&gt;_mapcount, PAGE_BUDDY_MAPCOUNT_VALUE);
<span class="lineNum">     548 </span>            : }
<span class="lineNum">     549 </span>            : 
<span class="lineNum">     550 </span>            : static inline void __ClearPageBuddy(struct page *page)
<span class="lineNum">     551 </span>            : {
<span class="lineNum">     552 </span>            :         VM_BUG_ON_PAGE(!PageBuddy(page), page);
<span class="lineNum">     553 </span>            :         atomic_set(&amp;page-&gt;_mapcount, -1);
<span class="lineNum">     554 </span>            : }
<span class="lineNum">     555 </span>            : 
<span class="lineNum">     556 </span>            : void put_page(struct page *page);
<span class="lineNum">     557 </span>            : void put_pages_list(struct list_head *pages);
<span class="lineNum">     558 </span>            : 
<span class="lineNum">     559 </span>            : void split_page(struct page *page, unsigned int order);
<span class="lineNum">     560 </span>            : int split_free_page(struct page *page);
<span class="lineNum">     561 </span>            : 
<span class="lineNum">     562 </span>            : /*
<span class="lineNum">     563 </span>            :  * Compound pages have a destructor function.  Provide a
<span class="lineNum">     564 </span>            :  * prototype for that function and accessor functions.
<span class="lineNum">     565 </span>            :  * These are _only_ valid on the head of a PG_compound page.
<span class="lineNum">     566 </span>            :  */
<span class="lineNum">     567 </span>            : typedef void compound_page_dtor(struct page *);
<span class="lineNum">     568 </span>            : 
<span class="lineNum">     569 </span>            : static inline void set_compound_page_dtor(struct page *page,
<span class="lineNum">     570 </span>            :                                                 compound_page_dtor *dtor)
<span class="lineNum">     571 </span>            : {
<span class="lineNum">     572 </span>            :         page[1].lru.next = (void *)dtor;
<span class="lineNum">     573 </span>            : }
<span class="lineNum">     574 </span>            : 
<span class="lineNum">     575 </span>            : static inline compound_page_dtor *get_compound_page_dtor(struct page *page)
<span class="lineNum">     576 </span>            : {
<span class="lineNum">     577 </span>            :         return (compound_page_dtor *)page[1].lru.next;
<span class="lineNum">     578 </span>            : }
<span class="lineNum">     579 </span>            : 
<span class="lineNum">     580 </span>            : static inline int compound_order(struct page *page)
<span class="lineNum">     581 </span>            : {
<span class="lineNum">     582 </span>            :         if (!PageHead(page))
<span class="lineNum">     583 </span>            :                 return 0;
<span class="lineNum">     584 </span>            :         return (unsigned long)page[1].lru.prev;
<span class="lineNum">     585 </span>            : }
<span class="lineNum">     586 </span>            : 
<span class="lineNum">     587 </span>            : static inline void set_compound_order(struct page *page, unsigned long order)
<span class="lineNum">     588 </span>            : {
<span class="lineNum">     589 </span>            :         page[1].lru.prev = (void *)order;
<span class="lineNum">     590 </span>            : }
<span class="lineNum">     591 </span>            : 
<span class="lineNum">     592 </span>            : #ifdef CONFIG_MMU
<span class="lineNum">     593 </span>            : /*
<span class="lineNum">     594 </span>            :  * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when
<span class="lineNum">     595 </span>            :  * servicing faults for write access.  In the normal case, do always want
<span class="lineNum">     596 </span>            :  * pte_mkwrite.  But get_user_pages can cause write faults for mappings
<span class="lineNum">     597 </span>            :  * that do not have writing enabled, when used by access_process_vm.
<span class="lineNum">     598 </span>            :  */
<span class="lineNum">     599 </span>            : static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
<span class="lineNum">     600 </span>            : {
<span class="lineNum">     601 </span>            :         if (likely(vma-&gt;vm_flags &amp; VM_WRITE))
<span class="lineNum">     602 </span>            :                 pte = pte_mkwrite(pte);
<span class="lineNum">     603 </span>            :         return pte;
<span class="lineNum">     604 </span>            : }
<span class="lineNum">     605 </span>            : 
<span class="lineNum">     606 </span>            : void do_set_pte(struct vm_area_struct *vma, unsigned long address,
<span class="lineNum">     607 </span>            :                 struct page *page, pte_t *pte, bool write, bool anon);
<span class="lineNum">     608 </span>            : #endif
<span class="lineNum">     609 </span>            : 
<span class="lineNum">     610 </span>            : /*
<span class="lineNum">     611 </span>            :  * Multiple processes may &quot;see&quot; the same page. E.g. for untouched
<span class="lineNum">     612 </span>            :  * mappings of /dev/null, all processes see the same page full of
<span class="lineNum">     613 </span>            :  * zeroes, and text pages of executables and shared libraries have
<span class="lineNum">     614 </span>            :  * only one copy in memory, at most, normally.
<span class="lineNum">     615 </span>            :  *
<span class="lineNum">     616 </span>            :  * For the non-reserved pages, page_count(page) denotes a reference count.
<span class="lineNum">     617 </span>            :  *   page_count() == 0 means the page is free. page-&gt;lru is then used for
<span class="lineNum">     618 </span>            :  *   freelist management in the buddy allocator.
<span class="lineNum">     619 </span>            :  *   page_count() &gt; 0  means the page has been allocated.
<span class="lineNum">     620 </span>            :  *
<span class="lineNum">     621 </span>            :  * Pages are allocated by the slab allocator in order to provide memory
<span class="lineNum">     622 </span>            :  * to kmalloc and kmem_cache_alloc. In this case, the management of the
<span class="lineNum">     623 </span>            :  * page, and the fields in 'struct page' are the responsibility of mm/slab.c
<span class="lineNum">     624 </span>            :  * unless a particular usage is carefully commented. (the responsibility of
<span class="lineNum">     625 </span>            :  * freeing the kmalloc memory is the caller's, of course).
<span class="lineNum">     626 </span>            :  *
<span class="lineNum">     627 </span>            :  * A page may be used by anyone else who does a __get_free_page().
<span class="lineNum">     628 </span>            :  * In this case, page_count still tracks the references, and should only
<span class="lineNum">     629 </span>            :  * be used through the normal accessor functions. The top bits of page-&gt;flags
<span class="lineNum">     630 </span>            :  * and page-&gt;virtual store page management information, but all other fields
<span class="lineNum">     631 </span>            :  * are unused and could be used privately, carefully. The management of this
<span class="lineNum">     632 </span>            :  * page is the responsibility of the one who allocated it, and those who have
<span class="lineNum">     633 </span>            :  * subsequently been given references to it.
<span class="lineNum">     634 </span>            :  *
<span class="lineNum">     635 </span>            :  * The other pages (we may call them &quot;pagecache pages&quot;) are completely
<span class="lineNum">     636 </span>            :  * managed by the Linux memory manager: I/O, buffers, swapping etc.
<span class="lineNum">     637 </span>            :  * The following discussion applies only to them.
<span class="lineNum">     638 </span>            :  *
<span class="lineNum">     639 </span>            :  * A pagecache page contains an opaque `private' member, which belongs to the
<span class="lineNum">     640 </span>            :  * page's address_space. Usually, this is the address of a circular list of
<span class="lineNum">     641 </span>            :  * the page's disk buffers. PG_private must be set to tell the VM to call
<span class="lineNum">     642 </span>            :  * into the filesystem to release these pages.
<span class="lineNum">     643 </span>            :  *
<span class="lineNum">     644 </span>            :  * A page may belong to an inode's memory mapping. In this case, page-&gt;mapping
<span class="lineNum">     645 </span>            :  * is the pointer to the inode, and page-&gt;index is the file offset of the page,
<span class="lineNum">     646 </span>            :  * in units of PAGE_CACHE_SIZE.
<span class="lineNum">     647 </span>            :  *
<span class="lineNum">     648 </span>            :  * If pagecache pages are not associated with an inode, they are said to be
<span class="lineNum">     649 </span>            :  * anonymous pages. These may become associated with the swapcache, and in that
<span class="lineNum">     650 </span>            :  * case PG_swapcache is set, and page-&gt;private is an offset into the swapcache.
<span class="lineNum">     651 </span>            :  *
<span class="lineNum">     652 </span>            :  * In either case (swapcache or inode backed), the pagecache itself holds one
<span class="lineNum">     653 </span>            :  * reference to the page. Setting PG_private should also increment the
<span class="lineNum">     654 </span>            :  * refcount. The each user mapping also has a reference to the page.
<span class="lineNum">     655 </span>            :  *
<span class="lineNum">     656 </span>            :  * The pagecache pages are stored in a per-mapping radix tree, which is
<span class="lineNum">     657 </span>            :  * rooted at mapping-&gt;page_tree, and indexed by offset.
<span class="lineNum">     658 </span>            :  * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space
<span class="lineNum">     659 </span>            :  * lists, we instead now tag pages as dirty/writeback in the radix tree.
<span class="lineNum">     660 </span>            :  *
<span class="lineNum">     661 </span>            :  * All pagecache pages may be subject to I/O:
<span class="lineNum">     662 </span>            :  * - inode pages may need to be read from disk,
<span class="lineNum">     663 </span>            :  * - inode pages which have been modified and are MAP_SHARED may need
<span class="lineNum">     664 </span>            :  *   to be written back to the inode on disk,
<span class="lineNum">     665 </span>            :  * - anonymous pages (including MAP_PRIVATE file mappings) which have been
<span class="lineNum">     666 </span>            :  *   modified may need to be swapped out to swap space and (later) to be read
<span class="lineNum">     667 </span>            :  *   back into memory.
<span class="lineNum">     668 </span>            :  */
<span class="lineNum">     669 </span>            : 
<span class="lineNum">     670 </span>            : /*
<span class="lineNum">     671 </span>            :  * The zone field is never updated after free_area_init_core()
<span class="lineNum">     672 </span>            :  * sets it, so none of the operations on it need to be atomic.
<span class="lineNum">     673 </span>            :  */
<span class="lineNum">     674 </span>            : 
<span class="lineNum">     675 </span>            : /* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */
<span class="lineNum">     676 </span>            : #define SECTIONS_PGOFF          ((sizeof(unsigned long)*8) - SECTIONS_WIDTH)
<span class="lineNum">     677 </span>            : #define NODES_PGOFF             (SECTIONS_PGOFF - NODES_WIDTH)
<span class="lineNum">     678 </span>            : #define ZONES_PGOFF             (NODES_PGOFF - ZONES_WIDTH)
<span class="lineNum">     679 </span>            : #define LAST_CPUPID_PGOFF       (ZONES_PGOFF - LAST_CPUPID_WIDTH)
<span class="lineNum">     680 </span>            : 
<span class="lineNum">     681 </span>            : /*
<span class="lineNum">     682 </span>            :  * Define the bit shifts to access each section.  For non-existent
<span class="lineNum">     683 </span>            :  * sections we define the shift as 0; that plus a 0 mask ensures
<span class="lineNum">     684 </span>            :  * the compiler will optimise away reference to them.
<span class="lineNum">     685 </span>            :  */
<span class="lineNum">     686 </span>            : #define SECTIONS_PGSHIFT        (SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))
<span class="lineNum">     687 </span>            : #define NODES_PGSHIFT           (NODES_PGOFF * (NODES_WIDTH != 0))
<span class="lineNum">     688 </span>            : #define ZONES_PGSHIFT           (ZONES_PGOFF * (ZONES_WIDTH != 0))
<span class="lineNum">     689 </span>            : #define LAST_CPUPID_PGSHIFT     (LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))
<span class="lineNum">     690 </span>            : 
<span class="lineNum">     691 </span>            : /* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */
<span class="lineNum">     692 </span>            : #ifdef NODE_NOT_IN_PAGE_FLAGS
<span class="lineNum">     693 </span>            : #define ZONEID_SHIFT            (SECTIONS_SHIFT + ZONES_SHIFT)
<span class="lineNum">     694 </span>            : #define ZONEID_PGOFF            ((SECTIONS_PGOFF &lt; ZONES_PGOFF)? \
<span class="lineNum">     695 </span>            :                                                 SECTIONS_PGOFF : ZONES_PGOFF)
<span class="lineNum">     696 </span>            : #else
<span class="lineNum">     697 </span>            : #define ZONEID_SHIFT            (NODES_SHIFT + ZONES_SHIFT)
<span class="lineNum">     698 </span>            : #define ZONEID_PGOFF            ((NODES_PGOFF &lt; ZONES_PGOFF)? \
<span class="lineNum">     699 </span>            :                                                 NODES_PGOFF : ZONES_PGOFF)
<span class="lineNum">     700 </span>            : #endif
<span class="lineNum">     701 </span>            : 
<span class="lineNum">     702 </span>            : #define ZONEID_PGSHIFT          (ZONEID_PGOFF * (ZONEID_SHIFT != 0))
<span class="lineNum">     703 </span>            : 
<span class="lineNum">     704 </span>            : #if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH &gt; BITS_PER_LONG - NR_PAGEFLAGS
<span class="lineNum">     705 </span>            : #error SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH &gt; BITS_PER_LONG - NR_PAGEFLAGS
<span class="lineNum">     706 </span>            : #endif
<span class="lineNum">     707 </span>            : 
<span class="lineNum">     708 </span>            : #define ZONES_MASK              ((1UL &lt;&lt; ZONES_WIDTH) - 1)
<span class="lineNum">     709 </span>            : #define NODES_MASK              ((1UL &lt;&lt; NODES_WIDTH) - 1)
<span class="lineNum">     710 </span>            : #define SECTIONS_MASK           ((1UL &lt;&lt; SECTIONS_WIDTH) - 1)
<span class="lineNum">     711 </span>            : #define LAST_CPUPID_MASK        ((1UL &lt;&lt; LAST_CPUPID_SHIFT) - 1)
<span class="lineNum">     712 </span>            : #define ZONEID_MASK             ((1UL &lt;&lt; ZONEID_SHIFT) - 1)
<span class="lineNum">     713 </span>            : 
<span class="lineNum">     714 </span>            : static inline enum zone_type page_zonenum(const struct page *page)
<span class="lineNum">     715 </span>            : {
<span class="lineNum">     716 </span>            :         return (page-&gt;flags &gt;&gt; ZONES_PGSHIFT) &amp; ZONES_MASK;
<span class="lineNum">     717 </span>            : }
<span class="lineNum">     718 </span>            : 
<span class="lineNum">     719 </span>            : #if defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)
<span class="lineNum">     720 </span>            : #define SECTION_IN_PAGE_FLAGS
<span class="lineNum">     721 </span>            : #endif
<span class="lineNum">     722 </span>            : 
<span class="lineNum">     723 </span>            : /*
<span class="lineNum">     724 </span>            :  * The identification function is mainly used by the buddy allocator for
<span class="lineNum">     725 </span>            :  * determining if two pages could be buddies. We are not really identifying
<span class="lineNum">     726 </span>            :  * the zone since we could be using the section number id if we do not have
<span class="lineNum">     727 </span>            :  * node id available in page flags.
<span class="lineNum">     728 </span>            :  * We only guarantee that it will return the same value for two combinable
<span class="lineNum">     729 </span>            :  * pages in a zone.
<span class="lineNum">     730 </span>            :  */
<span class="lineNum">     731 </span>            : static inline int page_zone_id(struct page *page)
<span class="lineNum">     732 </span>            : {
<span class="lineNum">     733 </span>            :         return (page-&gt;flags &gt;&gt; ZONEID_PGSHIFT) &amp; ZONEID_MASK;
<span class="lineNum">     734 </span>            : }
<span class="lineNum">     735 </span>            : 
<span class="lineNum">     736 </span>            : static inline int zone_to_nid(struct zone *zone)
<span class="lineNum">     737 </span>            : {
<span class="lineNum">     738 </span>            : #ifdef CONFIG_NUMA
<span class="lineNum">     739 </span>            :         return zone-&gt;node;
<span class="lineNum">     740 </span>            : #else
<span class="lineNum">     741 </span>            :         return 0;
<span class="lineNum">     742 </span>            : #endif
<span class="lineNum">     743 </span>            : }
<span class="lineNum">     744 </span>            : 
<span class="lineNum">     745 </span>            : #ifdef NODE_NOT_IN_PAGE_FLAGS
<span class="lineNum">     746 </span>            : extern int page_to_nid(const struct page *page);
<span class="lineNum">     747 </span>            : #else
<span class="lineNum">     748 </span>            : static inline int page_to_nid(const struct page *page)
<span class="lineNum">     749 </span>            : {
<span class="lineNum">     750 </span>            :         return (page-&gt;flags &gt;&gt; NODES_PGSHIFT) &amp; NODES_MASK;
<span class="lineNum">     751 </span>            : }
<span class="lineNum">     752 </span>            : #endif
<span class="lineNum">     753 </span>            : 
<span class="lineNum">     754 </span>            : #ifdef CONFIG_NUMA_BALANCING
<span class="lineNum">     755 </span>            : static inline int cpu_pid_to_cpupid(int cpu, int pid)
<span class="lineNum">     756 </span>            : {
<span class="lineNum">     757 </span>            :         return ((cpu &amp; LAST__CPU_MASK) &lt;&lt; LAST__PID_SHIFT) | (pid &amp; LAST__PID_MASK);
<span class="lineNum">     758 </span>            : }
<span class="lineNum">     759 </span>            : 
<span class="lineNum">     760 </span>            : static inline int cpupid_to_pid(int cpupid)
<span class="lineNum">     761 </span>            : {
<span class="lineNum">     762 </span>            :         return cpupid &amp; LAST__PID_MASK;
<span class="lineNum">     763 </span>            : }
<span class="lineNum">     764 </span>            : 
<span class="lineNum">     765 </span>            : static inline int cpupid_to_cpu(int cpupid)
<span class="lineNum">     766 </span>            : {
<span class="lineNum">     767 </span>            :         return (cpupid &gt;&gt; LAST__PID_SHIFT) &amp; LAST__CPU_MASK;
<span class="lineNum">     768 </span>            : }
<span class="lineNum">     769 </span>            : 
<span class="lineNum">     770 </span>            : static inline int cpupid_to_nid(int cpupid)
<span class="lineNum">     771 </span>            : {
<span class="lineNum">     772 </span>            :         return cpu_to_node(cpupid_to_cpu(cpupid));
<span class="lineNum">     773 </span>            : }
<span class="lineNum">     774 </span>            : 
<span class="lineNum">     775 </span>            : static inline bool cpupid_pid_unset(int cpupid)
<span class="lineNum">     776 </span>            : {
<span class="lineNum">     777 </span>            :         return cpupid_to_pid(cpupid) == (-1 &amp; LAST__PID_MASK);
<span class="lineNum">     778 </span>            : }
<span class="lineNum">     779 </span>            : 
<span class="lineNum">     780 </span>            : static inline bool cpupid_cpu_unset(int cpupid)
<span class="lineNum">     781 </span>            : {
<span class="lineNum">     782 </span>            :         return cpupid_to_cpu(cpupid) == (-1 &amp; LAST__CPU_MASK);
<span class="lineNum">     783 </span>            : }
<span class="lineNum">     784 </span>            : 
<span class="lineNum">     785 </span>            : static inline bool __cpupid_match_pid(pid_t task_pid, int cpupid)
<span class="lineNum">     786 </span>            : {
<span class="lineNum">     787 </span>            :         return (task_pid &amp; LAST__PID_MASK) == cpupid_to_pid(cpupid);
<span class="lineNum">     788 </span>            : }
<span class="lineNum">     789 </span>            : 
<span class="lineNum">     790 </span>            : #define cpupid_match_pid(task, cpupid) __cpupid_match_pid(task-&gt;pid, cpupid)
<span class="lineNum">     791 </span>            : #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
<span class="lineNum">     792 </span>            : static inline int page_cpupid_xchg_last(struct page *page, int cpupid)
<span class="lineNum">     793 </span>            : {
<span class="lineNum">     794 </span>            :         return xchg(&amp;page-&gt;_last_cpupid, cpupid &amp; LAST_CPUPID_MASK);
<span class="lineNum">     795 </span>            : }
<span class="lineNum">     796 </span>            : 
<span class="lineNum">     797 </span>            : static inline int page_cpupid_last(struct page *page)
<span class="lineNum">     798 </span>            : {
<span class="lineNum">     799 </span>            :         return page-&gt;_last_cpupid;
<span class="lineNum">     800 </span>            : }
<span class="lineNum">     801 </span>            : static inline void page_cpupid_reset_last(struct page *page)
<span class="lineNum">     802 </span>            : {
<span class="lineNum">     803 </span>            :         page-&gt;_last_cpupid = -1 &amp; LAST_CPUPID_MASK;
<span class="lineNum">     804 </span>            : }
<span class="lineNum">     805 </span>            : #else
<span class="lineNum">     806 </span>            : static inline int page_cpupid_last(struct page *page)
<span class="lineNum">     807 </span>            : {
<span class="lineNum">     808 </span>            :         return (page-&gt;flags &gt;&gt; LAST_CPUPID_PGSHIFT) &amp; LAST_CPUPID_MASK;
<span class="lineNum">     809 </span>            : }
<span class="lineNum">     810 </span>            : 
<span class="lineNum">     811 </span>            : extern int page_cpupid_xchg_last(struct page *page, int cpupid);
<span class="lineNum">     812 </span>            : 
<span class="lineNum">     813 </span>            : static inline void page_cpupid_reset_last(struct page *page)
<span class="lineNum">     814 </span>            : {
<span class="lineNum">     815 </span>            :         int cpupid = (1 &lt;&lt; LAST_CPUPID_SHIFT) - 1;
<span class="lineNum">     816 </span>            : 
<span class="lineNum">     817 </span>            :         page-&gt;flags &amp;= ~(LAST_CPUPID_MASK &lt;&lt; LAST_CPUPID_PGSHIFT);
<span class="lineNum">     818 </span>            :         page-&gt;flags |= (cpupid &amp; LAST_CPUPID_MASK) &lt;&lt; LAST_CPUPID_PGSHIFT;
<span class="lineNum">     819 </span>            : }
<span class="lineNum">     820 </span>            : #endif /* LAST_CPUPID_NOT_IN_PAGE_FLAGS */
<span class="lineNum">     821 </span>            : #else /* !CONFIG_NUMA_BALANCING */
<span class="lineNum">     822 </span>            : static inline int page_cpupid_xchg_last(struct page *page, int cpupid)
<span class="lineNum">     823 </span>            : {
<span class="lineNum">     824 </span>            :         return page_to_nid(page); /* XXX */
<span class="lineNum">     825 </span>            : }
<span class="lineNum">     826 </span>            : 
<span class="lineNum">     827 </span>            : static inline int page_cpupid_last(struct page *page)
<span class="lineNum">     828 </span>            : {
<span class="lineNum">     829 </span>            :         return page_to_nid(page); /* XXX */
<span class="lineNum">     830 </span>            : }
<span class="lineNum">     831 </span>            : 
<span class="lineNum">     832 </span>            : static inline int cpupid_to_nid(int cpupid)
<span class="lineNum">     833 </span>            : {
<span class="lineNum">     834 </span>            :         return -1;
<span class="lineNum">     835 </span>            : }
<span class="lineNum">     836 </span>            : 
<span class="lineNum">     837 </span>            : static inline int cpupid_to_pid(int cpupid)
<span class="lineNum">     838 </span>            : {
<span class="lineNum">     839 </span>            :         return -1;
<span class="lineNum">     840 </span>            : }
<span class="lineNum">     841 </span>            : 
<span class="lineNum">     842 </span>            : static inline int cpupid_to_cpu(int cpupid)
<span class="lineNum">     843 </span>            : {
<span class="lineNum">     844 </span>            :         return -1;
<span class="lineNum">     845 </span>            : }
<span class="lineNum">     846 </span>            : 
<span class="lineNum">     847 </span>            : static inline int cpu_pid_to_cpupid(int nid, int pid)
<span class="lineNum">     848 </span>            : {
<span class="lineNum">     849 </span>            :         return -1;
<span class="lineNum">     850 </span>            : }
<span class="lineNum">     851 </span>            : 
<span class="lineNum">     852 </span>            : static inline bool cpupid_pid_unset(int cpupid)
<span class="lineNum">     853 </span>            : {
<span class="lineNum">     854 </span>            :         return 1;
<span class="lineNum">     855 </span>            : }
<span class="lineNum">     856 </span>            : 
<span class="lineNum">     857 </span>            : static inline void page_cpupid_reset_last(struct page *page)
<span class="lineNum">     858 </span>            : {
<span class="lineNum">     859 </span>            : }
<span class="lineNum">     860 </span>            : 
<span class="lineNum">     861 </span>            : static inline bool cpupid_match_pid(struct task_struct *task, int cpupid)
<span class="lineNum">     862 </span>            : {
<span class="lineNum">     863 </span>            :         return false;
<span class="lineNum">     864 </span>            : }
<span class="lineNum">     865 </span>            : #endif /* CONFIG_NUMA_BALANCING */
<span class="lineNum">     866 </span>            : 
<span class="lineNum">     867 </span>            : static inline struct zone *page_zone(const struct page *page)
<span class="lineNum">     868 </span>            : {
<span class="lineNum">     869 </span>            :         return &amp;NODE_DATA(page_to_nid(page))-&gt;node_zones[page_zonenum(page)];
<span class="lineNum">     870 </span>            : }
<span class="lineNum">     871 </span>            : 
<span class="lineNum">     872 </span>            : #ifdef SECTION_IN_PAGE_FLAGS
<span class="lineNum">     873 </span>            : static inline void set_page_section(struct page *page, unsigned long section)
<span class="lineNum">     874 </span>            : {
<span class="lineNum">     875 </span>            :         page-&gt;flags &amp;= ~(SECTIONS_MASK &lt;&lt; SECTIONS_PGSHIFT);
<span class="lineNum">     876 </span>            :         page-&gt;flags |= (section &amp; SECTIONS_MASK) &lt;&lt; SECTIONS_PGSHIFT;
<span class="lineNum">     877 </span>            : }
<span class="lineNum">     878 </span>            : 
<span class="lineNum">     879 </span>            : static inline unsigned long page_to_section(const struct page *page)
<span class="lineNum">     880 </span>            : {
<span class="lineNum">     881 </span>            :         return (page-&gt;flags &gt;&gt; SECTIONS_PGSHIFT) &amp; SECTIONS_MASK;
<span class="lineNum">     882 </span>            : }
<span class="lineNum">     883 </span>            : #endif
<span class="lineNum">     884 </span>            : 
<span class="lineNum">     885 </span>            : static inline void set_page_zone(struct page *page, enum zone_type zone)
<span class="lineNum">     886 </span>            : {
<span class="lineNum">     887 </span>            :         page-&gt;flags &amp;= ~(ZONES_MASK &lt;&lt; ZONES_PGSHIFT);
<span class="lineNum">     888 </span>            :         page-&gt;flags |= (zone &amp; ZONES_MASK) &lt;&lt; ZONES_PGSHIFT;
<span class="lineNum">     889 </span>            : }
<span class="lineNum">     890 </span>            : 
<span class="lineNum">     891 </span>            : static inline void set_page_node(struct page *page, unsigned long node)
<span class="lineNum">     892 </span>            : {
<span class="lineNum">     893 </span>            :         page-&gt;flags &amp;= ~(NODES_MASK &lt;&lt; NODES_PGSHIFT);
<span class="lineNum">     894 </span>            :         page-&gt;flags |= (node &amp; NODES_MASK) &lt;&lt; NODES_PGSHIFT;
<span class="lineNum">     895 </span>            : }
<span class="lineNum">     896 </span>            : 
<span class="lineNum">     897 </span>            : static inline void set_page_links(struct page *page, enum zone_type zone,
<span class="lineNum">     898 </span>            :         unsigned long node, unsigned long pfn)
<span class="lineNum">     899 </span>            : {
<span class="lineNum">     900 </span>            :         set_page_zone(page, zone);
<span class="lineNum">     901 </span>            :         set_page_node(page, node);
<span class="lineNum">     902 </span>            : #ifdef SECTION_IN_PAGE_FLAGS
<span class="lineNum">     903 </span>            :         set_page_section(page, pfn_to_section_nr(pfn));
<span class="lineNum">     904 </span>            : #endif
<span class="lineNum">     905 </span>            : }
<span class="lineNum">     906 </span>            : 
<span class="lineNum">     907 </span>            : /*
<span class="lineNum">     908 </span>            :  * Some inline functions in vmstat.h depend on page_zone()
<span class="lineNum">     909 </span>            :  */
<span class="lineNum">     910 </span>            : #include &lt;linux/vmstat.h&gt;
<span class="lineNum">     911 </span>            : 
<span class="lineNum">     912 </span>            : static __always_inline void *lowmem_page_address(const struct page *page)
<span class="lineNum">     913 </span>            : {
<span class="lineNum">     914 </span><span class="lineCov">   98214134 :         return __va(PFN_PHYS(page_to_pfn(page)));</span>
<span class="lineNum">     915 </span>            : }
<span class="lineNum">     916 </span>            : 
<span class="lineNum">     917 </span>            : #if defined(CONFIG_HIGHMEM) &amp;&amp; !defined(WANT_PAGE_VIRTUAL)
<span class="lineNum">     918 </span>            : #define HASHED_PAGE_VIRTUAL
<span class="lineNum">     919 </span>            : #endif
<span class="lineNum">     920 </span>            : 
<span class="lineNum">     921 </span>            : #if defined(WANT_PAGE_VIRTUAL)
<span class="lineNum">     922 </span>            : static inline void *page_address(const struct page *page)
<span class="lineNum">     923 </span>            : {
<span class="lineNum">     924 </span>            :         return page-&gt;virtual;
<span class="lineNum">     925 </span>            : }
<span class="lineNum">     926 </span>            : static inline void set_page_address(struct page *page, void *address)
<span class="lineNum">     927 </span>            : {
<span class="lineNum">     928 </span>            :         page-&gt;virtual = address;
<span class="lineNum">     929 </span>            : }
<span class="lineNum">     930 </span>            : #define page_address_init()  do { } while(0)
<span class="lineNum">     931 </span>            : #endif
<span class="lineNum">     932 </span>            : 
<span class="lineNum">     933 </span>            : #if defined(HASHED_PAGE_VIRTUAL)
<span class="lineNum">     934 </span>            : void *page_address(const struct page *page);
<span class="lineNum">     935 </span>            : void set_page_address(struct page *page, void *virtual);
<span class="lineNum">     936 </span>            : void page_address_init(void);
<span class="lineNum">     937 </span>            : #endif
<span class="lineNum">     938 </span>            : 
<span class="lineNum">     939 </span>            : #if !defined(HASHED_PAGE_VIRTUAL) &amp;&amp; !defined(WANT_PAGE_VIRTUAL)
<span class="lineNum">     940 </span>            : #define page_address(page) lowmem_page_address(page)
<span class="lineNum">     941 </span>            : #define set_page_address(page, address)  do { } while(0)
<span class="lineNum">     942 </span>            : #define page_address_init()  do { } while(0)
<span class="lineNum">     943 </span>            : #endif
<span class="lineNum">     944 </span>            : 
<span class="lineNum">     945 </span>            : /*
<span class="lineNum">     946 </span>            :  * On an anonymous page mapped into a user virtual memory area,
<span class="lineNum">     947 </span>            :  * page-&gt;mapping points to its anon_vma, not to a struct address_space;
<span class="lineNum">     948 </span>            :  * with the PAGE_MAPPING_ANON bit set to distinguish it.  See rmap.h.
<span class="lineNum">     949 </span>            :  *
<span class="lineNum">     950 </span>            :  * On an anonymous page in a VM_MERGEABLE area, if CONFIG_KSM is enabled,
<span class="lineNum">     951 </span>            :  * the PAGE_MAPPING_KSM bit may be set along with the PAGE_MAPPING_ANON bit;
<span class="lineNum">     952 </span>            :  * and then page-&gt;mapping points, not to an anon_vma, but to a private
<span class="lineNum">     953 </span>            :  * structure which KSM associates with that merged page.  See ksm.h.
<span class="lineNum">     954 </span>            :  *
<span class="lineNum">     955 </span>            :  * PAGE_MAPPING_KSM without PAGE_MAPPING_ANON is currently never used.
<span class="lineNum">     956 </span>            :  *
<span class="lineNum">     957 </span>            :  * Please note that, confusingly, &quot;page_mapping&quot; refers to the inode
<span class="lineNum">     958 </span>            :  * address_space which maps the page from disk; whereas &quot;page_mapped&quot;
<span class="lineNum">     959 </span>            :  * refers to user virtual address space into which the page is mapped.
<span class="lineNum">     960 </span>            :  */
<span class="lineNum">     961 </span>            : #define PAGE_MAPPING_ANON       1
<span class="lineNum">     962 </span>            : #define PAGE_MAPPING_KSM        2
<span class="lineNum">     963 </span>            : #define PAGE_MAPPING_FLAGS      (PAGE_MAPPING_ANON | PAGE_MAPPING_KSM)
<span class="lineNum">     964 </span>            : 
<span class="lineNum">     965 </span>            : extern struct address_space *page_mapping(struct page *page);
<span class="lineNum">     966 </span>            : 
<span class="lineNum">     967 </span>            : /* Neutral page-&gt;mapping pointer to address_space or anon_vma or other */
<span class="lineNum">     968 </span>            : static inline void *page_rmapping(struct page *page)
<span class="lineNum">     969 </span>            : {
<span class="lineNum">     970 </span>            :         return (void *)((unsigned long)page-&gt;mapping &amp; ~PAGE_MAPPING_FLAGS);
<span class="lineNum">     971 </span>            : }
<span class="lineNum">     972 </span>            : 
<span class="lineNum">     973 </span>            : extern struct address_space *__page_file_mapping(struct page *);
<span class="lineNum">     974 </span>            : 
<span class="lineNum">     975 </span>            : static inline
<span class="lineNum">     976 </span>            : struct address_space *page_file_mapping(struct page *page)
<span class="lineNum">     977 </span>            : {
<span class="lineNum">     978 </span>            :         if (unlikely(PageSwapCache(page)))
<span class="lineNum">     979 </span>            :                 return __page_file_mapping(page);
<span class="lineNum">     980 </span>            : 
<span class="lineNum">     981 </span>            :         return page-&gt;mapping;
<span class="lineNum">     982 </span>            : }
<span class="lineNum">     983 </span>            : 
<span class="lineNum">     984 </span>            : static inline int PageAnon(struct page *page)
<span class="lineNum">     985 </span>            : {
<span class="lineNum">     986 </span>            :         return ((unsigned long)page-&gt;mapping &amp; PAGE_MAPPING_ANON) != 0;
<span class="lineNum">     987 </span>            : }
<span class="lineNum">     988 </span>            : 
<span class="lineNum">     989 </span>            : /*
<span class="lineNum">     990 </span>            :  * Return the pagecache index of the passed page.  Regular pagecache pages
<span class="lineNum">     991 </span>            :  * use -&gt;index whereas swapcache pages use -&gt;private
<span class="lineNum">     992 </span>            :  */
<span class="lineNum">     993 </span>            : static inline pgoff_t page_index(struct page *page)
<span class="lineNum">     994 </span>            : {
<span class="lineNum">     995 </span><span class="lineCov">       4706 :         if (unlikely(PageSwapCache(page)))</span>
<span class="lineNum">     996 </span><span class="lineNoCov">          0 :                 return page_private(page);</span>
<span class="lineNum">     997 </span><span class="lineCov">       4706 :         return page-&gt;index;</span>
<span class="lineNum">     998 </span>            : }
<span class="lineNum">     999 </span>            : 
<span class="lineNum">    1000 </span>            : extern pgoff_t __page_file_index(struct page *page);
<span class="lineNum">    1001 </span>            : 
<span class="lineNum">    1002 </span>            : /*
<span class="lineNum">    1003 </span>            :  * Return the file index of the page. Regular pagecache pages use -&gt;index
<span class="lineNum">    1004 </span>            :  * whereas swapcache pages use swp_offset(-&gt;private)
<span class="lineNum">    1005 </span>            :  */
<span class="lineNum">    1006 </span>            : static inline pgoff_t page_file_index(struct page *page)
<span class="lineNum">    1007 </span>            : {
<span class="lineNum">    1008 </span>            :         if (unlikely(PageSwapCache(page)))
<span class="lineNum">    1009 </span>            :                 return __page_file_index(page);
<span class="lineNum">    1010 </span>            : 
<span class="lineNum">    1011 </span>            :         return page-&gt;index;
<span class="lineNum">    1012 </span>            : }
<span class="lineNum">    1013 </span>            : 
<span class="lineNum">    1014 </span>            : /*
<span class="lineNum">    1015 </span>            :  * Return true if this page is mapped into pagetables.
<span class="lineNum">    1016 </span>            :  */
<span class="lineNum">    1017 </span>            : static inline int page_mapped(struct page *page)
<span class="lineNum">    1018 </span>            : {
<span class="lineNum">    1019 </span>            :         return atomic_read(&amp;(page)-&gt;_mapcount) &gt;= 0;
<span class="lineNum">    1020 </span>            : }
<span class="lineNum">    1021 </span>            : 
<span class="lineNum">    1022 </span>            : /*
<span class="lineNum">    1023 </span>            :  * Different kinds of faults, as returned by handle_mm_fault().
<span class="lineNum">    1024 </span>            :  * Used to decide whether a process gets delivered SIGBUS or
<span class="lineNum">    1025 </span>            :  * just gets major/minor fault counters bumped up.
<span class="lineNum">    1026 </span>            :  */
<span class="lineNum">    1027 </span>            : 
<span class="lineNum">    1028 </span>            : #define VM_FAULT_MINOR  0 /* For backwards compat. Remove me quickly. */
<span class="lineNum">    1029 </span>            : 
<span class="lineNum">    1030 </span>            : #define VM_FAULT_OOM    0x0001
<span class="lineNum">    1031 </span>            : #define VM_FAULT_SIGBUS 0x0002
<span class="lineNum">    1032 </span>            : #define VM_FAULT_MAJOR  0x0004
<span class="lineNum">    1033 </span>            : #define VM_FAULT_WRITE  0x0008  /* Special case for get_user_pages */
<span class="lineNum">    1034 </span>            : #define VM_FAULT_HWPOISON 0x0010        /* Hit poisoned small page */
<span class="lineNum">    1035 </span>            : #define VM_FAULT_HWPOISON_LARGE 0x0020  /* Hit poisoned large page. Index encoded in upper bits */
<span class="lineNum">    1036 </span>            : 
<span class="lineNum">    1037 </span>            : #define VM_FAULT_NOPAGE 0x0100  /* -&gt;fault installed the pte, not return page */
<span class="lineNum">    1038 </span>            : #define VM_FAULT_LOCKED 0x0200  /* -&gt;fault locked the returned page */
<span class="lineNum">    1039 </span>            : #define VM_FAULT_RETRY  0x0400  /* -&gt;fault blocked, must retry */
<span class="lineNum">    1040 </span>            : #define VM_FAULT_FALLBACK 0x0800        /* huge page fault failed, fall back to small */
<span class="lineNum">    1041 </span>            : 
<span class="lineNum">    1042 </span>            : #define VM_FAULT_HWPOISON_LARGE_MASK 0xf000 /* encodes hpage index for large hwpoison */
<span class="lineNum">    1043 </span>            : 
<span class="lineNum">    1044 </span>            : #define VM_FAULT_ERROR  (VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_HWPOISON | \
<span class="lineNum">    1045 </span>            :                          VM_FAULT_FALLBACK | VM_FAULT_HWPOISON_LARGE)
<span class="lineNum">    1046 </span>            : 
<span class="lineNum">    1047 </span>            : /* Encode hstate index for a hwpoisoned large page */
<span class="lineNum">    1048 </span>            : #define VM_FAULT_SET_HINDEX(x) ((x) &lt;&lt; 12)
<span class="lineNum">    1049 </span>            : #define VM_FAULT_GET_HINDEX(x) (((x) &gt;&gt; 12) &amp; 0xf)
<span class="lineNum">    1050 </span>            : 
<span class="lineNum">    1051 </span>            : /*
<span class="lineNum">    1052 </span>            :  * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.
<span class="lineNum">    1053 </span>            :  */
<span class="lineNum">    1054 </span>            : extern void pagefault_out_of_memory(void);
<span class="lineNum">    1055 </span>            : 
<span class="lineNum">    1056 </span>            : #define offset_in_page(p)       ((unsigned long)(p) &amp; ~PAGE_MASK)
<span class="lineNum">    1057 </span>            : 
<span class="lineNum">    1058 </span>            : /*
<span class="lineNum">    1059 </span>            :  * Flags passed to show_mem() and show_free_areas() to suppress output in
<span class="lineNum">    1060 </span>            :  * various contexts.
<span class="lineNum">    1061 </span>            :  */
<span class="lineNum">    1062 </span>            : #define SHOW_MEM_FILTER_NODES           (0x0001u)       /* disallowed nodes */
<span class="lineNum">    1063 </span>            : 
<span class="lineNum">    1064 </span>            : extern void show_free_areas(unsigned int flags);
<span class="lineNum">    1065 </span>            : extern bool skip_free_areas_node(unsigned int flags, int nid);
<span class="lineNum">    1066 </span>            : 
<span class="lineNum">    1067 </span>            : int shmem_zero_setup(struct vm_area_struct *);
<span class="lineNum">    1068 </span>            : #ifdef CONFIG_SHMEM
<span class="lineNum">    1069 </span>            : bool shmem_mapping(struct address_space *mapping);
<span class="lineNum">    1070 </span>            : #else
<span class="lineNum">    1071 </span>            : static inline bool shmem_mapping(struct address_space *mapping)
<span class="lineNum">    1072 </span>            : {
<span class="lineNum">    1073 </span>            :         return false;
<span class="lineNum">    1074 </span>            : }
<span class="lineNum">    1075 </span>            : #endif
<span class="lineNum">    1076 </span>            : 
<span class="lineNum">    1077 </span>            : extern int can_do_mlock(void);
<span class="lineNum">    1078 </span>            : extern int user_shm_lock(size_t, struct user_struct *);
<span class="lineNum">    1079 </span>            : extern void user_shm_unlock(size_t, struct user_struct *);
<span class="lineNum">    1080 </span>            : 
<span class="lineNum">    1081 </span>            : /*
<span class="lineNum">    1082 </span>            :  * Parameter block passed down to zap_pte_range in exceptional cases.
<span class="lineNum">    1083 </span>            :  */
<span class="lineNum">    1084 </span>            : struct zap_details {
<span class="lineNum">    1085 </span>            :         struct vm_area_struct *nonlinear_vma;   /* Check page-&gt;index if set */
<span class="lineNum">    1086 </span>            :         struct address_space *check_mapping;    /* Check page-&gt;mapping if set */
<span class="lineNum">    1087 </span>            :         pgoff_t first_index;                    /* Lowest page-&gt;index to unmap */
<span class="lineNum">    1088 </span>            :         pgoff_t last_index;                     /* Highest page-&gt;index to unmap */
<span class="lineNum">    1089 </span>            : };
<span class="lineNum">    1090 </span>            : 
<span class="lineNum">    1091 </span>            : struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
<span class="lineNum">    1092 </span>            :                 pte_t pte);
<span class="lineNum">    1093 </span>            : 
<span class="lineNum">    1094 </span>            : int zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
<span class="lineNum">    1095 </span>            :                 unsigned long size);
<span class="lineNum">    1096 </span>            : void zap_page_range(struct vm_area_struct *vma, unsigned long address,
<span class="lineNum">    1097 </span>            :                 unsigned long size, struct zap_details *);
<span class="lineNum">    1098 </span>            : void unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
<span class="lineNum">    1099 </span>            :                 unsigned long start, unsigned long end);
<span class="lineNum">    1100 </span>            : 
<span class="lineNum">    1101 </span>            : /**
<span class="lineNum">    1102 </span>            :  * mm_walk - callbacks for walk_page_range
<span class="lineNum">    1103 </span>            :  * @pgd_entry: if set, called for each non-empty PGD (top-level) entry
<span class="lineNum">    1104 </span>            :  * @pud_entry: if set, called for each non-empty PUD (2nd-level) entry
<span class="lineNum">    1105 </span>            :  * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry
<span class="lineNum">    1106 </span>            :  *             this handler is required to be able to handle
<span class="lineNum">    1107 </span>            :  *             pmd_trans_huge() pmds.  They may simply choose to
<span class="lineNum">    1108 </span>            :  *             split_huge_page() instead of handling it explicitly.
<span class="lineNum">    1109 </span>            :  * @pte_entry: if set, called for each non-empty PTE (4th-level) entry
<span class="lineNum">    1110 </span>            :  * @pte_hole: if set, called for each hole at all levels
<span class="lineNum">    1111 </span>            :  * @hugetlb_entry: if set, called for each hugetlb entry
<span class="lineNum">    1112 </span>            :  *                 *Caution*: The caller must hold mmap_sem() if @hugetlb_entry
<span class="lineNum">    1113 </span>            :  *                            is used.
<span class="lineNum">    1114 </span>            :  *
<span class="lineNum">    1115 </span>            :  * (see walk_page_range for more details)
<span class="lineNum">    1116 </span>            :  */
<span class="lineNum">    1117 </span>            : struct mm_walk {
<span class="lineNum">    1118 </span>            :         int (*pgd_entry)(pgd_t *pgd, unsigned long addr,
<span class="lineNum">    1119 </span>            :                          unsigned long next, struct mm_walk *walk);
<span class="lineNum">    1120 </span>            :         int (*pud_entry)(pud_t *pud, unsigned long addr,
<span class="lineNum">    1121 </span>            :                          unsigned long next, struct mm_walk *walk);
<span class="lineNum">    1122 </span>            :         int (*pmd_entry)(pmd_t *pmd, unsigned long addr,
<span class="lineNum">    1123 </span>            :                          unsigned long next, struct mm_walk *walk);
<span class="lineNum">    1124 </span>            :         int (*pte_entry)(pte_t *pte, unsigned long addr,
<span class="lineNum">    1125 </span>            :                          unsigned long next, struct mm_walk *walk);
<span class="lineNum">    1126 </span>            :         int (*pte_hole)(unsigned long addr, unsigned long next,
<span class="lineNum">    1127 </span>            :                         struct mm_walk *walk);
<span class="lineNum">    1128 </span>            :         int (*hugetlb_entry)(pte_t *pte, unsigned long hmask,
<span class="lineNum">    1129 </span>            :                              unsigned long addr, unsigned long next,
<span class="lineNum">    1130 </span>            :                              struct mm_walk *walk);
<span class="lineNum">    1131 </span>            :         struct mm_struct *mm;
<span class="lineNum">    1132 </span>            :         void *private;
<span class="lineNum">    1133 </span>            : };
<span class="lineNum">    1134 </span>            : 
<span class="lineNum">    1135 </span>            : int walk_page_range(unsigned long addr, unsigned long end,
<span class="lineNum">    1136 </span>            :                 struct mm_walk *walk);
<span class="lineNum">    1137 </span>            : void free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
<span class="lineNum">    1138 </span>            :                 unsigned long end, unsigned long floor, unsigned long ceiling);
<span class="lineNum">    1139 </span>            : int copy_page_range(struct mm_struct *dst, struct mm_struct *src,
<span class="lineNum">    1140 </span>            :                         struct vm_area_struct *vma);
<span class="lineNum">    1141 </span>            : void unmap_mapping_range(struct address_space *mapping,
<span class="lineNum">    1142 </span>            :                 loff_t const holebegin, loff_t const holelen, int even_cows);
<span class="lineNum">    1143 </span>            : int follow_pfn(struct vm_area_struct *vma, unsigned long address,
<span class="lineNum">    1144 </span>            :         unsigned long *pfn);
<span class="lineNum">    1145 </span>            : int follow_phys(struct vm_area_struct *vma, unsigned long address,
<span class="lineNum">    1146 </span>            :                 unsigned int flags, unsigned long *prot, resource_size_t *phys);
<span class="lineNum">    1147 </span>            : int generic_access_phys(struct vm_area_struct *vma, unsigned long addr,
<span class="lineNum">    1148 </span>            :                         void *buf, int len, int write);
<span class="lineNum">    1149 </span>            : 
<span class="lineNum">    1150 </span>            : static inline void unmap_shared_mapping_range(struct address_space *mapping,
<span class="lineNum">    1151 </span>            :                 loff_t const holebegin, loff_t const holelen)
<span class="lineNum">    1152 </span>            : {
<span class="lineNum">    1153 </span>            :         unmap_mapping_range(mapping, holebegin, holelen, 0);
<span class="lineNum">    1154 </span>            : }
<span class="lineNum">    1155 </span>            : 
<span class="lineNum">    1156 </span>            : extern void truncate_pagecache(struct inode *inode, loff_t new);
<span class="lineNum">    1157 </span>            : extern void truncate_setsize(struct inode *inode, loff_t newsize);
<span class="lineNum">    1158 </span>            : void truncate_pagecache_range(struct inode *inode, loff_t offset, loff_t end);
<span class="lineNum">    1159 </span>            : int truncate_inode_page(struct address_space *mapping, struct page *page);
<span class="lineNum">    1160 </span>            : int generic_error_remove_page(struct address_space *mapping, struct page *page);
<span class="lineNum">    1161 </span>            : int invalidate_inode_page(struct page *page);
<span class="lineNum">    1162 </span>            : 
<span class="lineNum">    1163 </span>            : #ifdef CONFIG_MMU
<span class="lineNum">    1164 </span>            : extern int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
<span class="lineNum">    1165 </span>            :                         unsigned long address, unsigned int flags);
<span class="lineNum">    1166 </span>            : extern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
<span class="lineNum">    1167 </span>            :                             unsigned long address, unsigned int fault_flags);
<span class="lineNum">    1168 </span>            : #else
<span class="lineNum">    1169 </span>            : static inline int handle_mm_fault(struct mm_struct *mm,
<span class="lineNum">    1170 </span>            :                         struct vm_area_struct *vma, unsigned long address,
<span class="lineNum">    1171 </span>            :                         unsigned int flags)
<span class="lineNum">    1172 </span>            : {
<span class="lineNum">    1173 </span>            :         /* should never happen if there's no MMU */
<span class="lineNum">    1174 </span>            :         BUG();
<span class="lineNum">    1175 </span>            :         return VM_FAULT_SIGBUS;
<span class="lineNum">    1176 </span>            : }
<span class="lineNum">    1177 </span>            : static inline int fixup_user_fault(struct task_struct *tsk,
<span class="lineNum">    1178 </span>            :                 struct mm_struct *mm, unsigned long address,
<span class="lineNum">    1179 </span>            :                 unsigned int fault_flags)
<span class="lineNum">    1180 </span>            : {
<span class="lineNum">    1181 </span>            :         /* should never happen if there's no MMU */
<span class="lineNum">    1182 </span>            :         BUG();
<span class="lineNum">    1183 </span>            :         return -EFAULT;
<span class="lineNum">    1184 </span>            : }
<span class="lineNum">    1185 </span>            : #endif
<span class="lineNum">    1186 </span>            : 
<span class="lineNum">    1187 </span>            : extern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);
<span class="lineNum">    1188 </span>            : extern int access_remote_vm(struct mm_struct *mm, unsigned long addr,
<span class="lineNum">    1189 </span>            :                 void *buf, int len, int write);
<span class="lineNum">    1190 </span>            : 
<span class="lineNum">    1191 </span>            : long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
<span class="lineNum">    1192 </span>            :                       unsigned long start, unsigned long nr_pages,
<span class="lineNum">    1193 </span>            :                       unsigned int foll_flags, struct page **pages,
<span class="lineNum">    1194 </span>            :                       struct vm_area_struct **vmas, int *nonblocking);
<span class="lineNum">    1195 </span>            : long get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
<span class="lineNum">    1196 </span>            :                     unsigned long start, unsigned long nr_pages,
<span class="lineNum">    1197 </span>            :                     int write, int force, struct page **pages,
<span class="lineNum">    1198 </span>            :                     struct vm_area_struct **vmas);
<span class="lineNum">    1199 </span>            : int get_user_pages_fast(unsigned long start, int nr_pages, int write,
<span class="lineNum">    1200 </span>            :                         struct page **pages);
<span class="lineNum">    1201 </span>            : struct kvec;
<span class="lineNum">    1202 </span>            : int get_kernel_pages(const struct kvec *iov, int nr_pages, int write,
<span class="lineNum">    1203 </span>            :                         struct page **pages);
<span class="lineNum">    1204 </span>            : int get_kernel_page(unsigned long start, int write, struct page **pages);
<span class="lineNum">    1205 </span>            : struct page *get_dump_page(unsigned long addr);
<span class="lineNum">    1206 </span>            : 
<span class="lineNum">    1207 </span>            : extern int try_to_release_page(struct page * page, gfp_t gfp_mask);
<span class="lineNum">    1208 </span>            : extern void do_invalidatepage(struct page *page, unsigned int offset,
<span class="lineNum">    1209 </span>            :                               unsigned int length);
<span class="lineNum">    1210 </span>            : 
<span class="lineNum">    1211 </span>            : int __set_page_dirty_nobuffers(struct page *page);
<span class="lineNum">    1212 </span>            : int __set_page_dirty_no_writeback(struct page *page);
<span class="lineNum">    1213 </span>            : int redirty_page_for_writepage(struct writeback_control *wbc,
<span class="lineNum">    1214 </span>            :                                 struct page *page);
<span class="lineNum">    1215 </span>            : void account_page_dirtied(struct page *page, struct address_space *mapping);
<span class="lineNum">    1216 </span>            : void account_page_writeback(struct page *page);
<span class="lineNum">    1217 </span>            : int set_page_dirty(struct page *page);
<span class="lineNum">    1218 </span>            : int set_page_dirty_lock(struct page *page);
<span class="lineNum">    1219 </span>            : int clear_page_dirty_for_io(struct page *page);
<span class="lineNum">    1220 </span>            : int get_cmdline(struct task_struct *task, char *buffer, int buflen);
<span class="lineNum">    1221 </span>            : 
<span class="lineNum">    1222 </span>            : /* Is the vma a continuation of the stack vma above it? */
<span class="lineNum">    1223 </span>            : static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)
<span class="lineNum">    1224 </span>            : {
<span class="lineNum">    1225 </span>            :         return vma &amp;&amp; (vma-&gt;vm_end == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSDOWN);
<span class="lineNum">    1226 </span>            : }
<span class="lineNum">    1227 </span>            : 
<span class="lineNum">    1228 </span>            : static inline int stack_guard_page_start(struct vm_area_struct *vma,
<span class="lineNum">    1229 </span>            :                                              unsigned long addr)
<span class="lineNum">    1230 </span>            : {
<span class="lineNum">    1231 </span>            :         return (vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;
<span class="lineNum">    1232 </span>            :                 (vma-&gt;vm_start == addr) &amp;&amp;
<span class="lineNum">    1233 </span>            :                 !vma_growsdown(vma-&gt;vm_prev, addr);
<span class="lineNum">    1234 </span>            : }
<span class="lineNum">    1235 </span>            : 
<span class="lineNum">    1236 </span>            : /* Is the vma a continuation of the stack vma below it? */
<span class="lineNum">    1237 </span>            : static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)
<span class="lineNum">    1238 </span>            : {
<span class="lineNum">    1239 </span>            :         return vma &amp;&amp; (vma-&gt;vm_start == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSUP);
<span class="lineNum">    1240 </span>            : }
<span class="lineNum">    1241 </span>            : 
<span class="lineNum">    1242 </span>            : static inline int stack_guard_page_end(struct vm_area_struct *vma,
<span class="lineNum">    1243 </span>            :                                            unsigned long addr)
<span class="lineNum">    1244 </span>            : {
<span class="lineNum">    1245 </span>            :         return (vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp;
<span class="lineNum">    1246 </span>            :                 (vma-&gt;vm_end == addr) &amp;&amp;
<span class="lineNum">    1247 </span>            :                 !vma_growsup(vma-&gt;vm_next, addr);
<span class="lineNum">    1248 </span>            : }
<span class="lineNum">    1249 </span>            : 
<span class="lineNum">    1250 </span>            : extern pid_t
<span class="lineNum">    1251 </span>            : vm_is_stack(struct task_struct *task, struct vm_area_struct *vma, int in_group);
<span class="lineNum">    1252 </span>            : 
<span class="lineNum">    1253 </span>            : extern unsigned long move_page_tables(struct vm_area_struct *vma,
<span class="lineNum">    1254 </span>            :                 unsigned long old_addr, struct vm_area_struct *new_vma,
<span class="lineNum">    1255 </span>            :                 unsigned long new_addr, unsigned long len,
<span class="lineNum">    1256 </span>            :                 bool need_rmap_locks);
<span class="lineNum">    1257 </span>            : extern unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
<span class="lineNum">    1258 </span>            :                               unsigned long end, pgprot_t newprot,
<span class="lineNum">    1259 </span>            :                               int dirty_accountable, int prot_numa);
<span class="lineNum">    1260 </span>            : extern int mprotect_fixup(struct vm_area_struct *vma,
<span class="lineNum">    1261 </span>            :                           struct vm_area_struct **pprev, unsigned long start,
<span class="lineNum">    1262 </span>            :                           unsigned long end, unsigned long newflags);
<span class="lineNum">    1263 </span>            : 
<span class="lineNum">    1264 </span>            : /*
<span class="lineNum">    1265 </span>            :  * doesn't attempt to fault and will return short.
<span class="lineNum">    1266 </span>            :  */
<span class="lineNum">    1267 </span>            : int __get_user_pages_fast(unsigned long start, int nr_pages, int write,
<span class="lineNum">    1268 </span>            :                           struct page **pages);
<span class="lineNum">    1269 </span>            : /*
<span class="lineNum">    1270 </span>            :  * per-process(per-mm_struct) statistics.
<span class="lineNum">    1271 </span>            :  */
<span class="lineNum">    1272 </span>            : static inline unsigned long get_mm_counter(struct mm_struct *mm, int member)
<span class="lineNum">    1273 </span>            : {
<span class="lineNum">    1274 </span>            :         long val = atomic_long_read(&amp;mm-&gt;rss_stat.count[member]);
<span class="lineNum">    1275 </span>            : 
<span class="lineNum">    1276 </span>            : #ifdef SPLIT_RSS_COUNTING
<span class="lineNum">    1277 </span>            :         /*
<span class="lineNum">    1278 </span>            :          * counter is updated in asynchronous manner and may go to minus.
<span class="lineNum">    1279 </span>            :          * But it's never be expected number for users.
<span class="lineNum">    1280 </span>            :          */
<span class="lineNum">    1281 </span>            :         if (val &lt; 0)
<span class="lineNum">    1282 </span>            :                 val = 0;
<span class="lineNum">    1283 </span>            : #endif
<span class="lineNum">    1284 </span>            :         return (unsigned long)val;
<span class="lineNum">    1285 </span>            : }
<span class="lineNum">    1286 </span>            : 
<span class="lineNum">    1287 </span>            : static inline void add_mm_counter(struct mm_struct *mm, int member, long value)
<span class="lineNum">    1288 </span>            : {
<span class="lineNum">    1289 </span>            :         atomic_long_add(value, &amp;mm-&gt;rss_stat.count[member]);
<span class="lineNum">    1290 </span>            : }
<span class="lineNum">    1291 </span>            : 
<span class="lineNum">    1292 </span>            : static inline void inc_mm_counter(struct mm_struct *mm, int member)
<span class="lineNum">    1293 </span>            : {
<span class="lineNum">    1294 </span>            :         atomic_long_inc(&amp;mm-&gt;rss_stat.count[member]);
<span class="lineNum">    1295 </span>            : }
<span class="lineNum">    1296 </span>            : 
<span class="lineNum">    1297 </span>            : static inline void dec_mm_counter(struct mm_struct *mm, int member)
<span class="lineNum">    1298 </span>            : {
<span class="lineNum">    1299 </span>            :         atomic_long_dec(&amp;mm-&gt;rss_stat.count[member]);
<span class="lineNum">    1300 </span>            : }
<span class="lineNum">    1301 </span>            : 
<span class="lineNum">    1302 </span>            : static inline unsigned long get_mm_rss(struct mm_struct *mm)
<span class="lineNum">    1303 </span>            : {
<span class="lineNum">    1304 </span>            :         return get_mm_counter(mm, MM_FILEPAGES) +
<span class="lineNum">    1305 </span>            :                 get_mm_counter(mm, MM_ANONPAGES);
<span class="lineNum">    1306 </span>            : }
<span class="lineNum">    1307 </span>            : 
<span class="lineNum">    1308 </span>            : static inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)
<span class="lineNum">    1309 </span>            : {
<span class="lineNum">    1310 </span>            :         return max(mm-&gt;hiwater_rss, get_mm_rss(mm));
<span class="lineNum">    1311 </span>            : }
<span class="lineNum">    1312 </span>            : 
<span class="lineNum">    1313 </span>            : static inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)
<span class="lineNum">    1314 </span>            : {
<span class="lineNum">    1315 </span>            :         return max(mm-&gt;hiwater_vm, mm-&gt;total_vm);
<span class="lineNum">    1316 </span>            : }
<span class="lineNum">    1317 </span>            : 
<span class="lineNum">    1318 </span>            : static inline void update_hiwater_rss(struct mm_struct *mm)
<span class="lineNum">    1319 </span>            : {
<span class="lineNum">    1320 </span>            :         unsigned long _rss = get_mm_rss(mm);
<span class="lineNum">    1321 </span>            : 
<span class="lineNum">    1322 </span>            :         if ((mm)-&gt;hiwater_rss &lt; _rss)
<span class="lineNum">    1323 </span>            :                 (mm)-&gt;hiwater_rss = _rss;
<span class="lineNum">    1324 </span>            : }
<span class="lineNum">    1325 </span>            : 
<span class="lineNum">    1326 </span>            : static inline void update_hiwater_vm(struct mm_struct *mm)
<span class="lineNum">    1327 </span>            : {
<span class="lineNum">    1328 </span>            :         if (mm-&gt;hiwater_vm &lt; mm-&gt;total_vm)
<span class="lineNum">    1329 </span>            :                 mm-&gt;hiwater_vm = mm-&gt;total_vm;
<span class="lineNum">    1330 </span>            : }
<span class="lineNum">    1331 </span>            : 
<span class="lineNum">    1332 </span>            : static inline void setmax_mm_hiwater_rss(unsigned long *maxrss,
<span class="lineNum">    1333 </span>            :                                          struct mm_struct *mm)
<span class="lineNum">    1334 </span>            : {
<span class="lineNum">    1335 </span>            :         unsigned long hiwater_rss = get_mm_hiwater_rss(mm);
<span class="lineNum">    1336 </span>            : 
<span class="lineNum">    1337 </span>            :         if (*maxrss &lt; hiwater_rss)
<span class="lineNum">    1338 </span>            :                 *maxrss = hiwater_rss;
<span class="lineNum">    1339 </span>            : }
<span class="lineNum">    1340 </span>            : 
<span class="lineNum">    1341 </span>            : #if defined(SPLIT_RSS_COUNTING)
<span class="lineNum">    1342 </span>            : void sync_mm_rss(struct mm_struct *mm);
<span class="lineNum">    1343 </span>            : #else
<span class="lineNum">    1344 </span>            : static inline void sync_mm_rss(struct mm_struct *mm)
<span class="lineNum">    1345 </span>            : {
<span class="lineNum">    1346 </span>            : }
<span class="lineNum">    1347 </span>            : #endif
<span class="lineNum">    1348 </span>            : 
<span class="lineNum">    1349 </span>            : int vma_wants_writenotify(struct vm_area_struct *vma);
<span class="lineNum">    1350 </span>            : 
<span class="lineNum">    1351 </span>            : extern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,
<span class="lineNum">    1352 </span>            :                                spinlock_t **ptl);
<span class="lineNum">    1353 </span>            : static inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,
<span class="lineNum">    1354 </span>            :                                     spinlock_t **ptl)
<span class="lineNum">    1355 </span>            : {
<span class="lineNum">    1356 </span>            :         pte_t *ptep;
<span class="lineNum">    1357 </span>            :         __cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));
<span class="lineNum">    1358 </span>            :         return ptep;
<span class="lineNum">    1359 </span>            : }
<span class="lineNum">    1360 </span>            : 
<span class="lineNum">    1361 </span>            : #ifdef __PAGETABLE_PUD_FOLDED
<span class="lineNum">    1362 </span>            : static inline int __pud_alloc(struct mm_struct *mm, pgd_t *pgd,
<span class="lineNum">    1363 </span>            :                                                 unsigned long address)
<span class="lineNum">    1364 </span>            : {
<span class="lineNum">    1365 </span>            :         return 0;
<span class="lineNum">    1366 </span>            : }
<span class="lineNum">    1367 </span>            : #else
<span class="lineNum">    1368 </span>            : int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);
<span class="lineNum">    1369 </span>            : #endif
<span class="lineNum">    1370 </span>            : 
<span class="lineNum">    1371 </span>            : #ifdef __PAGETABLE_PMD_FOLDED
<span class="lineNum">    1372 </span>            : static inline int __pmd_alloc(struct mm_struct *mm, pud_t *pud,
<span class="lineNum">    1373 </span>            :                                                 unsigned long address)
<span class="lineNum">    1374 </span>            : {
<span class="lineNum">    1375 </span>            :         return 0;
<span class="lineNum">    1376 </span>            : }
<span class="lineNum">    1377 </span>            : #else
<span class="lineNum">    1378 </span>            : int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);
<span class="lineNum">    1379 </span>            : #endif
<span class="lineNum">    1380 </span>            : 
<span class="lineNum">    1381 </span>            : int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,
<span class="lineNum">    1382 </span>            :                 pmd_t *pmd, unsigned long address);
<span class="lineNum">    1383 </span>            : int __pte_alloc_kernel(pmd_t *pmd, unsigned long address);
<span class="lineNum">    1384 </span>            : 
<span class="lineNum">    1385 </span>            : /*
<span class="lineNum">    1386 </span>            :  * The following ifdef needed to get the 4level-fixup.h header to work.
<span class="lineNum">    1387 </span>            :  * Remove it when 4level-fixup.h has been removed.
<span class="lineNum">    1388 </span>            :  */
<span class="lineNum">    1389 </span>            : #if defined(CONFIG_MMU) &amp;&amp; !defined(__ARCH_HAS_4LEVEL_HACK)
<span class="lineNum">    1390 </span>            : static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
<span class="lineNum">    1391 </span>            : {
<span class="lineNum">    1392 </span>            :         return (unlikely(pgd_none(*pgd)) &amp;&amp; __pud_alloc(mm, pgd, address))?
<span class="lineNum">    1393 </span>            :                 NULL: pud_offset(pgd, address);
<span class="lineNum">    1394 </span>            : }
<span class="lineNum">    1395 </span>            : 
<span class="lineNum">    1396 </span>            : static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
<span class="lineNum">    1397 </span>            : {
<span class="lineNum">    1398 </span>            :         return (unlikely(pud_none(*pud)) &amp;&amp; __pmd_alloc(mm, pud, address))?
<span class="lineNum">    1399 </span>            :                 NULL: pmd_offset(pud, address);
<span class="lineNum">    1400 </span>            : }
<span class="lineNum">    1401 </span>            : #endif /* CONFIG_MMU &amp;&amp; !__ARCH_HAS_4LEVEL_HACK */
<span class="lineNum">    1402 </span>            : 
<span class="lineNum">    1403 </span>            : #if USE_SPLIT_PTE_PTLOCKS
<span class="lineNum">    1404 </span>            : #if ALLOC_SPLIT_PTLOCKS
<span class="lineNum">    1405 </span>            : void __init ptlock_cache_init(void);
<span class="lineNum">    1406 </span>            : extern bool ptlock_alloc(struct page *page);
<span class="lineNum">    1407 </span>            : extern void ptlock_free(struct page *page);
<span class="lineNum">    1408 </span>            : 
<span class="lineNum">    1409 </span>            : static inline spinlock_t *ptlock_ptr(struct page *page)
<span class="lineNum">    1410 </span>            : {
<span class="lineNum">    1411 </span>            :         return page-&gt;ptl;
<span class="lineNum">    1412 </span>            : }
<span class="lineNum">    1413 </span>            : #else /* ALLOC_SPLIT_PTLOCKS */
<span class="lineNum">    1414 </span>            : static inline void ptlock_cache_init(void)
<span class="lineNum">    1415 </span>            : {
<span class="lineNum">    1416 </span>            : }
<span class="lineNum">    1417 </span>            : 
<span class="lineNum">    1418 </span>            : static inline bool ptlock_alloc(struct page *page)
<span class="lineNum">    1419 </span>            : {
<span class="lineNum">    1420 </span>            :         return true;
<span class="lineNum">    1421 </span>            : }
<span class="lineNum">    1422 </span>            : 
<span class="lineNum">    1423 </span>            : static inline void ptlock_free(struct page *page)
<span class="lineNum">    1424 </span>            : {
<span class="lineNum">    1425 </span>            : }
<span class="lineNum">    1426 </span>            : 
<span class="lineNum">    1427 </span>            : static inline spinlock_t *ptlock_ptr(struct page *page)
<span class="lineNum">    1428 </span>            : {
<span class="lineNum">    1429 </span>            :         return &amp;page-&gt;ptl;
<span class="lineNum">    1430 </span>            : }
<span class="lineNum">    1431 </span>            : #endif /* ALLOC_SPLIT_PTLOCKS */
<span class="lineNum">    1432 </span>            : 
<span class="lineNum">    1433 </span>            : static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
<span class="lineNum">    1434 </span>            : {
<span class="lineNum">    1435 </span>            :         return ptlock_ptr(pmd_page(*pmd));
<span class="lineNum">    1436 </span>            : }
<span class="lineNum">    1437 </span>            : 
<span class="lineNum">    1438 </span>            : static inline bool ptlock_init(struct page *page)
<span class="lineNum">    1439 </span>            : {
<span class="lineNum">    1440 </span>            :         /*
<span class="lineNum">    1441 </span>            :          * prep_new_page() initialize page-&gt;private (and therefore page-&gt;ptl)
<span class="lineNum">    1442 </span>            :          * with 0. Make sure nobody took it in use in between.
<span class="lineNum">    1443 </span>            :          *
<span class="lineNum">    1444 </span>            :          * It can happen if arch try to use slab for page table allocation:
<span class="lineNum">    1445 </span>            :          * slab code uses page-&gt;slab_cache and page-&gt;first_page (for tail
<span class="lineNum">    1446 </span>            :          * pages), which share storage with page-&gt;ptl.
<span class="lineNum">    1447 </span>            :          */
<span class="lineNum">    1448 </span>            :         VM_BUG_ON_PAGE(*(unsigned long *)&amp;page-&gt;ptl, page);
<span class="lineNum">    1449 </span>            :         if (!ptlock_alloc(page))
<span class="lineNum">    1450 </span>            :                 return false;
<span class="lineNum">    1451 </span>            :         spin_lock_init(ptlock_ptr(page));
<span class="lineNum">    1452 </span>            :         return true;
<span class="lineNum">    1453 </span>            : }
<span class="lineNum">    1454 </span>            : 
<span class="lineNum">    1455 </span>            : /* Reset page-&gt;mapping so free_pages_check won't complain. */
<span class="lineNum">    1456 </span>            : static inline void pte_lock_deinit(struct page *page)
<span class="lineNum">    1457 </span>            : {
<span class="lineNum">    1458 </span>            :         page-&gt;mapping = NULL;
<span class="lineNum">    1459 </span>            :         ptlock_free(page);
<span class="lineNum">    1460 </span>            : }
<span class="lineNum">    1461 </span>            : 
<span class="lineNum">    1462 </span>            : #else   /* !USE_SPLIT_PTE_PTLOCKS */
<span class="lineNum">    1463 </span>            : /*
<span class="lineNum">    1464 </span>            :  * We use mm-&gt;page_table_lock to guard all pagetable pages of the mm.
<span class="lineNum">    1465 </span>            :  */
<span class="lineNum">    1466 </span>            : static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
<span class="lineNum">    1467 </span>            : {
<span class="lineNum">    1468 </span>            :         return &amp;mm-&gt;page_table_lock;
<span class="lineNum">    1469 </span>            : }
<span class="lineNum">    1470 </span>            : static inline void ptlock_cache_init(void) {}
<span class="lineNum">    1471 </span>            : static inline bool ptlock_init(struct page *page) { return true; }
<span class="lineNum">    1472 </span>            : static inline void pte_lock_deinit(struct page *page) {}
<span class="lineNum">    1473 </span>            : #endif /* USE_SPLIT_PTE_PTLOCKS */
<span class="lineNum">    1474 </span>            : 
<span class="lineNum">    1475 </span>            : static inline void pgtable_init(void)
<span class="lineNum">    1476 </span>            : {
<span class="lineNum">    1477 </span>            :         ptlock_cache_init();
<span class="lineNum">    1478 </span>            :         pgtable_cache_init();
<span class="lineNum">    1479 </span>            : }
<span class="lineNum">    1480 </span>            : 
<span class="lineNum">    1481 </span>            : static inline bool pgtable_page_ctor(struct page *page)
<span class="lineNum">    1482 </span>            : {
<span class="lineNum">    1483 </span>            :         inc_zone_page_state(page, NR_PAGETABLE);
<span class="lineNum">    1484 </span>            :         return ptlock_init(page);
<span class="lineNum">    1485 </span>            : }
<span class="lineNum">    1486 </span>            : 
<span class="lineNum">    1487 </span>            : static inline void pgtable_page_dtor(struct page *page)
<span class="lineNum">    1488 </span>            : {
<span class="lineNum">    1489 </span>            :         pte_lock_deinit(page);
<span class="lineNum">    1490 </span>            :         dec_zone_page_state(page, NR_PAGETABLE);
<span class="lineNum">    1491 </span>            : }
<span class="lineNum">    1492 </span>            : 
<span class="lineNum">    1493 </span>            : #define pte_offset_map_lock(mm, pmd, address, ptlp)     \
<span class="lineNum">    1494 </span>            : ({                                                      \
<span class="lineNum">    1495 </span>            :         spinlock_t *__ptl = pte_lockptr(mm, pmd);       \
<span class="lineNum">    1496 </span>            :         pte_t *__pte = pte_offset_map(pmd, address);    \
<span class="lineNum">    1497 </span>            :         *(ptlp) = __ptl;                                \
<span class="lineNum">    1498 </span>            :         spin_lock(__ptl);                               \
<span class="lineNum">    1499 </span>            :         __pte;                                          \
<span class="lineNum">    1500 </span>            : })
<span class="lineNum">    1501 </span>            : 
<span class="lineNum">    1502 </span>            : #define pte_unmap_unlock(pte, ptl)      do {            \
<span class="lineNum">    1503 </span>            :         spin_unlock(ptl);                               \
<span class="lineNum">    1504 </span>            :         pte_unmap(pte);                                 \
<span class="lineNum">    1505 </span>            : } while (0)
<span class="lineNum">    1506 </span>            : 
<span class="lineNum">    1507 </span>            : #define pte_alloc_map(mm, vma, pmd, address)                            \
<span class="lineNum">    1508 </span>            :         ((unlikely(pmd_none(*(pmd))) &amp;&amp; __pte_alloc(mm, vma,    \
<span class="lineNum">    1509 </span>            :                                                         pmd, address))? \
<span class="lineNum">    1510 </span>            :          NULL: pte_offset_map(pmd, address))
<span class="lineNum">    1511 </span>            : 
<span class="lineNum">    1512 </span>            : #define pte_alloc_map_lock(mm, pmd, address, ptlp)      \
<span class="lineNum">    1513 </span>            :         ((unlikely(pmd_none(*(pmd))) &amp;&amp; __pte_alloc(mm, NULL,   \
<span class="lineNum">    1514 </span>            :                                                         pmd, address))? \
<span class="lineNum">    1515 </span>            :                 NULL: pte_offset_map_lock(mm, pmd, address, ptlp))
<span class="lineNum">    1516 </span>            : 
<span class="lineNum">    1517 </span>            : #define pte_alloc_kernel(pmd, address)                  \
<span class="lineNum">    1518 </span>            :         ((unlikely(pmd_none(*(pmd))) &amp;&amp; __pte_alloc_kernel(pmd, address))? \
<span class="lineNum">    1519 </span>            :                 NULL: pte_offset_kernel(pmd, address))
<span class="lineNum">    1520 </span>            : 
<span class="lineNum">    1521 </span>            : #if USE_SPLIT_PMD_PTLOCKS
<span class="lineNum">    1522 </span>            : 
<span class="lineNum">    1523 </span>            : static struct page *pmd_to_page(pmd_t *pmd)
<span class="lineNum">    1524 </span>            : {
<span class="lineNum">    1525 </span>            :         unsigned long mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);
<span class="lineNum">    1526 </span>            :         return virt_to_page((void *)((unsigned long) pmd &amp; mask));
<span class="lineNum">    1527 </span>            : }
<span class="lineNum">    1528 </span>            : 
<span class="lineNum">    1529 </span>            : static inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)
<span class="lineNum">    1530 </span>            : {
<span class="lineNum">    1531 </span>            :         return ptlock_ptr(pmd_to_page(pmd));
<span class="lineNum">    1532 </span>            : }
<span class="lineNum">    1533 </span>            : 
<span class="lineNum">    1534 </span>            : static inline bool pgtable_pmd_page_ctor(struct page *page)
<span class="lineNum">    1535 </span>            : {
<span class="lineNum">    1536 </span>            : #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="lineNum">    1537 </span>            :         page-&gt;pmd_huge_pte = NULL;
<span class="lineNum">    1538 </span>            : #endif
<span class="lineNum">    1539 </span>            :         return ptlock_init(page);
<span class="lineNum">    1540 </span>            : }
<span class="lineNum">    1541 </span>            : 
<span class="lineNum">    1542 </span>            : static inline void pgtable_pmd_page_dtor(struct page *page)
<span class="lineNum">    1543 </span>            : {
<span class="lineNum">    1544 </span>            : #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="lineNum">    1545 </span>            :         VM_BUG_ON_PAGE(page-&gt;pmd_huge_pte, page);
<span class="lineNum">    1546 </span>            : #endif
<span class="lineNum">    1547 </span>            :         ptlock_free(page);
<span class="lineNum">    1548 </span>            : }
<span class="lineNum">    1549 </span>            : 
<span class="lineNum">    1550 </span>            : #define pmd_huge_pte(mm, pmd) (pmd_to_page(pmd)-&gt;pmd_huge_pte)
<span class="lineNum">    1551 </span>            : 
<span class="lineNum">    1552 </span>            : #else
<span class="lineNum">    1553 </span>            : 
<span class="lineNum">    1554 </span>            : static inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)
<span class="lineNum">    1555 </span>            : {
<span class="lineNum">    1556 </span>            :         return &amp;mm-&gt;page_table_lock;
<span class="lineNum">    1557 </span>            : }
<span class="lineNum">    1558 </span>            : 
<span class="lineNum">    1559 </span>            : static inline bool pgtable_pmd_page_ctor(struct page *page) { return true; }
<span class="lineNum">    1560 </span>            : static inline void pgtable_pmd_page_dtor(struct page *page) {}
<span class="lineNum">    1561 </span>            : 
<span class="lineNum">    1562 </span>            : #define pmd_huge_pte(mm, pmd) ((mm)-&gt;pmd_huge_pte)
<span class="lineNum">    1563 </span>            : 
<span class="lineNum">    1564 </span>            : #endif
<span class="lineNum">    1565 </span>            : 
<span class="lineNum">    1566 </span>            : static inline spinlock_t *pmd_lock(struct mm_struct *mm, pmd_t *pmd)
<span class="lineNum">    1567 </span>            : {
<span class="lineNum">    1568 </span>            :         spinlock_t *ptl = pmd_lockptr(mm, pmd);
<span class="lineNum">    1569 </span>            :         spin_lock(ptl);
<span class="lineNum">    1570 </span>            :         return ptl;
<span class="lineNum">    1571 </span>            : }
<span class="lineNum">    1572 </span>            : 
<span class="lineNum">    1573 </span>            : extern void free_area_init(unsigned long * zones_size);
<span class="lineNum">    1574 </span>            : extern void free_area_init_node(int nid, unsigned long * zones_size,
<span class="lineNum">    1575 </span>            :                 unsigned long zone_start_pfn, unsigned long *zholes_size);
<span class="lineNum">    1576 </span>            : extern void free_initmem(void);
<span class="lineNum">    1577 </span>            : 
<span class="lineNum">    1578 </span>            : /*
<span class="lineNum">    1579 </span>            :  * Free reserved pages within range [PAGE_ALIGN(start), end &amp; PAGE_MASK)
<span class="lineNum">    1580 </span>            :  * into the buddy system. The freed pages will be poisoned with pattern
<span class="lineNum">    1581 </span>            :  * &quot;poison&quot; if it's within range [0, UCHAR_MAX].
<span class="lineNum">    1582 </span>            :  * Return pages freed into the buddy system.
<span class="lineNum">    1583 </span>            :  */
<span class="lineNum">    1584 </span>            : extern unsigned long free_reserved_area(void *start, void *end,
<span class="lineNum">    1585 </span>            :                                         int poison, char *s);
<span class="lineNum">    1586 </span>            : 
<span class="lineNum">    1587 </span>            : #ifdef  CONFIG_HIGHMEM
<span class="lineNum">    1588 </span>            : /*
<span class="lineNum">    1589 </span>            :  * Free a highmem page into the buddy system, adjusting totalhigh_pages
<span class="lineNum">    1590 </span>            :  * and totalram_pages.
<span class="lineNum">    1591 </span>            :  */
<span class="lineNum">    1592 </span>            : extern void free_highmem_page(struct page *page);
<span class="lineNum">    1593 </span>            : #endif
<span class="lineNum">    1594 </span>            : 
<span class="lineNum">    1595 </span>            : extern void adjust_managed_page_count(struct page *page, long count);
<span class="lineNum">    1596 </span>            : extern void mem_init_print_info(const char *str);
<span class="lineNum">    1597 </span>            : 
<span class="lineNum">    1598 </span>            : /* Free the reserved page into the buddy system, so it gets managed. */
<span class="lineNum">    1599 </span>            : static inline void __free_reserved_page(struct page *page)
<span class="lineNum">    1600 </span>            : {
<span class="lineNum">    1601 </span>            :         ClearPageReserved(page);
<span class="lineNum">    1602 </span>            :         init_page_count(page);
<span class="lineNum">    1603 </span>            :         __free_page(page);
<span class="lineNum">    1604 </span>            : }
<span class="lineNum">    1605 </span>            : 
<span class="lineNum">    1606 </span>            : static inline void free_reserved_page(struct page *page)
<span class="lineNum">    1607 </span>            : {
<span class="lineNum">    1608 </span>            :         __free_reserved_page(page);
<span class="lineNum">    1609 </span>            :         adjust_managed_page_count(page, 1);
<span class="lineNum">    1610 </span>            : }
<span class="lineNum">    1611 </span>            : 
<span class="lineNum">    1612 </span>            : static inline void mark_page_reserved(struct page *page)
<span class="lineNum">    1613 </span>            : {
<span class="lineNum">    1614 </span>            :         SetPageReserved(page);
<span class="lineNum">    1615 </span>            :         adjust_managed_page_count(page, -1);
<span class="lineNum">    1616 </span>            : }
<span class="lineNum">    1617 </span>            : 
<span class="lineNum">    1618 </span>            : /*
<span class="lineNum">    1619 </span>            :  * Default method to free all the __init memory into the buddy system.
<span class="lineNum">    1620 </span>            :  * The freed pages will be poisoned with pattern &quot;poison&quot; if it's within
<span class="lineNum">    1621 </span>            :  * range [0, UCHAR_MAX].
<span class="lineNum">    1622 </span>            :  * Return pages freed into the buddy system.
<span class="lineNum">    1623 </span>            :  */
<span class="lineNum">    1624 </span>            : static inline unsigned long free_initmem_default(int poison)
<span class="lineNum">    1625 </span>            : {
<span class="lineNum">    1626 </span>            :         extern char __init_begin[], __init_end[];
<span class="lineNum">    1627 </span>            : 
<span class="lineNum">    1628 </span>            :         return free_reserved_area(&amp;__init_begin, &amp;__init_end,
<span class="lineNum">    1629 </span>            :                                   poison, &quot;unused kernel&quot;);
<span class="lineNum">    1630 </span>            : }
<span class="lineNum">    1631 </span>            : 
<span class="lineNum">    1632 </span>            : static inline unsigned long get_num_physpages(void)
<span class="lineNum">    1633 </span>            : {
<span class="lineNum">    1634 </span>            :         int nid;
<span class="lineNum">    1635 </span>            :         unsigned long phys_pages = 0;
<span class="lineNum">    1636 </span>            : 
<span class="lineNum">    1637 </span>            :         for_each_online_node(nid)
<span class="lineNum">    1638 </span>            :                 phys_pages += node_present_pages(nid);
<span class="lineNum">    1639 </span>            : 
<span class="lineNum">    1640 </span>            :         return phys_pages;
<span class="lineNum">    1641 </span>            : }
<span class="lineNum">    1642 </span>            : 
<span class="lineNum">    1643 </span>            : #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
<span class="lineNum">    1644 </span>            : /*
<span class="lineNum">    1645 </span>            :  * With CONFIG_HAVE_MEMBLOCK_NODE_MAP set, an architecture may initialise its
<span class="lineNum">    1646 </span>            :  * zones, allocate the backing mem_map and account for memory holes in a more
<span class="lineNum">    1647 </span>            :  * architecture independent manner. This is a substitute for creating the
<span class="lineNum">    1648 </span>            :  * zone_sizes[] and zholes_size[] arrays and passing them to
<span class="lineNum">    1649 </span>            :  * free_area_init_node()
<span class="lineNum">    1650 </span>            :  *
<span class="lineNum">    1651 </span>            :  * An architecture is expected to register range of page frames backed by
<span class="lineNum">    1652 </span>            :  * physical memory with memblock_add[_node]() before calling
<span class="lineNum">    1653 </span>            :  * free_area_init_nodes() passing in the PFN each zone ends at. At a basic
<span class="lineNum">    1654 </span>            :  * usage, an architecture is expected to do something like
<span class="lineNum">    1655 </span>            :  *
<span class="lineNum">    1656 </span>            :  * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,
<span class="lineNum">    1657 </span>            :  *                                                       max_highmem_pfn};
<span class="lineNum">    1658 </span>            :  * for_each_valid_physical_page_range()
<span class="lineNum">    1659 </span>            :  *      memblock_add_node(base, size, nid)
<span class="lineNum">    1660 </span>            :  * free_area_init_nodes(max_zone_pfns);
<span class="lineNum">    1661 </span>            :  *
<span class="lineNum">    1662 </span>            :  * free_bootmem_with_active_regions() calls free_bootmem_node() for each
<span class="lineNum">    1663 </span>            :  * registered physical page range.  Similarly
<span class="lineNum">    1664 </span>            :  * sparse_memory_present_with_active_regions() calls memory_present() for
<span class="lineNum">    1665 </span>            :  * each range when SPARSEMEM is enabled.
<span class="lineNum">    1666 </span>            :  *
<span class="lineNum">    1667 </span>            :  * See mm/page_alloc.c for more information on each function exposed by
<span class="lineNum">    1668 </span>            :  * CONFIG_HAVE_MEMBLOCK_NODE_MAP.
<span class="lineNum">    1669 </span>            :  */
<span class="lineNum">    1670 </span>            : extern void free_area_init_nodes(unsigned long *max_zone_pfn);
<span class="lineNum">    1671 </span>            : unsigned long node_map_pfn_alignment(void);
<span class="lineNum">    1672 </span>            : unsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,
<span class="lineNum">    1673 </span>            :                                                 unsigned long end_pfn);
<span class="lineNum">    1674 </span>            : extern unsigned long absent_pages_in_range(unsigned long start_pfn,
<span class="lineNum">    1675 </span>            :                                                 unsigned long end_pfn);
<span class="lineNum">    1676 </span>            : extern void get_pfn_range_for_nid(unsigned int nid,
<span class="lineNum">    1677 </span>            :                         unsigned long *start_pfn, unsigned long *end_pfn);
<span class="lineNum">    1678 </span>            : extern unsigned long find_min_pfn_with_active_regions(void);
<span class="lineNum">    1679 </span>            : extern void free_bootmem_with_active_regions(int nid,
<span class="lineNum">    1680 </span>            :                                                 unsigned long max_low_pfn);
<span class="lineNum">    1681 </span>            : extern void sparse_memory_present_with_active_regions(int nid);
<span class="lineNum">    1682 </span>            : 
<span class="lineNum">    1683 </span>            : #endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
<span class="lineNum">    1684 </span>            : 
<span class="lineNum">    1685 </span>            : #if !defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) &amp;&amp; \
<span class="lineNum">    1686 </span>            :     !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID)
<span class="lineNum">    1687 </span>            : static inline int __early_pfn_to_nid(unsigned long pfn)
<span class="lineNum">    1688 </span>            : {
<span class="lineNum">    1689 </span>            :         return 0;
<span class="lineNum">    1690 </span>            : }
<span class="lineNum">    1691 </span>            : #else
<span class="lineNum">    1692 </span>            : /* please see mm/page_alloc.c */
<span class="lineNum">    1693 </span>            : extern int __meminit early_pfn_to_nid(unsigned long pfn);
<span class="lineNum">    1694 </span>            : /* there is a per-arch backend function. */
<span class="lineNum">    1695 </span>            : extern int __meminit __early_pfn_to_nid(unsigned long pfn);
<span class="lineNum">    1696 </span>            : #endif
<span class="lineNum">    1697 </span>            : 
<span class="lineNum">    1698 </span>            : extern void set_dma_reserve(unsigned long new_dma_reserve);
<span class="lineNum">    1699 </span>            : extern void memmap_init_zone(unsigned long, int, unsigned long,
<span class="lineNum">    1700 </span>            :                                 unsigned long, enum memmap_context);
<span class="lineNum">    1701 </span>            : extern void setup_per_zone_wmarks(void);
<span class="lineNum">    1702 </span>            : extern int __meminit init_per_zone_wmark_min(void);
<span class="lineNum">    1703 </span>            : extern void mem_init(void);
<span class="lineNum">    1704 </span>            : extern void __init mmap_init(void);
<span class="lineNum">    1705 </span>            : extern void show_mem(unsigned int flags);
<span class="lineNum">    1706 </span>            : extern void si_meminfo(struct sysinfo * val);
<span class="lineNum">    1707 </span>            : extern void si_meminfo_node(struct sysinfo *val, int nid);
<span class="lineNum">    1708 </span>            : 
<span class="lineNum">    1709 </span>            : extern __printf(3, 4)
<span class="lineNum">    1710 </span>            : void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...);
<span class="lineNum">    1711 </span>            : 
<span class="lineNum">    1712 </span>            : extern void setup_per_cpu_pageset(void);
<span class="lineNum">    1713 </span>            : 
<span class="lineNum">    1714 </span>            : extern void zone_pcp_update(struct zone *zone);
<span class="lineNum">    1715 </span>            : extern void zone_pcp_reset(struct zone *zone);
<span class="lineNum">    1716 </span>            : 
<span class="lineNum">    1717 </span>            : /* page_alloc.c */
<span class="lineNum">    1718 </span>            : extern int min_free_kbytes;
<span class="lineNum">    1719 </span>            : 
<span class="lineNum">    1720 </span>            : /* nommu.c */
<span class="lineNum">    1721 </span>            : extern atomic_long_t mmap_pages_allocated;
<span class="lineNum">    1722 </span>            : extern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);
<span class="lineNum">    1723 </span>            : 
<span class="lineNum">    1724 </span>            : /* interval_tree.c */
<span class="lineNum">    1725 </span>            : void vma_interval_tree_insert(struct vm_area_struct *node,
<span class="lineNum">    1726 </span>            :                               struct rb_root *root);
<span class="lineNum">    1727 </span>            : void vma_interval_tree_insert_after(struct vm_area_struct *node,
<span class="lineNum">    1728 </span>            :                                     struct vm_area_struct *prev,
<span class="lineNum">    1729 </span>            :                                     struct rb_root *root);
<span class="lineNum">    1730 </span>            : void vma_interval_tree_remove(struct vm_area_struct *node,
<span class="lineNum">    1731 </span>            :                               struct rb_root *root);
<span class="lineNum">    1732 </span>            : struct vm_area_struct *vma_interval_tree_iter_first(struct rb_root *root,
<span class="lineNum">    1733 </span>            :                                 unsigned long start, unsigned long last);
<span class="lineNum">    1734 </span>            : struct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,
<span class="lineNum">    1735 </span>            :                                 unsigned long start, unsigned long last);
<span class="lineNum">    1736 </span>            : 
<span class="lineNum">    1737 </span>            : #define vma_interval_tree_foreach(vma, root, start, last)               \
<span class="lineNum">    1738 </span>            :         for (vma = vma_interval_tree_iter_first(root, start, last);     \
<span class="lineNum">    1739 </span>            :              vma; vma = vma_interval_tree_iter_next(vma, start, last))
<span class="lineNum">    1740 </span>            : 
<span class="lineNum">    1741 </span>            : static inline void vma_nonlinear_insert(struct vm_area_struct *vma,
<span class="lineNum">    1742 </span>            :                                         struct list_head *list)
<span class="lineNum">    1743 </span>            : {
<span class="lineNum">    1744 </span>            :         list_add_tail(&amp;vma-&gt;shared.nonlinear, list);
<span class="lineNum">    1745 </span>            : }
<span class="lineNum">    1746 </span>            : 
<span class="lineNum">    1747 </span>            : void anon_vma_interval_tree_insert(struct anon_vma_chain *node,
<span class="lineNum">    1748 </span>            :                                    struct rb_root *root);
<span class="lineNum">    1749 </span>            : void anon_vma_interval_tree_remove(struct anon_vma_chain *node,
<span class="lineNum">    1750 </span>            :                                    struct rb_root *root);
<span class="lineNum">    1751 </span>            : struct anon_vma_chain *anon_vma_interval_tree_iter_first(
<span class="lineNum">    1752 </span>            :         struct rb_root *root, unsigned long start, unsigned long last);
<span class="lineNum">    1753 </span>            : struct anon_vma_chain *anon_vma_interval_tree_iter_next(
<span class="lineNum">    1754 </span>            :         struct anon_vma_chain *node, unsigned long start, unsigned long last);
<span class="lineNum">    1755 </span>            : #ifdef CONFIG_DEBUG_VM_RB
<span class="lineNum">    1756 </span>            : void anon_vma_interval_tree_verify(struct anon_vma_chain *node);
<span class="lineNum">    1757 </span>            : #endif
<span class="lineNum">    1758 </span>            : 
<span class="lineNum">    1759 </span>            : #define anon_vma_interval_tree_foreach(avc, root, start, last)           \
<span class="lineNum">    1760 </span>            :         for (avc = anon_vma_interval_tree_iter_first(root, start, last); \
<span class="lineNum">    1761 </span>            :              avc; avc = anon_vma_interval_tree_iter_next(avc, start, last))
<span class="lineNum">    1762 </span>            : 
<span class="lineNum">    1763 </span>            : /* mmap.c */
<span class="lineNum">    1764 </span>            : extern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);
<span class="lineNum">    1765 </span>            : extern int vma_adjust(struct vm_area_struct *vma, unsigned long start,
<span class="lineNum">    1766 </span>            :         unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert);
<span class="lineNum">    1767 </span>            : extern struct vm_area_struct *vma_merge(struct mm_struct *,
<span class="lineNum">    1768 </span>            :         struct vm_area_struct *prev, unsigned long addr, unsigned long end,
<span class="lineNum">    1769 </span>            :         unsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,
<span class="lineNum">    1770 </span>            :         struct mempolicy *);
<span class="lineNum">    1771 </span>            : extern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);
<span class="lineNum">    1772 </span>            : extern int split_vma(struct mm_struct *,
<span class="lineNum">    1773 </span>            :         struct vm_area_struct *, unsigned long addr, int new_below);
<span class="lineNum">    1774 </span>            : extern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);
<span class="lineNum">    1775 </span>            : extern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,
<span class="lineNum">    1776 </span>            :         struct rb_node **, struct rb_node *);
<span class="lineNum">    1777 </span>            : extern void unlink_file_vma(struct vm_area_struct *);
<span class="lineNum">    1778 </span>            : extern struct vm_area_struct *copy_vma(struct vm_area_struct **,
<span class="lineNum">    1779 </span>            :         unsigned long addr, unsigned long len, pgoff_t pgoff,
<span class="lineNum">    1780 </span>            :         bool *need_rmap_locks);
<span class="lineNum">    1781 </span>            : extern void exit_mmap(struct mm_struct *);
<span class="lineNum">    1782 </span>            : 
<span class="lineNum">    1783 </span>            : extern int mm_take_all_locks(struct mm_struct *mm);
<span class="lineNum">    1784 </span>            : extern void mm_drop_all_locks(struct mm_struct *mm);
<span class="lineNum">    1785 </span>            : 
<span class="lineNum">    1786 </span>            : extern void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);
<span class="lineNum">    1787 </span>            : extern struct file *get_mm_exe_file(struct mm_struct *mm);
<span class="lineNum">    1788 </span>            : 
<span class="lineNum">    1789 </span>            : extern int may_expand_vm(struct mm_struct *mm, unsigned long npages);
<span class="lineNum">    1790 </span>            : extern struct vm_area_struct *_install_special_mapping(struct mm_struct *mm,
<span class="lineNum">    1791 </span>            :                                    unsigned long addr, unsigned long len,
<span class="lineNum">    1792 </span>            :                                    unsigned long flags,
<span class="lineNum">    1793 </span>            :                                    const struct vm_special_mapping *spec);
<span class="lineNum">    1794 </span>            : /* This is an obsolete alternative to _install_special_mapping. */
<span class="lineNum">    1795 </span>            : extern int install_special_mapping(struct mm_struct *mm,
<span class="lineNum">    1796 </span>            :                                    unsigned long addr, unsigned long len,
<span class="lineNum">    1797 </span>            :                                    unsigned long flags, struct page **pages);
<span class="lineNum">    1798 </span>            : 
<span class="lineNum">    1799 </span>            : extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
<span class="lineNum">    1800 </span>            : 
<span class="lineNum">    1801 </span>            : extern unsigned long mmap_region(struct file *file, unsigned long addr,
<span class="lineNum">    1802 </span>            :         unsigned long len, vm_flags_t vm_flags, unsigned long pgoff);
<span class="lineNum">    1803 </span>            : extern unsigned long do_mmap_pgoff(struct file *file, unsigned long addr,
<span class="lineNum">    1804 </span>            :         unsigned long len, unsigned long prot, unsigned long flags,
<span class="lineNum">    1805 </span>            :         unsigned long pgoff, unsigned long *populate);
<span class="lineNum">    1806 </span>            : extern int do_munmap(struct mm_struct *, unsigned long, size_t);
<span class="lineNum">    1807 </span>            : 
<span class="lineNum">    1808 </span>            : #ifdef CONFIG_MMU
<span class="lineNum">    1809 </span>            : extern int __mm_populate(unsigned long addr, unsigned long len,
<span class="lineNum">    1810 </span>            :                          int ignore_errors);
<span class="lineNum">    1811 </span>            : static inline void mm_populate(unsigned long addr, unsigned long len)
<span class="lineNum">    1812 </span>            : {
<span class="lineNum">    1813 </span>            :         /* Ignore errors */
<span class="lineNum">    1814 </span>            :         (void) __mm_populate(addr, len, 1);
<span class="lineNum">    1815 </span>            : }
<span class="lineNum">    1816 </span>            : #else
<span class="lineNum">    1817 </span>            : static inline void mm_populate(unsigned long addr, unsigned long len) {}
<span class="lineNum">    1818 </span>            : #endif
<span class="lineNum">    1819 </span>            : 
<span class="lineNum">    1820 </span>            : /* These take the mm semaphore themselves */
<span class="lineNum">    1821 </span>            : extern unsigned long vm_brk(unsigned long, unsigned long);
<span class="lineNum">    1822 </span>            : extern int vm_munmap(unsigned long, size_t);
<span class="lineNum">    1823 </span>            : extern unsigned long vm_mmap(struct file *, unsigned long,
<span class="lineNum">    1824 </span>            :         unsigned long, unsigned long,
<span class="lineNum">    1825 </span>            :         unsigned long, unsigned long);
<span class="lineNum">    1826 </span>            : 
<span class="lineNum">    1827 </span>            : struct vm_unmapped_area_info {
<span class="lineNum">    1828 </span>            : #define VM_UNMAPPED_AREA_TOPDOWN 1
<span class="lineNum">    1829 </span>            :         unsigned long flags;
<span class="lineNum">    1830 </span>            :         unsigned long length;
<span class="lineNum">    1831 </span>            :         unsigned long low_limit;
<span class="lineNum">    1832 </span>            :         unsigned long high_limit;
<span class="lineNum">    1833 </span>            :         unsigned long align_mask;
<span class="lineNum">    1834 </span>            :         unsigned long align_offset;
<span class="lineNum">    1835 </span>            : };
<span class="lineNum">    1836 </span>            : 
<span class="lineNum">    1837 </span>            : extern unsigned long unmapped_area(struct vm_unmapped_area_info *info);
<span class="lineNum">    1838 </span>            : extern unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info);
<span class="lineNum">    1839 </span>            : 
<span class="lineNum">    1840 </span>            : /*
<span class="lineNum">    1841 </span>            :  * Search for an unmapped address range.
<span class="lineNum">    1842 </span>            :  *
<span class="lineNum">    1843 </span>            :  * We are looking for a range that:
<span class="lineNum">    1844 </span>            :  * - does not intersect with any VMA;
<span class="lineNum">    1845 </span>            :  * - is contained within the [low_limit, high_limit) interval;
<span class="lineNum">    1846 </span>            :  * - is at least the desired size.
<span class="lineNum">    1847 </span>            :  * - satisfies (begin_addr &amp; align_mask) == (align_offset &amp; align_mask)
<span class="lineNum">    1848 </span>            :  */
<span class="lineNum">    1849 </span>            : static inline unsigned long
<span class="lineNum">    1850 </span>            : vm_unmapped_area(struct vm_unmapped_area_info *info)
<span class="lineNum">    1851 </span>            : {
<span class="lineNum">    1852 </span>            :         if (!(info-&gt;flags &amp; VM_UNMAPPED_AREA_TOPDOWN))
<span class="lineNum">    1853 </span>            :                 return unmapped_area(info);
<span class="lineNum">    1854 </span>            :         else
<span class="lineNum">    1855 </span>            :                 return unmapped_area_topdown(info);
<span class="lineNum">    1856 </span>            : }
<span class="lineNum">    1857 </span>            : 
<span class="lineNum">    1858 </span>            : /* truncate.c */
<span class="lineNum">    1859 </span>            : extern void truncate_inode_pages(struct address_space *, loff_t);
<span class="lineNum">    1860 </span>            : extern void truncate_inode_pages_range(struct address_space *,
<span class="lineNum">    1861 </span>            :                                        loff_t lstart, loff_t lend);
<span class="lineNum">    1862 </span>            : extern void truncate_inode_pages_final(struct address_space *);
<span class="lineNum">    1863 </span>            : 
<span class="lineNum">    1864 </span>            : /* generic vm_area_ops exported for stackable file systems */
<span class="lineNum">    1865 </span>            : extern int filemap_fault(struct vm_area_struct *, struct vm_fault *);
<span class="lineNum">    1866 </span>            : extern void filemap_map_pages(struct vm_area_struct *vma, struct vm_fault *vmf);
<span class="lineNum">    1867 </span>            : extern int filemap_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);
<span class="lineNum">    1868 </span>            : 
<span class="lineNum">    1869 </span>            : /* mm/page-writeback.c */
<span class="lineNum">    1870 </span>            : int write_one_page(struct page *page, int wait);
<span class="lineNum">    1871 </span>            : void task_dirty_inc(struct task_struct *tsk);
<span class="lineNum">    1872 </span>            : 
<span class="lineNum">    1873 </span>            : /* readahead.c */
<span class="lineNum">    1874 </span>            : #define VM_MAX_READAHEAD        128     /* kbytes */
<span class="lineNum">    1875 </span>            : #define VM_MIN_READAHEAD        16      /* kbytes (includes current page) */
<span class="lineNum">    1876 </span>            : 
<span class="lineNum">    1877 </span>            : int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
<span class="lineNum">    1878 </span>            :                         pgoff_t offset, unsigned long nr_to_read);
<span class="lineNum">    1879 </span>            : 
<span class="lineNum">    1880 </span>            : void page_cache_sync_readahead(struct address_space *mapping,
<span class="lineNum">    1881 </span>            :                                struct file_ra_state *ra,
<span class="lineNum">    1882 </span>            :                                struct file *filp,
<span class="lineNum">    1883 </span>            :                                pgoff_t offset,
<span class="lineNum">    1884 </span>            :                                unsigned long size);
<span class="lineNum">    1885 </span>            : 
<span class="lineNum">    1886 </span>            : void page_cache_async_readahead(struct address_space *mapping,
<span class="lineNum">    1887 </span>            :                                 struct file_ra_state *ra,
<span class="lineNum">    1888 </span>            :                                 struct file *filp,
<span class="lineNum">    1889 </span>            :                                 struct page *pg,
<span class="lineNum">    1890 </span>            :                                 pgoff_t offset,
<span class="lineNum">    1891 </span>            :                                 unsigned long size);
<span class="lineNum">    1892 </span>            : 
<span class="lineNum">    1893 </span>            : unsigned long max_sane_readahead(unsigned long nr);
<span class="lineNum">    1894 </span>            : 
<span class="lineNum">    1895 </span>            : /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
<span class="lineNum">    1896 </span>            : extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
<span class="lineNum">    1897 </span>            : 
<span class="lineNum">    1898 </span>            : /* CONFIG_STACK_GROWSUP still needs to to grow downwards at some places */
<span class="lineNum">    1899 </span>            : extern int expand_downwards(struct vm_area_struct *vma,
<span class="lineNum">    1900 </span>            :                 unsigned long address);
<span class="lineNum">    1901 </span>            : #if VM_GROWSUP
<span class="lineNum">    1902 </span>            : extern int expand_upwards(struct vm_area_struct *vma, unsigned long address);
<span class="lineNum">    1903 </span>            : #else
<span class="lineNum">    1904 </span>            :   #define expand_upwards(vma, address) do { } while (0)
<span class="lineNum">    1905 </span>            : #endif
<span class="lineNum">    1906 </span>            : 
<span class="lineNum">    1907 </span>            : /* Look up the first VMA which satisfies  addr &lt; vm_end,  NULL if none. */
<span class="lineNum">    1908 </span>            : extern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);
<span class="lineNum">    1909 </span>            : extern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,
<span class="lineNum">    1910 </span>            :                                              struct vm_area_struct **pprev);
<span class="lineNum">    1911 </span>            : 
<span class="lineNum">    1912 </span>            : /* Look up the first VMA which intersects the interval start_addr..end_addr-1,
<span class="lineNum">    1913 </span>            :    NULL if none.  Assume start_addr &lt; end_addr. */
<span class="lineNum">    1914 </span>            : static inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)
<span class="lineNum">    1915 </span>            : {
<span class="lineNum">    1916 </span>            :         struct vm_area_struct * vma = find_vma(mm,start_addr);
<span class="lineNum">    1917 </span>            : 
<span class="lineNum">    1918 </span>            :         if (vma &amp;&amp; end_addr &lt;= vma-&gt;vm_start)
<span class="lineNum">    1919 </span>            :                 vma = NULL;
<span class="lineNum">    1920 </span>            :         return vma;
<span class="lineNum">    1921 </span>            : }
<span class="lineNum">    1922 </span>            : 
<span class="lineNum">    1923 </span>            : static inline unsigned long vma_pages(struct vm_area_struct *vma)
<span class="lineNum">    1924 </span>            : {
<span class="lineNum">    1925 </span>            :         return (vma-&gt;vm_end - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;
<span class="lineNum">    1926 </span>            : }
<span class="lineNum">    1927 </span>            : 
<span class="lineNum">    1928 </span>            : /* Look up the first VMA which exactly match the interval vm_start ... vm_end */
<span class="lineNum">    1929 </span>            : static inline struct vm_area_struct *find_exact_vma(struct mm_struct *mm,
<span class="lineNum">    1930 </span>            :                                 unsigned long vm_start, unsigned long vm_end)
<span class="lineNum">    1931 </span>            : {
<span class="lineNum">    1932 </span>            :         struct vm_area_struct *vma = find_vma(mm, vm_start);
<span class="lineNum">    1933 </span>            : 
<span class="lineNum">    1934 </span>            :         if (vma &amp;&amp; (vma-&gt;vm_start != vm_start || vma-&gt;vm_end != vm_end))
<span class="lineNum">    1935 </span>            :                 vma = NULL;
<span class="lineNum">    1936 </span>            : 
<span class="lineNum">    1937 </span>            :         return vma;
<span class="lineNum">    1938 </span>            : }
<span class="lineNum">    1939 </span>            : 
<span class="lineNum">    1940 </span>            : #ifdef CONFIG_MMU
<span class="lineNum">    1941 </span>            : pgprot_t vm_get_page_prot(unsigned long vm_flags);
<span class="lineNum">    1942 </span>            : #else
<span class="lineNum">    1943 </span>            : static inline pgprot_t vm_get_page_prot(unsigned long vm_flags)
<span class="lineNum">    1944 </span>            : {
<span class="lineNum">    1945 </span>            :         return __pgprot(0);
<span class="lineNum">    1946 </span>            : }
<span class="lineNum">    1947 </span>            : #endif
<span class="lineNum">    1948 </span>            : 
<span class="lineNum">    1949 </span>            : #ifdef CONFIG_NUMA_BALANCING
<span class="lineNum">    1950 </span>            : unsigned long change_prot_numa(struct vm_area_struct *vma,
<span class="lineNum">    1951 </span>            :                         unsigned long start, unsigned long end);
<span class="lineNum">    1952 </span>            : #endif
<span class="lineNum">    1953 </span>            : 
<span class="lineNum">    1954 </span>            : struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);
<span class="lineNum">    1955 </span>            : int remap_pfn_range(struct vm_area_struct *, unsigned long addr,
<span class="lineNum">    1956 </span>            :                         unsigned long pfn, unsigned long size, pgprot_t);
<span class="lineNum">    1957 </span>            : int vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);
<span class="lineNum">    1958 </span>            : int vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,
<span class="lineNum">    1959 </span>            :                         unsigned long pfn);
<span class="lineNum">    1960 </span>            : int vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
<span class="lineNum">    1961 </span>            :                         unsigned long pfn);
<span class="lineNum">    1962 </span>            : int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len);
<span class="lineNum">    1963 </span>            : 
<span class="lineNum">    1964 </span>            : 
<span class="lineNum">    1965 </span>            : struct page *follow_page_mask(struct vm_area_struct *vma,
<span class="lineNum">    1966 </span>            :                               unsigned long address, unsigned int foll_flags,
<span class="lineNum">    1967 </span>            :                               unsigned int *page_mask);
<span class="lineNum">    1968 </span>            : 
<span class="lineNum">    1969 </span>            : static inline struct page *follow_page(struct vm_area_struct *vma,
<span class="lineNum">    1970 </span>            :                 unsigned long address, unsigned int foll_flags)
<span class="lineNum">    1971 </span>            : {
<span class="lineNum">    1972 </span>            :         unsigned int unused_page_mask;
<span class="lineNum">    1973 </span>            :         return follow_page_mask(vma, address, foll_flags, &amp;unused_page_mask);
<span class="lineNum">    1974 </span>            : }
<span class="lineNum">    1975 </span>            : 
<span class="lineNum">    1976 </span>            : #define FOLL_WRITE      0x01    /* check pte is writable */
<span class="lineNum">    1977 </span>            : #define FOLL_TOUCH      0x02    /* mark page accessed */
<span class="lineNum">    1978 </span>            : #define FOLL_GET        0x04    /* do get_page on page */
<span class="lineNum">    1979 </span>            : #define FOLL_DUMP       0x08    /* give error on hole if it would be zero */
<span class="lineNum">    1980 </span>            : #define FOLL_FORCE      0x10    /* get_user_pages read/write w/o permission */
<span class="lineNum">    1981 </span>            : #define FOLL_NOWAIT     0x20    /* if a disk transfer is needed, start the IO
<span class="lineNum">    1982 </span>            :                                  * and return without waiting upon it */
<span class="lineNum">    1983 </span>            : #define FOLL_MLOCK      0x40    /* mark page as mlocked */
<span class="lineNum">    1984 </span>            : #define FOLL_SPLIT      0x80    /* don't return transhuge pages, split them */
<span class="lineNum">    1985 </span>            : #define FOLL_HWPOISON   0x100   /* check page is hwpoisoned */
<span class="lineNum">    1986 </span>            : #define FOLL_NUMA       0x200   /* force NUMA hinting page fault */
<span class="lineNum">    1987 </span>            : #define FOLL_MIGRATION  0x400   /* wait for page to replace migration entry */
<span class="lineNum">    1988 </span>            : 
<span class="lineNum">    1989 </span>            : typedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,
<span class="lineNum">    1990 </span>            :                         void *data);
<span class="lineNum">    1991 </span>            : extern int apply_to_page_range(struct mm_struct *mm, unsigned long address,
<span class="lineNum">    1992 </span>            :                                unsigned long size, pte_fn_t fn, void *data);
<span class="lineNum">    1993 </span>            : 
<span class="lineNum">    1994 </span>            : #ifdef CONFIG_PROC_FS
<span class="lineNum">    1995 </span>            : void vm_stat_account(struct mm_struct *, unsigned long, struct file *, long);
<span class="lineNum">    1996 </span>            : #else
<span class="lineNum">    1997 </span>            : static inline void vm_stat_account(struct mm_struct *mm,
<span class="lineNum">    1998 </span>            :                         unsigned long flags, struct file *file, long pages)
<span class="lineNum">    1999 </span>            : {
<span class="lineNum">    2000 </span>            :         mm-&gt;total_vm += pages;
<span class="lineNum">    2001 </span>            : }
<span class="lineNum">    2002 </span>            : #endif /* CONFIG_PROC_FS */
<span class="lineNum">    2003 </span>            : 
<span class="lineNum">    2004 </span>            : #ifdef CONFIG_DEBUG_PAGEALLOC
<span class="lineNum">    2005 </span>            : extern void kernel_map_pages(struct page *page, int numpages, int enable);
<span class="lineNum">    2006 </span>            : #ifdef CONFIG_HIBERNATION
<span class="lineNum">    2007 </span>            : extern bool kernel_page_present(struct page *page);
<span class="lineNum">    2008 </span>            : #endif /* CONFIG_HIBERNATION */
<span class="lineNum">    2009 </span>            : #else
<span class="lineNum">    2010 </span>            : static inline void
<span class="lineNum">    2011 </span>            : kernel_map_pages(struct page *page, int numpages, int enable) {}
<span class="lineNum">    2012 </span>            : #ifdef CONFIG_HIBERNATION
<span class="lineNum">    2013 </span>            : static inline bool kernel_page_present(struct page *page) { return true; }
<span class="lineNum">    2014 </span>            : #endif /* CONFIG_HIBERNATION */
<span class="lineNum">    2015 </span>            : #endif
<span class="lineNum">    2016 </span>            : 
<span class="lineNum">    2017 </span>            : #ifdef __HAVE_ARCH_GATE_AREA
<span class="lineNum">    2018 </span>            : extern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);
<span class="lineNum">    2019 </span>            : extern int in_gate_area_no_mm(unsigned long addr);
<span class="lineNum">    2020 </span>            : extern int in_gate_area(struct mm_struct *mm, unsigned long addr);
<span class="lineNum">    2021 </span>            : #else
<span class="lineNum">    2022 </span>            : static inline struct vm_area_struct *get_gate_vma(struct mm_struct *mm)
<span class="lineNum">    2023 </span>            : {
<span class="lineNum">    2024 </span>            :         return NULL;
<span class="lineNum">    2025 </span>            : }
<span class="lineNum">    2026 </span>            : static inline int in_gate_area_no_mm(unsigned long addr) { return 0; }
<span class="lineNum">    2027 </span>            : static inline int in_gate_area(struct mm_struct *mm, unsigned long addr)
<span class="lineNum">    2028 </span>            : {
<span class="lineNum">    2029 </span>            :         return 0;
<span class="lineNum">    2030 </span>            : }
<span class="lineNum">    2031 </span>            : #endif  /* __HAVE_ARCH_GATE_AREA */
<span class="lineNum">    2032 </span>            : 
<span class="lineNum">    2033 </span>            : #ifdef CONFIG_SYSCTL
<span class="lineNum">    2034 </span>            : extern int sysctl_drop_caches;
<span class="lineNum">    2035 </span>            : int drop_caches_sysctl_handler(struct ctl_table *, int,
<span class="lineNum">    2036 </span>            :                                         void __user *, size_t *, loff_t *);
<span class="lineNum">    2037 </span>            : #endif
<span class="lineNum">    2038 </span>            : 
<span class="lineNum">    2039 </span>            : unsigned long shrink_slab(struct shrink_control *shrink,
<span class="lineNum">    2040 </span>            :                           unsigned long nr_pages_scanned,
<span class="lineNum">    2041 </span>            :                           unsigned long lru_pages);
<span class="lineNum">    2042 </span>            : 
<span class="lineNum">    2043 </span>            : #ifndef CONFIG_MMU
<span class="lineNum">    2044 </span>            : #define randomize_va_space 0
<span class="lineNum">    2045 </span>            : #else
<span class="lineNum">    2046 </span>            : extern int randomize_va_space;
<span class="lineNum">    2047 </span>            : #endif
<span class="lineNum">    2048 </span>            : 
<span class="lineNum">    2049 </span>            : const char * arch_vma_name(struct vm_area_struct *vma);
<span class="lineNum">    2050 </span>            : void print_vma_addr(char *prefix, unsigned long rip);
<span class="lineNum">    2051 </span>            : 
<span class="lineNum">    2052 </span>            : void sparse_mem_maps_populate_node(struct page **map_map,
<span class="lineNum">    2053 </span>            :                                    unsigned long pnum_begin,
<span class="lineNum">    2054 </span>            :                                    unsigned long pnum_end,
<span class="lineNum">    2055 </span>            :                                    unsigned long map_count,
<span class="lineNum">    2056 </span>            :                                    int nodeid);
<span class="lineNum">    2057 </span>            : 
<span class="lineNum">    2058 </span>            : struct page *sparse_mem_map_populate(unsigned long pnum, int nid);
<span class="lineNum">    2059 </span>            : pgd_t *vmemmap_pgd_populate(unsigned long addr, int node);
<span class="lineNum">    2060 </span>            : pud_t *vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node);
<span class="lineNum">    2061 </span>            : pmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);
<span class="lineNum">    2062 </span>            : pte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node);
<span class="lineNum">    2063 </span>            : void *vmemmap_alloc_block(unsigned long size, int node);
<span class="lineNum">    2064 </span>            : void *vmemmap_alloc_block_buf(unsigned long size, int node);
<span class="lineNum">    2065 </span>            : void vmemmap_verify(pte_t *, int, unsigned long, unsigned long);
<span class="lineNum">    2066 </span>            : int vmemmap_populate_basepages(unsigned long start, unsigned long end,
<span class="lineNum">    2067 </span>            :                                int node);
<span class="lineNum">    2068 </span>            : int vmemmap_populate(unsigned long start, unsigned long end, int node);
<span class="lineNum">    2069 </span>            : void vmemmap_populate_print_last(void);
<span class="lineNum">    2070 </span>            : #ifdef CONFIG_MEMORY_HOTPLUG
<span class="lineNum">    2071 </span>            : void vmemmap_free(unsigned long start, unsigned long end);
<span class="lineNum">    2072 </span>            : #endif
<span class="lineNum">    2073 </span>            : void register_page_bootmem_memmap(unsigned long section_nr, struct page *map,
<span class="lineNum">    2074 </span>            :                                   unsigned long size);
<span class="lineNum">    2075 </span>            : 
<span class="lineNum">    2076 </span>            : enum mf_flags {
<span class="lineNum">    2077 </span>            :         MF_COUNT_INCREASED = 1 &lt;&lt; 0,
<span class="lineNum">    2078 </span>            :         MF_ACTION_REQUIRED = 1 &lt;&lt; 1,
<span class="lineNum">    2079 </span>            :         MF_MUST_KILL = 1 &lt;&lt; 2,
<span class="lineNum">    2080 </span>            :         MF_SOFT_OFFLINE = 1 &lt;&lt; 3,
<span class="lineNum">    2081 </span>            : };
<span class="lineNum">    2082 </span>            : extern int memory_failure(unsigned long pfn, int trapno, int flags);
<span class="lineNum">    2083 </span>            : extern void memory_failure_queue(unsigned long pfn, int trapno, int flags);
<span class="lineNum">    2084 </span>            : extern int unpoison_memory(unsigned long pfn);
<span class="lineNum">    2085 </span>            : extern int sysctl_memory_failure_early_kill;
<span class="lineNum">    2086 </span>            : extern int sysctl_memory_failure_recovery;
<span class="lineNum">    2087 </span>            : extern void shake_page(struct page *p, int access);
<span class="lineNum">    2088 </span>            : extern atomic_long_t num_poisoned_pages;
<span class="lineNum">    2089 </span>            : extern int soft_offline_page(struct page *page, int flags);
<span class="lineNum">    2090 </span>            : 
<span class="lineNum">    2091 </span>            : #if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)
<span class="lineNum">    2092 </span>            : extern void clear_huge_page(struct page *page,
<span class="lineNum">    2093 </span>            :                             unsigned long addr,
<span class="lineNum">    2094 </span>            :                             unsigned int pages_per_huge_page);
<span class="lineNum">    2095 </span>            : extern void copy_user_huge_page(struct page *dst, struct page *src,
<span class="lineNum">    2096 </span>            :                                 unsigned long addr, struct vm_area_struct *vma,
<span class="lineNum">    2097 </span>            :                                 unsigned int pages_per_huge_page);
<span class="lineNum">    2098 </span>            : #endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */
<span class="lineNum">    2099 </span>            : 
<span class="lineNum">    2100 </span>            : #ifdef CONFIG_DEBUG_PAGEALLOC
<span class="lineNum">    2101 </span>            : extern unsigned int _debug_guardpage_minorder;
<span class="lineNum">    2102 </span>            : 
<span class="lineNum">    2103 </span>            : static inline unsigned int debug_guardpage_minorder(void)
<span class="lineNum">    2104 </span>            : {
<span class="lineNum">    2105 </span>            :         return _debug_guardpage_minorder;
<span class="lineNum">    2106 </span>            : }
<span class="lineNum">    2107 </span>            : 
<span class="lineNum">    2108 </span>            : static inline bool page_is_guard(struct page *page)
<span class="lineNum">    2109 </span>            : {
<span class="lineNum">    2110 </span>            :         return test_bit(PAGE_DEBUG_FLAG_GUARD, &amp;page-&gt;debug_flags);
<span class="lineNum">    2111 </span>            : }
<span class="lineNum">    2112 </span>            : #else
<span class="lineNum">    2113 </span>            : static inline unsigned int debug_guardpage_minorder(void) { return 0; }
<span class="lineNum">    2114 </span>            : static inline bool page_is_guard(struct page *page) { return false; }
<span class="lineNum">    2115 </span>            : #endif /* CONFIG_DEBUG_PAGEALLOC */
<span class="lineNum">    2116 </span>            : 
<span class="lineNum">    2117 </span>            : #if MAX_NUMNODES &gt; 1
<span class="lineNum">    2118 </span>            : void __init setup_nr_node_ids(void);
<span class="lineNum">    2119 </span>            : #else
<span class="lineNum">    2120 </span>            : static inline void setup_nr_node_ids(void) {}
<span class="lineNum">    2121 </span>            : #endif
<span class="lineNum">    2122 </span>            : 
<span class="lineNum">    2123 </span>            : #endif /* __KERNEL__ */
<span class="lineNum">    2124 </span>            : #endif /* _LINUX_MM_H */
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.10</a></td></tr>
  </table>
  <br>

</body>
</html>

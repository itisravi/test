<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - btrfstest.info - include/linux/blkdev.h</title>
  <link rel="stylesheet" type="text/css" href="../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../index.html">top level</a> - <a href="index.html">include/linux</a> - blkdev.h<span style="font-size: 80%;"> (source / <a href="blkdev.h.func.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">btrfstest.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">4</td>
            <td class="headerCovTableEntry">4</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2014-11-28</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">0</td>
            <td class="headerCovTableEntry">0</td>
            <td class="headerCovTableEntryHi">-</td>
          </tr>
          <tr><td><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : #ifndef _LINUX_BLKDEV_H</a>
<span class="lineNum">       2 </span>            : #define _LINUX_BLKDEV_H
<span class="lineNum">       3 </span>            : 
<span class="lineNum">       4 </span>            : #include &lt;linux/sched.h&gt;
<span class="lineNum">       5 </span>            : 
<span class="lineNum">       6 </span>            : #ifdef CONFIG_BLOCK
<span class="lineNum">       7 </span>            : 
<span class="lineNum">       8 </span>            : #include &lt;linux/major.h&gt;
<span class="lineNum">       9 </span>            : #include &lt;linux/genhd.h&gt;
<span class="lineNum">      10 </span>            : #include &lt;linux/list.h&gt;
<span class="lineNum">      11 </span>            : #include &lt;linux/llist.h&gt;
<span class="lineNum">      12 </span>            : #include &lt;linux/timer.h&gt;
<span class="lineNum">      13 </span>            : #include &lt;linux/workqueue.h&gt;
<span class="lineNum">      14 </span>            : #include &lt;linux/pagemap.h&gt;
<span class="lineNum">      15 </span>            : #include &lt;linux/backing-dev.h&gt;
<span class="lineNum">      16 </span>            : #include &lt;linux/wait.h&gt;
<span class="lineNum">      17 </span>            : #include &lt;linux/mempool.h&gt;
<span class="lineNum">      18 </span>            : #include &lt;linux/bio.h&gt;
<span class="lineNum">      19 </span>            : #include &lt;linux/stringify.h&gt;
<span class="lineNum">      20 </span>            : #include &lt;linux/gfp.h&gt;
<span class="lineNum">      21 </span>            : #include &lt;linux/bsg.h&gt;
<span class="lineNum">      22 </span>            : #include &lt;linux/smp.h&gt;
<span class="lineNum">      23 </span>            : #include &lt;linux/rcupdate.h&gt;
<span class="lineNum">      24 </span>            : #include &lt;linux/percpu-refcount.h&gt;
<span class="lineNum">      25 </span>            : 
<span class="lineNum">      26 </span>            : #include &lt;asm/scatterlist.h&gt;
<span class="lineNum">      27 </span>            : 
<span class="lineNum">      28 </span>            : struct module;
<span class="lineNum">      29 </span>            : struct scsi_ioctl_command;
<span class="lineNum">      30 </span>            : 
<span class="lineNum">      31 </span>            : struct request_queue;
<span class="lineNum">      32 </span>            : struct elevator_queue;
<span class="lineNum">      33 </span>            : struct request_pm_state;
<span class="lineNum">      34 </span>            : struct blk_trace;
<span class="lineNum">      35 </span>            : struct request;
<span class="lineNum">      36 </span>            : struct sg_io_hdr;
<span class="lineNum">      37 </span>            : struct bsg_job;
<span class="lineNum">      38 </span>            : struct blkcg_gq;
<span class="lineNum">      39 </span>            : 
<span class="lineNum">      40 </span>            : #define BLKDEV_MIN_RQ   4
<span class="lineNum">      41 </span>            : #define BLKDEV_MAX_RQ   128     /* Default maximum */
<span class="lineNum">      42 </span>            : 
<span class="lineNum">      43 </span>            : /*
<span class="lineNum">      44 </span>            :  * Maximum number of blkcg policies allowed to be registered concurrently.
<span class="lineNum">      45 </span>            :  * Defined here to simplify include dependency.
<span class="lineNum">      46 </span>            :  */
<span class="lineNum">      47 </span>            : #define BLKCG_MAX_POLS          2
<span class="lineNum">      48 </span>            : 
<span class="lineNum">      49 </span>            : struct request;
<span class="lineNum">      50 </span>            : typedef void (rq_end_io_fn)(struct request *, int);
<span class="lineNum">      51 </span>            : 
<span class="lineNum">      52 </span>            : #define BLK_RL_SYNCFULL         (1U &lt;&lt; 0)
<span class="lineNum">      53 </span>            : #define BLK_RL_ASYNCFULL        (1U &lt;&lt; 1)
<span class="lineNum">      54 </span>            : 
<span class="lineNum">      55 </span>            : struct request_list {
<span class="lineNum">      56 </span>            :         struct request_queue    *q;     /* the queue this rl belongs to */
<span class="lineNum">      57 </span>            : #ifdef CONFIG_BLK_CGROUP
<span class="lineNum">      58 </span>            :         struct blkcg_gq         *blkg;  /* blkg this request pool belongs to */
<span class="lineNum">      59 </span>            : #endif
<span class="lineNum">      60 </span>            :         /*
<span class="lineNum">      61 </span>            :          * count[], starved[], and wait[] are indexed by
<span class="lineNum">      62 </span>            :          * BLK_RW_SYNC/BLK_RW_ASYNC
<span class="lineNum">      63 </span>            :          */
<span class="lineNum">      64 </span>            :         int                     count[2];
<span class="lineNum">      65 </span>            :         int                     starved[2];
<span class="lineNum">      66 </span>            :         mempool_t               *rq_pool;
<span class="lineNum">      67 </span>            :         wait_queue_head_t       wait[2];
<span class="lineNum">      68 </span>            :         unsigned int            flags;
<span class="lineNum">      69 </span>            : };
<span class="lineNum">      70 </span>            : 
<span class="lineNum">      71 </span>            : /*
<span class="lineNum">      72 </span>            :  * request command types
<span class="lineNum">      73 </span>            :  */
<span class="lineNum">      74 </span>            : enum rq_cmd_type_bits {
<span class="lineNum">      75 </span>            :         REQ_TYPE_FS             = 1,    /* fs request */
<span class="lineNum">      76 </span>            :         REQ_TYPE_BLOCK_PC,              /* scsi command */
<span class="lineNum">      77 </span>            :         REQ_TYPE_SENSE,                 /* sense request */
<span class="lineNum">      78 </span>            :         REQ_TYPE_PM_SUSPEND,            /* suspend request */
<span class="lineNum">      79 </span>            :         REQ_TYPE_PM_RESUME,             /* resume request */
<span class="lineNum">      80 </span>            :         REQ_TYPE_PM_SHUTDOWN,           /* shutdown request */
<span class="lineNum">      81 </span>            :         REQ_TYPE_SPECIAL,               /* driver defined type */
<span class="lineNum">      82 </span>            :         /*
<span class="lineNum">      83 </span>            :          * for ATA/ATAPI devices. this really doesn't belong here, ide should
<span class="lineNum">      84 </span>            :          * use REQ_TYPE_SPECIAL and use rq-&gt;cmd[0] with the range of driver
<span class="lineNum">      85 </span>            :          * private REQ_LB opcodes to differentiate what type of request this is
<span class="lineNum">      86 </span>            :          */
<span class="lineNum">      87 </span>            :         REQ_TYPE_ATA_TASKFILE,
<span class="lineNum">      88 </span>            :         REQ_TYPE_ATA_PC,
<span class="lineNum">      89 </span>            : };
<span class="lineNum">      90 </span>            : 
<span class="lineNum">      91 </span>            : #define BLK_MAX_CDB     16
<span class="lineNum">      92 </span>            : 
<span class="lineNum">      93 </span>            : /*
<span class="lineNum">      94 </span>            :  * Try to put the fields that are referenced together in the same cacheline.
<span class="lineNum">      95 </span>            :  *
<span class="lineNum">      96 </span>            :  * If you modify this structure, make sure to update blk_rq_init() and
<span class="lineNum">      97 </span>            :  * especially blk_mq_rq_ctx_init() to take care of the added fields.
<span class="lineNum">      98 </span>            :  */
<span class="lineNum">      99 </span>            : struct request {
<span class="lineNum">     100 </span>            :         struct list_head queuelist;
<span class="lineNum">     101 </span>            :         union {
<span class="lineNum">     102 </span>            :                 struct call_single_data csd;
<span class="lineNum">     103 </span>            :                 unsigned long fifo_time;
<span class="lineNum">     104 </span>            :         };
<span class="lineNum">     105 </span>            : 
<span class="lineNum">     106 </span>            :         struct request_queue *q;
<span class="lineNum">     107 </span>            :         struct blk_mq_ctx *mq_ctx;
<span class="lineNum">     108 </span>            : 
<span class="lineNum">     109 </span>            :         u64 cmd_flags;
<span class="lineNum">     110 </span>            :         enum rq_cmd_type_bits cmd_type;
<span class="lineNum">     111 </span>            :         unsigned long atomic_flags;
<span class="lineNum">     112 </span>            : 
<span class="lineNum">     113 </span>            :         int cpu;
<span class="lineNum">     114 </span>            : 
<span class="lineNum">     115 </span>            :         /* the following two fields are internal, NEVER access directly */
<span class="lineNum">     116 </span>            :         unsigned int __data_len;        /* total data len */
<span class="lineNum">     117 </span>            :         sector_t __sector;              /* sector cursor */
<span class="lineNum">     118 </span>            : 
<span class="lineNum">     119 </span>            :         struct bio *bio;
<span class="lineNum">     120 </span>            :         struct bio *biotail;
<span class="lineNum">     121 </span>            : 
<span class="lineNum">     122 </span>            :         /*
<span class="lineNum">     123 </span>            :          * The hash is used inside the scheduler, and killed once the
<span class="lineNum">     124 </span>            :          * request reaches the dispatch list. The ipi_list is only used
<span class="lineNum">     125 </span>            :          * to queue the request for softirq completion, which is long
<span class="lineNum">     126 </span>            :          * after the request has been unhashed (and even removed from
<span class="lineNum">     127 </span>            :          * the dispatch list).
<span class="lineNum">     128 </span>            :          */
<span class="lineNum">     129 </span>            :         union {
<span class="lineNum">     130 </span>            :                 struct hlist_node hash; /* merge hash */
<span class="lineNum">     131 </span>            :                 struct list_head ipi_list;
<span class="lineNum">     132 </span>            :         };
<span class="lineNum">     133 </span>            : 
<span class="lineNum">     134 </span>            :         /*
<span class="lineNum">     135 </span>            :          * The rb_node is only used inside the io scheduler, requests
<span class="lineNum">     136 </span>            :          * are pruned when moved to the dispatch queue. So let the
<span class="lineNum">     137 </span>            :          * completion_data share space with the rb_node.
<span class="lineNum">     138 </span>            :          */
<span class="lineNum">     139 </span>            :         union {
<span class="lineNum">     140 </span>            :                 struct rb_node rb_node; /* sort/lookup */
<span class="lineNum">     141 </span>            :                 void *completion_data;
<span class="lineNum">     142 </span>            :         };
<span class="lineNum">     143 </span>            : 
<span class="lineNum">     144 </span>            :         /*
<span class="lineNum">     145 </span>            :          * Three pointers are available for the IO schedulers, if they need
<span class="lineNum">     146 </span>            :          * more they have to dynamically allocate it.  Flush requests are
<span class="lineNum">     147 </span>            :          * never put on the IO scheduler. So let the flush fields share
<span class="lineNum">     148 </span>            :          * space with the elevator data.
<span class="lineNum">     149 </span>            :          */
<span class="lineNum">     150 </span>            :         union {
<span class="lineNum">     151 </span>            :                 struct {
<span class="lineNum">     152 </span>            :                         struct io_cq            *icq;
<span class="lineNum">     153 </span>            :                         void                    *priv[2];
<span class="lineNum">     154 </span>            :                 } elv;
<span class="lineNum">     155 </span>            : 
<span class="lineNum">     156 </span>            :                 struct {
<span class="lineNum">     157 </span>            :                         unsigned int            seq;
<span class="lineNum">     158 </span>            :                         struct list_head        list;
<span class="lineNum">     159 </span>            :                         rq_end_io_fn            *saved_end_io;
<span class="lineNum">     160 </span>            :                 } flush;
<span class="lineNum">     161 </span>            :         };
<span class="lineNum">     162 </span>            : 
<span class="lineNum">     163 </span>            :         struct gendisk *rq_disk;
<span class="lineNum">     164 </span>            :         struct hd_struct *part;
<span class="lineNum">     165 </span>            :         unsigned long start_time;
<span class="lineNum">     166 </span>            : #ifdef CONFIG_BLK_CGROUP
<span class="lineNum">     167 </span>            :         struct request_list *rl;                /* rl this rq is alloced from */
<span class="lineNum">     168 </span>            :         unsigned long long start_time_ns;
<span class="lineNum">     169 </span>            :         unsigned long long io_start_time_ns;    /* when passed to hardware */
<span class="lineNum">     170 </span>            : #endif
<span class="lineNum">     171 </span>            :         /* Number of scatter-gather DMA addr+len pairs after
<span class="lineNum">     172 </span>            :          * physical address coalescing is performed.
<span class="lineNum">     173 </span>            :          */
<span class="lineNum">     174 </span>            :         unsigned short nr_phys_segments;
<span class="lineNum">     175 </span>            : #if defined(CONFIG_BLK_DEV_INTEGRITY)
<span class="lineNum">     176 </span>            :         unsigned short nr_integrity_segments;
<span class="lineNum">     177 </span>            : #endif
<span class="lineNum">     178 </span>            : 
<span class="lineNum">     179 </span>            :         unsigned short ioprio;
<span class="lineNum">     180 </span>            : 
<span class="lineNum">     181 </span>            :         void *special;          /* opaque pointer available for LLD use */
<span class="lineNum">     182 </span>            : 
<span class="lineNum">     183 </span>            :         int tag;
<span class="lineNum">     184 </span>            :         int errors;
<span class="lineNum">     185 </span>            : 
<span class="lineNum">     186 </span>            :         /*
<span class="lineNum">     187 </span>            :          * when request is used as a packet command carrier
<span class="lineNum">     188 </span>            :          */
<span class="lineNum">     189 </span>            :         unsigned char __cmd[BLK_MAX_CDB];
<span class="lineNum">     190 </span>            :         unsigned char *cmd;
<span class="lineNum">     191 </span>            :         unsigned short cmd_len;
<span class="lineNum">     192 </span>            : 
<span class="lineNum">     193 </span>            :         unsigned int extra_len; /* length of alignment and padding */
<span class="lineNum">     194 </span>            :         unsigned int sense_len;
<span class="lineNum">     195 </span>            :         unsigned int resid_len; /* residual count */
<span class="lineNum">     196 </span>            :         void *sense;
<span class="lineNum">     197 </span>            : 
<span class="lineNum">     198 </span>            :         unsigned long deadline;
<span class="lineNum">     199 </span>            :         struct list_head timeout_list;
<span class="lineNum">     200 </span>            :         unsigned int timeout;
<span class="lineNum">     201 </span>            :         int retries;
<span class="lineNum">     202 </span>            : 
<span class="lineNum">     203 </span>            :         /*
<span class="lineNum">     204 </span>            :          * completion callback.
<span class="lineNum">     205 </span>            :          */
<span class="lineNum">     206 </span>            :         rq_end_io_fn *end_io;
<span class="lineNum">     207 </span>            :         void *end_io_data;
<span class="lineNum">     208 </span>            : 
<span class="lineNum">     209 </span>            :         /* for bidi */
<span class="lineNum">     210 </span>            :         struct request *next_rq;
<span class="lineNum">     211 </span>            : };
<span class="lineNum">     212 </span>            : 
<span class="lineNum">     213 </span>            : static inline unsigned short req_get_ioprio(struct request *req)
<span class="lineNum">     214 </span>            : {
<span class="lineNum">     215 </span>            :         return req-&gt;ioprio;
<span class="lineNum">     216 </span>            : }
<span class="lineNum">     217 </span>            : 
<span class="lineNum">     218 </span>            : /*
<span class="lineNum">     219 </span>            :  * State information carried for REQ_TYPE_PM_SUSPEND and REQ_TYPE_PM_RESUME
<span class="lineNum">     220 </span>            :  * requests. Some step values could eventually be made generic.
<span class="lineNum">     221 </span>            :  */
<span class="lineNum">     222 </span>            : struct request_pm_state
<span class="lineNum">     223 </span>            : {
<span class="lineNum">     224 </span>            :         /* PM state machine step value, currently driver specific */
<span class="lineNum">     225 </span>            :         int     pm_step;
<span class="lineNum">     226 </span>            :         /* requested PM state value (S1, S2, S3, S4, ...) */
<span class="lineNum">     227 </span>            :         u32     pm_state;
<span class="lineNum">     228 </span>            :         void*   data;           /* for driver use */
<span class="lineNum">     229 </span>            : };
<span class="lineNum">     230 </span>            : 
<span class="lineNum">     231 </span>            : #include &lt;linux/elevator.h&gt;
<span class="lineNum">     232 </span>            : 
<span class="lineNum">     233 </span>            : struct blk_queue_ctx;
<span class="lineNum">     234 </span>            : 
<span class="lineNum">     235 </span>            : typedef void (request_fn_proc) (struct request_queue *q);
<span class="lineNum">     236 </span>            : typedef void (make_request_fn) (struct request_queue *q, struct bio *bio);
<span class="lineNum">     237 </span>            : typedef int (prep_rq_fn) (struct request_queue *, struct request *);
<span class="lineNum">     238 </span>            : typedef void (unprep_rq_fn) (struct request_queue *, struct request *);
<span class="lineNum">     239 </span>            : 
<span class="lineNum">     240 </span>            : struct bio_vec;
<span class="lineNum">     241 </span>            : struct bvec_merge_data {
<span class="lineNum">     242 </span>            :         struct block_device *bi_bdev;
<span class="lineNum">     243 </span>            :         sector_t bi_sector;
<span class="lineNum">     244 </span>            :         unsigned bi_size;
<span class="lineNum">     245 </span>            :         unsigned long bi_rw;
<span class="lineNum">     246 </span>            : };
<span class="lineNum">     247 </span>            : typedef int (merge_bvec_fn) (struct request_queue *, struct bvec_merge_data *,
<span class="lineNum">     248 </span>            :                              struct bio_vec *);
<span class="lineNum">     249 </span>            : typedef void (softirq_done_fn)(struct request *);
<span class="lineNum">     250 </span>            : typedef int (dma_drain_needed_fn)(struct request *);
<span class="lineNum">     251 </span>            : typedef int (lld_busy_fn) (struct request_queue *q);
<span class="lineNum">     252 </span>            : typedef int (bsg_job_fn) (struct bsg_job *);
<span class="lineNum">     253 </span>            : 
<span class="lineNum">     254 </span>            : enum blk_eh_timer_return {
<span class="lineNum">     255 </span>            :         BLK_EH_NOT_HANDLED,
<span class="lineNum">     256 </span>            :         BLK_EH_HANDLED,
<span class="lineNum">     257 </span>            :         BLK_EH_RESET_TIMER,
<span class="lineNum">     258 </span>            : };
<span class="lineNum">     259 </span>            : 
<span class="lineNum">     260 </span>            : typedef enum blk_eh_timer_return (rq_timed_out_fn)(struct request *);
<span class="lineNum">     261 </span>            : 
<span class="lineNum">     262 </span>            : enum blk_queue_state {
<span class="lineNum">     263 </span>            :         Queue_down,
<span class="lineNum">     264 </span>            :         Queue_up,
<span class="lineNum">     265 </span>            : };
<span class="lineNum">     266 </span>            : 
<span class="lineNum">     267 </span>            : struct blk_queue_tag {
<span class="lineNum">     268 </span>            :         struct request **tag_index;     /* map of busy tags */
<span class="lineNum">     269 </span>            :         unsigned long *tag_map;         /* bit map of free/busy tags */
<span class="lineNum">     270 </span>            :         int busy;                       /* current depth */
<span class="lineNum">     271 </span>            :         int max_depth;                  /* what we will send to device */
<span class="lineNum">     272 </span>            :         int real_max_depth;             /* what the array can hold */
<span class="lineNum">     273 </span>            :         atomic_t refcnt;                /* map can be shared */
<span class="lineNum">     274 </span>            : };
<span class="lineNum">     275 </span>            : 
<span class="lineNum">     276 </span>            : #define BLK_SCSI_MAX_CMDS       (256)
<span class="lineNum">     277 </span>            : #define BLK_SCSI_CMD_PER_LONG   (BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))
<span class="lineNum">     278 </span>            : 
<span class="lineNum">     279 </span>            : struct queue_limits {
<span class="lineNum">     280 </span>            :         unsigned long           bounce_pfn;
<span class="lineNum">     281 </span>            :         unsigned long           seg_boundary_mask;
<span class="lineNum">     282 </span>            : 
<span class="lineNum">     283 </span>            :         unsigned int            max_hw_sectors;
<span class="lineNum">     284 </span>            :         unsigned int            chunk_sectors;
<span class="lineNum">     285 </span>            :         unsigned int            max_sectors;
<span class="lineNum">     286 </span>            :         unsigned int            max_segment_size;
<span class="lineNum">     287 </span>            :         unsigned int            physical_block_size;
<span class="lineNum">     288 </span>            :         unsigned int            alignment_offset;
<span class="lineNum">     289 </span>            :         unsigned int            io_min;
<span class="lineNum">     290 </span>            :         unsigned int            io_opt;
<span class="lineNum">     291 </span>            :         unsigned int            max_discard_sectors;
<span class="lineNum">     292 </span>            :         unsigned int            max_write_same_sectors;
<span class="lineNum">     293 </span>            :         unsigned int            discard_granularity;
<span class="lineNum">     294 </span>            :         unsigned int            discard_alignment;
<span class="lineNum">     295 </span>            : 
<span class="lineNum">     296 </span>            :         unsigned short          logical_block_size;
<span class="lineNum">     297 </span>            :         unsigned short          max_segments;
<span class="lineNum">     298 </span>            :         unsigned short          max_integrity_segments;
<span class="lineNum">     299 </span>            : 
<span class="lineNum">     300 </span>            :         unsigned char           misaligned;
<span class="lineNum">     301 </span>            :         unsigned char           discard_misaligned;
<span class="lineNum">     302 </span>            :         unsigned char           cluster;
<span class="lineNum">     303 </span>            :         unsigned char           discard_zeroes_data;
<span class="lineNum">     304 </span>            :         unsigned char           raid_partial_stripes_expensive;
<span class="lineNum">     305 </span>            : };
<span class="lineNum">     306 </span>            : 
<span class="lineNum">     307 </span>            : struct request_queue {
<span class="lineNum">     308 </span>            :         /*
<span class="lineNum">     309 </span>            :          * Together with queue_head for cacheline sharing
<span class="lineNum">     310 </span>            :          */
<span class="lineNum">     311 </span>            :         struct list_head        queue_head;
<span class="lineNum">     312 </span>            :         struct request          *last_merge;
<span class="lineNum">     313 </span>            :         struct elevator_queue   *elevator;
<span class="lineNum">     314 </span>            :         int                     nr_rqs[2];      /* # allocated [a]sync rqs */
<span class="lineNum">     315 </span>            :         int                     nr_rqs_elvpriv; /* # allocated rqs w/ elvpriv */
<span class="lineNum">     316 </span>            : 
<span class="lineNum">     317 </span>            :         /*
<span class="lineNum">     318 </span>            :          * If blkcg is not used, @q-&gt;root_rl serves all requests.  If blkcg
<span class="lineNum">     319 </span>            :          * is used, root blkg allocates from @q-&gt;root_rl and all other
<span class="lineNum">     320 </span>            :          * blkgs from their own blkg-&gt;rl.  Which one to use should be
<span class="lineNum">     321 </span>            :          * determined using bio_request_list().
<span class="lineNum">     322 </span>            :          */
<span class="lineNum">     323 </span>            :         struct request_list     root_rl;
<span class="lineNum">     324 </span>            : 
<span class="lineNum">     325 </span>            :         request_fn_proc         *request_fn;
<span class="lineNum">     326 </span>            :         make_request_fn         *make_request_fn;
<span class="lineNum">     327 </span>            :         prep_rq_fn              *prep_rq_fn;
<span class="lineNum">     328 </span>            :         unprep_rq_fn            *unprep_rq_fn;
<span class="lineNum">     329 </span>            :         merge_bvec_fn           *merge_bvec_fn;
<span class="lineNum">     330 </span>            :         softirq_done_fn         *softirq_done_fn;
<span class="lineNum">     331 </span>            :         rq_timed_out_fn         *rq_timed_out_fn;
<span class="lineNum">     332 </span>            :         dma_drain_needed_fn     *dma_drain_needed;
<span class="lineNum">     333 </span>            :         lld_busy_fn             *lld_busy_fn;
<span class="lineNum">     334 </span>            : 
<span class="lineNum">     335 </span>            :         struct blk_mq_ops       *mq_ops;
<span class="lineNum">     336 </span>            : 
<span class="lineNum">     337 </span>            :         unsigned int            *mq_map;
<span class="lineNum">     338 </span>            : 
<span class="lineNum">     339 </span>            :         /* sw queues */
<span class="lineNum">     340 </span>            :         struct blk_mq_ctx __percpu      *queue_ctx;
<span class="lineNum">     341 </span>            :         unsigned int            nr_queues;
<span class="lineNum">     342 </span>            : 
<span class="lineNum">     343 </span>            :         /* hw dispatch queues */
<span class="lineNum">     344 </span>            :         struct blk_mq_hw_ctx    **queue_hw_ctx;
<span class="lineNum">     345 </span>            :         unsigned int            nr_hw_queues;
<span class="lineNum">     346 </span>            : 
<span class="lineNum">     347 </span>            :         /*
<span class="lineNum">     348 </span>            :          * Dispatch queue sorting
<span class="lineNum">     349 </span>            :          */
<span class="lineNum">     350 </span>            :         sector_t                end_sector;
<span class="lineNum">     351 </span>            :         struct request          *boundary_rq;
<span class="lineNum">     352 </span>            : 
<span class="lineNum">     353 </span>            :         /*
<span class="lineNum">     354 </span>            :          * Delayed queue handling
<span class="lineNum">     355 </span>            :          */
<span class="lineNum">     356 </span>            :         struct delayed_work     delay_work;
<span class="lineNum">     357 </span>            : 
<span class="lineNum">     358 </span>            :         struct backing_dev_info backing_dev_info;
<span class="lineNum">     359 </span>            : 
<span class="lineNum">     360 </span>            :         /*
<span class="lineNum">     361 </span>            :          * The queue owner gets to use this for whatever they like.
<span class="lineNum">     362 </span>            :          * ll_rw_blk doesn't touch it.
<span class="lineNum">     363 </span>            :          */
<span class="lineNum">     364 </span>            :         void                    *queuedata;
<span class="lineNum">     365 </span>            : 
<span class="lineNum">     366 </span>            :         /*
<span class="lineNum">     367 </span>            :          * various queue flags, see QUEUE_* below
<span class="lineNum">     368 </span>            :          */
<span class="lineNum">     369 </span>            :         unsigned long           queue_flags;
<span class="lineNum">     370 </span>            : 
<span class="lineNum">     371 </span>            :         /*
<span class="lineNum">     372 </span>            :          * ida allocated id for this queue.  Used to index queues from
<span class="lineNum">     373 </span>            :          * ioctx.
<span class="lineNum">     374 </span>            :          */
<span class="lineNum">     375 </span>            :         int                     id;
<span class="lineNum">     376 </span>            : 
<span class="lineNum">     377 </span>            :         /*
<span class="lineNum">     378 </span>            :          * queue needs bounce pages for pages above this limit
<span class="lineNum">     379 </span>            :          */
<span class="lineNum">     380 </span>            :         gfp_t                   bounce_gfp;
<span class="lineNum">     381 </span>            : 
<span class="lineNum">     382 </span>            :         /*
<span class="lineNum">     383 </span>            :          * protects queue structures from reentrancy. -&gt;__queue_lock should
<span class="lineNum">     384 </span>            :          * _never_ be used directly, it is queue private. always use
<span class="lineNum">     385 </span>            :          * -&gt;queue_lock.
<span class="lineNum">     386 </span>            :          */
<span class="lineNum">     387 </span>            :         spinlock_t              __queue_lock;
<span class="lineNum">     388 </span>            :         spinlock_t              *queue_lock;
<span class="lineNum">     389 </span>            : 
<span class="lineNum">     390 </span>            :         /*
<span class="lineNum">     391 </span>            :          * queue kobject
<span class="lineNum">     392 </span>            :          */
<span class="lineNum">     393 </span>            :         struct kobject kobj;
<span class="lineNum">     394 </span>            : 
<span class="lineNum">     395 </span>            :         /*
<span class="lineNum">     396 </span>            :          * mq queue kobject
<span class="lineNum">     397 </span>            :          */
<span class="lineNum">     398 </span>            :         struct kobject mq_kobj;
<span class="lineNum">     399 </span>            : 
<span class="lineNum">     400 </span>            : #ifdef CONFIG_PM_RUNTIME
<span class="lineNum">     401 </span>            :         struct device           *dev;
<span class="lineNum">     402 </span>            :         int                     rpm_status;
<span class="lineNum">     403 </span>            :         unsigned int            nr_pending;
<span class="lineNum">     404 </span>            : #endif
<span class="lineNum">     405 </span>            : 
<span class="lineNum">     406 </span>            :         /*
<span class="lineNum">     407 </span>            :          * queue settings
<span class="lineNum">     408 </span>            :          */
<span class="lineNum">     409 </span>            :         unsigned long           nr_requests;    /* Max # of requests */
<span class="lineNum">     410 </span>            :         unsigned int            nr_congestion_on;
<span class="lineNum">     411 </span>            :         unsigned int            nr_congestion_off;
<span class="lineNum">     412 </span>            :         unsigned int            nr_batching;
<span class="lineNum">     413 </span>            : 
<span class="lineNum">     414 </span>            :         unsigned int            dma_drain_size;
<span class="lineNum">     415 </span>            :         void                    *dma_drain_buffer;
<span class="lineNum">     416 </span>            :         unsigned int            dma_pad_mask;
<span class="lineNum">     417 </span>            :         unsigned int            dma_alignment;
<span class="lineNum">     418 </span>            : 
<span class="lineNum">     419 </span>            :         struct blk_queue_tag    *queue_tags;
<span class="lineNum">     420 </span>            :         struct list_head        tag_busy_list;
<span class="lineNum">     421 </span>            : 
<span class="lineNum">     422 </span>            :         unsigned int            nr_sorted;
<span class="lineNum">     423 </span>            :         unsigned int            in_flight[2];
<span class="lineNum">     424 </span>            :         /*
<span class="lineNum">     425 </span>            :          * Number of active block driver functions for which blk_drain_queue()
<span class="lineNum">     426 </span>            :          * must wait. Must be incremented around functions that unlock the
<span class="lineNum">     427 </span>            :          * queue_lock internally, e.g. scsi_request_fn().
<span class="lineNum">     428 </span>            :          */
<span class="lineNum">     429 </span>            :         unsigned int            request_fn_active;
<span class="lineNum">     430 </span>            : 
<span class="lineNum">     431 </span>            :         unsigned int            rq_timeout;
<span class="lineNum">     432 </span>            :         struct timer_list       timeout;
<span class="lineNum">     433 </span>            :         struct list_head        timeout_list;
<span class="lineNum">     434 </span>            : 
<span class="lineNum">     435 </span>            :         struct list_head        icq_list;
<span class="lineNum">     436 </span>            : #ifdef CONFIG_BLK_CGROUP
<span class="lineNum">     437 </span>            :         DECLARE_BITMAP          (blkcg_pols, BLKCG_MAX_POLS);
<span class="lineNum">     438 </span>            :         struct blkcg_gq         *root_blkg;
<span class="lineNum">     439 </span>            :         struct list_head        blkg_list;
<span class="lineNum">     440 </span>            : #endif
<span class="lineNum">     441 </span>            : 
<span class="lineNum">     442 </span>            :         struct queue_limits     limits;
<span class="lineNum">     443 </span>            : 
<span class="lineNum">     444 </span>            :         /*
<span class="lineNum">     445 </span>            :          * sg stuff
<span class="lineNum">     446 </span>            :          */
<span class="lineNum">     447 </span>            :         unsigned int            sg_timeout;
<span class="lineNum">     448 </span>            :         unsigned int            sg_reserved_size;
<span class="lineNum">     449 </span>            :         int                     node;
<span class="lineNum">     450 </span>            : #ifdef CONFIG_BLK_DEV_IO_TRACE
<span class="lineNum">     451 </span>            :         struct blk_trace        *blk_trace;
<span class="lineNum">     452 </span>            : #endif
<span class="lineNum">     453 </span>            :         /*
<span class="lineNum">     454 </span>            :          * for flush operations
<span class="lineNum">     455 </span>            :          */
<span class="lineNum">     456 </span>            :         unsigned int            flush_flags;
<span class="lineNum">     457 </span>            :         unsigned int            flush_not_queueable:1;
<span class="lineNum">     458 </span>            :         unsigned int            flush_queue_delayed:1;
<span class="lineNum">     459 </span>            :         unsigned int            flush_pending_idx:1;
<span class="lineNum">     460 </span>            :         unsigned int            flush_running_idx:1;
<span class="lineNum">     461 </span>            :         unsigned long           flush_pending_since;
<span class="lineNum">     462 </span>            :         struct list_head        flush_queue[2];
<span class="lineNum">     463 </span>            :         struct list_head        flush_data_in_flight;
<span class="lineNum">     464 </span>            :         struct request          *flush_rq;
<span class="lineNum">     465 </span>            :         spinlock_t              mq_flush_lock;
<span class="lineNum">     466 </span>            : 
<span class="lineNum">     467 </span>            :         struct list_head        requeue_list;
<span class="lineNum">     468 </span>            :         spinlock_t              requeue_lock;
<span class="lineNum">     469 </span>            :         struct work_struct      requeue_work;
<span class="lineNum">     470 </span>            : 
<span class="lineNum">     471 </span>            :         struct mutex            sysfs_lock;
<span class="lineNum">     472 </span>            : 
<span class="lineNum">     473 </span>            :         int                     bypass_depth;
<span class="lineNum">     474 </span>            :         int                     mq_freeze_depth;
<span class="lineNum">     475 </span>            : 
<span class="lineNum">     476 </span>            : #if defined(CONFIG_BLK_DEV_BSG)
<span class="lineNum">     477 </span>            :         bsg_job_fn              *bsg_job_fn;
<span class="lineNum">     478 </span>            :         int                     bsg_job_size;
<span class="lineNum">     479 </span>            :         struct bsg_class_device bsg_dev;
<span class="lineNum">     480 </span>            : #endif
<span class="lineNum">     481 </span>            : 
<span class="lineNum">     482 </span>            : #ifdef CONFIG_BLK_DEV_THROTTLING
<span class="lineNum">     483 </span>            :         /* Throttle data */
<span class="lineNum">     484 </span>            :         struct throtl_data *td;
<span class="lineNum">     485 </span>            : #endif
<span class="lineNum">     486 </span>            :         struct rcu_head         rcu_head;
<span class="lineNum">     487 </span>            :         wait_queue_head_t       mq_freeze_wq;
<span class="lineNum">     488 </span>            :         struct percpu_ref       mq_usage_counter;
<span class="lineNum">     489 </span>            :         struct list_head        all_q_node;
<span class="lineNum">     490 </span>            : 
<span class="lineNum">     491 </span>            :         struct blk_mq_tag_set   *tag_set;
<span class="lineNum">     492 </span>            :         struct list_head        tag_set_list;
<span class="lineNum">     493 </span>            : };
<span class="lineNum">     494 </span>            : 
<span class="lineNum">     495 </span>            : #define QUEUE_FLAG_QUEUED       1       /* uses generic tag queueing */
<span class="lineNum">     496 </span>            : #define QUEUE_FLAG_STOPPED      2       /* queue is stopped */
<span class="lineNum">     497 </span>            : #define QUEUE_FLAG_SYNCFULL     3       /* read queue has been filled */
<span class="lineNum">     498 </span>            : #define QUEUE_FLAG_ASYNCFULL    4       /* write queue has been filled */
<span class="lineNum">     499 </span>            : #define QUEUE_FLAG_DYING        5       /* queue being torn down */
<span class="lineNum">     500 </span>            : #define QUEUE_FLAG_BYPASS       6       /* act as dumb FIFO queue */
<span class="lineNum">     501 </span>            : #define QUEUE_FLAG_BIDI         7       /* queue supports bidi requests */
<span class="lineNum">     502 </span>            : #define QUEUE_FLAG_NOMERGES     8       /* disable merge attempts */
<span class="lineNum">     503 </span>            : #define QUEUE_FLAG_SAME_COMP    9       /* complete on same CPU-group */
<span class="lineNum">     504 </span>            : #define QUEUE_FLAG_FAIL_IO     10       /* fake timeout */
<span class="lineNum">     505 </span>            : #define QUEUE_FLAG_STACKABLE   11       /* supports request stacking */
<span class="lineNum">     506 </span>            : #define QUEUE_FLAG_NONROT      12       /* non-rotational device (SSD) */
<span class="lineNum">     507 </span>            : #define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */
<span class="lineNum">     508 </span>            : #define QUEUE_FLAG_IO_STAT     13       /* do IO stats */
<span class="lineNum">     509 </span>            : #define QUEUE_FLAG_DISCARD     14       /* supports DISCARD */
<span class="lineNum">     510 </span>            : #define QUEUE_FLAG_NOXMERGES   15       /* No extended merges */
<span class="lineNum">     511 </span>            : #define QUEUE_FLAG_ADD_RANDOM  16       /* Contributes to random pool */
<span class="lineNum">     512 </span>            : #define QUEUE_FLAG_SECDISCARD  17       /* supports SECDISCARD */
<span class="lineNum">     513 </span>            : #define QUEUE_FLAG_SAME_FORCE  18       /* force complete on same CPU */
<span class="lineNum">     514 </span>            : #define QUEUE_FLAG_DEAD        19       /* queue tear-down finished */
<span class="lineNum">     515 </span>            : #define QUEUE_FLAG_INIT_DONE   20       /* queue is initialized */
<span class="lineNum">     516 </span>            : #define QUEUE_FLAG_NO_SG_MERGE 21       /* don't attempt to merge SG segments*/
<span class="lineNum">     517 </span>            : #define QUEUE_FLAG_SG_GAPS     22       /* queue doesn't support SG gaps */
<span class="lineNum">     518 </span>            : 
<span class="lineNum">     519 </span>            : #define QUEUE_FLAG_DEFAULT      ((1 &lt;&lt; QUEUE_FLAG_IO_STAT) |              \
<span class="lineNum">     520 </span>            :                                  (1 &lt;&lt; QUEUE_FLAG_STACKABLE)      |       \
<span class="lineNum">     521 </span>            :                                  (1 &lt;&lt; QUEUE_FLAG_SAME_COMP)      |       \
<span class="lineNum">     522 </span>            :                                  (1 &lt;&lt; QUEUE_FLAG_ADD_RANDOM))
<span class="lineNum">     523 </span>            : 
<span class="lineNum">     524 </span>            : #define QUEUE_FLAG_MQ_DEFAULT   ((1 &lt;&lt; QUEUE_FLAG_IO_STAT) |              \
<span class="lineNum">     525 </span>            :                                  (1 &lt;&lt; QUEUE_FLAG_SAME_COMP))
<span class="lineNum">     526 </span>            : 
<span class="lineNum">     527 </span>            : static inline void queue_lockdep_assert_held(struct request_queue *q)
<span class="lineNum">     528 </span>            : {
<span class="lineNum">     529 </span>            :         if (q-&gt;queue_lock)
<span class="lineNum">     530 </span>            :                 lockdep_assert_held(q-&gt;queue_lock);
<span class="lineNum">     531 </span>            : }
<span class="lineNum">     532 </span>            : 
<span class="lineNum">     533 </span>            : static inline void queue_flag_set_unlocked(unsigned int flag,
<span class="lineNum">     534 </span>            :                                            struct request_queue *q)
<span class="lineNum">     535 </span>            : {
<span class="lineNum">     536 </span>            :         __set_bit(flag, &amp;q-&gt;queue_flags);
<span class="lineNum">     537 </span>            : }
<span class="lineNum">     538 </span>            : 
<span class="lineNum">     539 </span>            : static inline int queue_flag_test_and_clear(unsigned int flag,
<span class="lineNum">     540 </span>            :                                             struct request_queue *q)
<span class="lineNum">     541 </span>            : {
<span class="lineNum">     542 </span>            :         queue_lockdep_assert_held(q);
<span class="lineNum">     543 </span>            : 
<span class="lineNum">     544 </span>            :         if (test_bit(flag, &amp;q-&gt;queue_flags)) {
<span class="lineNum">     545 </span>            :                 __clear_bit(flag, &amp;q-&gt;queue_flags);
<span class="lineNum">     546 </span>            :                 return 1;
<span class="lineNum">     547 </span>            :         }
<span class="lineNum">     548 </span>            : 
<span class="lineNum">     549 </span>            :         return 0;
<span class="lineNum">     550 </span>            : }
<span class="lineNum">     551 </span>            : 
<span class="lineNum">     552 </span>            : static inline int queue_flag_test_and_set(unsigned int flag,
<span class="lineNum">     553 </span>            :                                           struct request_queue *q)
<span class="lineNum">     554 </span>            : {
<span class="lineNum">     555 </span>            :         queue_lockdep_assert_held(q);
<span class="lineNum">     556 </span>            : 
<span class="lineNum">     557 </span>            :         if (!test_bit(flag, &amp;q-&gt;queue_flags)) {
<span class="lineNum">     558 </span>            :                 __set_bit(flag, &amp;q-&gt;queue_flags);
<span class="lineNum">     559 </span>            :                 return 0;
<span class="lineNum">     560 </span>            :         }
<span class="lineNum">     561 </span>            : 
<span class="lineNum">     562 </span>            :         return 1;
<span class="lineNum">     563 </span>            : }
<span class="lineNum">     564 </span>            : 
<span class="lineNum">     565 </span>            : static inline void queue_flag_set(unsigned int flag, struct request_queue *q)
<span class="lineNum">     566 </span>            : {
<span class="lineNum">     567 </span>            :         queue_lockdep_assert_held(q);
<span class="lineNum">     568 </span>            :         __set_bit(flag, &amp;q-&gt;queue_flags);
<span class="lineNum">     569 </span>            : }
<span class="lineNum">     570 </span>            : 
<span class="lineNum">     571 </span>            : static inline void queue_flag_clear_unlocked(unsigned int flag,
<span class="lineNum">     572 </span>            :                                              struct request_queue *q)
<span class="lineNum">     573 </span>            : {
<span class="lineNum">     574 </span>            :         __clear_bit(flag, &amp;q-&gt;queue_flags);
<span class="lineNum">     575 </span>            : }
<span class="lineNum">     576 </span>            : 
<span class="lineNum">     577 </span>            : static inline int queue_in_flight(struct request_queue *q)
<span class="lineNum">     578 </span>            : {
<span class="lineNum">     579 </span>            :         return q-&gt;in_flight[0] + q-&gt;in_flight[1];
<span class="lineNum">     580 </span>            : }
<span class="lineNum">     581 </span>            : 
<span class="lineNum">     582 </span>            : static inline void queue_flag_clear(unsigned int flag, struct request_queue *q)
<span class="lineNum">     583 </span>            : {
<span class="lineNum">     584 </span>            :         queue_lockdep_assert_held(q);
<span class="lineNum">     585 </span>            :         __clear_bit(flag, &amp;q-&gt;queue_flags);
<span class="lineNum">     586 </span>            : }
<span class="lineNum">     587 </span>            : 
<span class="lineNum">     588 </span>            : #define blk_queue_tagged(q)     test_bit(QUEUE_FLAG_QUEUED, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     589 </span>            : #define blk_queue_stopped(q)    test_bit(QUEUE_FLAG_STOPPED, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     590 </span>            : #define blk_queue_dying(q)      test_bit(QUEUE_FLAG_DYING, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     591 </span>            : #define blk_queue_dead(q)       test_bit(QUEUE_FLAG_DEAD, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     592 </span>            : #define blk_queue_bypass(q)     test_bit(QUEUE_FLAG_BYPASS, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     593 </span>            : #define blk_queue_init_done(q)  test_bit(QUEUE_FLAG_INIT_DONE, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     594 </span>            : #define blk_queue_nomerges(q)   test_bit(QUEUE_FLAG_NOMERGES, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     595 </span>            : #define blk_queue_noxmerges(q)  \
<span class="lineNum">     596 </span>            :         test_bit(QUEUE_FLAG_NOXMERGES, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     597 </span>            : #define blk_queue_nonrot(q)     test_bit(QUEUE_FLAG_NONROT, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     598 </span>            : #define blk_queue_io_stat(q)    test_bit(QUEUE_FLAG_IO_STAT, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     599 </span>            : #define blk_queue_add_random(q) test_bit(QUEUE_FLAG_ADD_RANDOM, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     600 </span>            : #define blk_queue_stackable(q)  \
<span class="lineNum">     601 </span>            :         test_bit(QUEUE_FLAG_STACKABLE, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     602 </span>            : #define blk_queue_discard(q)    test_bit(QUEUE_FLAG_DISCARD, &amp;(q)-&gt;queue_flags)
<span class="lineNum">     603 </span>            : #define blk_queue_secdiscard(q) (blk_queue_discard(q) &amp;&amp; \
<span class="lineNum">     604 </span>            :         test_bit(QUEUE_FLAG_SECDISCARD, &amp;(q)-&gt;queue_flags))
<span class="lineNum">     605 </span>            : 
<span class="lineNum">     606 </span>            : #define blk_noretry_request(rq) \
<span class="lineNum">     607 </span>            :         ((rq)-&gt;cmd_flags &amp; (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
<span class="lineNum">     608 </span>            :                              REQ_FAILFAST_DRIVER))
<span class="lineNum">     609 </span>            : 
<span class="lineNum">     610 </span>            : #define blk_account_rq(rq) \
<span class="lineNum">     611 </span>            :         (((rq)-&gt;cmd_flags &amp; REQ_STARTED) &amp;&amp; \
<span class="lineNum">     612 </span>            :          ((rq)-&gt;cmd_type == REQ_TYPE_FS))
<span class="lineNum">     613 </span>            : 
<span class="lineNum">     614 </span>            : #define blk_pm_request(rq)      \
<span class="lineNum">     615 </span>            :         ((rq)-&gt;cmd_type == REQ_TYPE_PM_SUSPEND || \
<span class="lineNum">     616 </span>            :          (rq)-&gt;cmd_type == REQ_TYPE_PM_RESUME)
<span class="lineNum">     617 </span>            : 
<span class="lineNum">     618 </span>            : #define blk_rq_cpu_valid(rq)    ((rq)-&gt;cpu != -1)
<span class="lineNum">     619 </span>            : #define blk_bidi_rq(rq)         ((rq)-&gt;next_rq != NULL)
<span class="lineNum">     620 </span>            : /* rq-&gt;queuelist of dequeued request must be list_empty() */
<span class="lineNum">     621 </span>            : #define blk_queued_rq(rq)       (!list_empty(&amp;(rq)-&gt;queuelist))
<span class="lineNum">     622 </span>            : 
<span class="lineNum">     623 </span>            : #define list_entry_rq(ptr)      list_entry((ptr), struct request, queuelist)
<span class="lineNum">     624 </span>            : 
<span class="lineNum">     625 </span>            : #define rq_data_dir(rq)         (((rq)-&gt;cmd_flags &amp; 1) != 0)
<span class="lineNum">     626 </span>            : 
<span class="lineNum">     627 </span>            : /*
<span class="lineNum">     628 </span>            :  * Driver can handle struct request, if it either has an old style
<span class="lineNum">     629 </span>            :  * request_fn defined, or is blk-mq based.
<span class="lineNum">     630 </span>            :  */
<span class="lineNum">     631 </span>            : static inline bool queue_is_rq_based(struct request_queue *q)
<span class="lineNum">     632 </span>            : {
<span class="lineNum">     633 </span>            :         return q-&gt;request_fn || q-&gt;mq_ops;
<span class="lineNum">     634 </span>            : }
<span class="lineNum">     635 </span>            : 
<span class="lineNum">     636 </span>            : static inline unsigned int blk_queue_cluster(struct request_queue *q)
<span class="lineNum">     637 </span>            : {
<span class="lineNum">     638 </span>            :         return q-&gt;limits.cluster;
<span class="lineNum">     639 </span>            : }
<span class="lineNum">     640 </span>            : 
<span class="lineNum">     641 </span>            : /*
<span class="lineNum">     642 </span>            :  * We regard a request as sync, if either a read or a sync write
<span class="lineNum">     643 </span>            :  */
<span class="lineNum">     644 </span>            : static inline bool rw_is_sync(unsigned int rw_flags)
<span class="lineNum">     645 </span>            : {
<span class="lineNum">     646 </span>            :         return !(rw_flags &amp; REQ_WRITE) || (rw_flags &amp; REQ_SYNC);
<span class="lineNum">     647 </span>            : }
<span class="lineNum">     648 </span>            : 
<span class="lineNum">     649 </span>            : static inline bool rq_is_sync(struct request *rq)
<span class="lineNum">     650 </span>            : {
<span class="lineNum">     651 </span>            :         return rw_is_sync(rq-&gt;cmd_flags);
<span class="lineNum">     652 </span>            : }
<span class="lineNum">     653 </span>            : 
<span class="lineNum">     654 </span>            : static inline bool blk_rl_full(struct request_list *rl, bool sync)
<span class="lineNum">     655 </span>            : {
<span class="lineNum">     656 </span>            :         unsigned int flag = sync ? BLK_RL_SYNCFULL : BLK_RL_ASYNCFULL;
<span class="lineNum">     657 </span>            : 
<span class="lineNum">     658 </span>            :         return rl-&gt;flags &amp; flag;
<span class="lineNum">     659 </span>            : }
<span class="lineNum">     660 </span>            : 
<span class="lineNum">     661 </span>            : static inline void blk_set_rl_full(struct request_list *rl, bool sync)
<span class="lineNum">     662 </span>            : {
<span class="lineNum">     663 </span>            :         unsigned int flag = sync ? BLK_RL_SYNCFULL : BLK_RL_ASYNCFULL;
<span class="lineNum">     664 </span>            : 
<span class="lineNum">     665 </span>            :         rl-&gt;flags |= flag;
<span class="lineNum">     666 </span>            : }
<span class="lineNum">     667 </span>            : 
<span class="lineNum">     668 </span>            : static inline void blk_clear_rl_full(struct request_list *rl, bool sync)
<span class="lineNum">     669 </span>            : {
<span class="lineNum">     670 </span>            :         unsigned int flag = sync ? BLK_RL_SYNCFULL : BLK_RL_ASYNCFULL;
<span class="lineNum">     671 </span>            : 
<span class="lineNum">     672 </span>            :         rl-&gt;flags &amp;= ~flag;
<span class="lineNum">     673 </span>            : }
<span class="lineNum">     674 </span>            : 
<span class="lineNum">     675 </span>            : static inline bool rq_mergeable(struct request *rq)
<span class="lineNum">     676 </span>            : {
<span class="lineNum">     677 </span>            :         if (rq-&gt;cmd_type != REQ_TYPE_FS)
<span class="lineNum">     678 </span>            :                 return false;
<span class="lineNum">     679 </span>            : 
<span class="lineNum">     680 </span>            :         if (rq-&gt;cmd_flags &amp; REQ_NOMERGE_FLAGS)
<span class="lineNum">     681 </span>            :                 return false;
<span class="lineNum">     682 </span>            : 
<span class="lineNum">     683 </span>            :         return true;
<span class="lineNum">     684 </span>            : }
<span class="lineNum">     685 </span>            : 
<span class="lineNum">     686 </span>            : static inline bool blk_check_merge_flags(unsigned int flags1,
<span class="lineNum">     687 </span>            :                                          unsigned int flags2)
<span class="lineNum">     688 </span>            : {
<span class="lineNum">     689 </span>            :         if ((flags1 &amp; REQ_DISCARD) != (flags2 &amp; REQ_DISCARD))
<span class="lineNum">     690 </span>            :                 return false;
<span class="lineNum">     691 </span>            : 
<span class="lineNum">     692 </span>            :         if ((flags1 &amp; REQ_SECURE) != (flags2 &amp; REQ_SECURE))
<span class="lineNum">     693 </span>            :                 return false;
<span class="lineNum">     694 </span>            : 
<span class="lineNum">     695 </span>            :         if ((flags1 &amp; REQ_WRITE_SAME) != (flags2 &amp; REQ_WRITE_SAME))
<span class="lineNum">     696 </span>            :                 return false;
<span class="lineNum">     697 </span>            : 
<span class="lineNum">     698 </span>            :         return true;
<span class="lineNum">     699 </span>            : }
<span class="lineNum">     700 </span>            : 
<span class="lineNum">     701 </span>            : static inline bool blk_write_same_mergeable(struct bio *a, struct bio *b)
<span class="lineNum">     702 </span>            : {
<span class="lineNum">     703 </span>            :         if (bio_data(a) == bio_data(b))
<span class="lineNum">     704 </span>            :                 return true;
<span class="lineNum">     705 </span>            : 
<span class="lineNum">     706 </span>            :         return false;
<span class="lineNum">     707 </span>            : }
<span class="lineNum">     708 </span>            : 
<span class="lineNum">     709 </span>            : /*
<span class="lineNum">     710 </span>            :  * q-&gt;prep_rq_fn return values
<span class="lineNum">     711 </span>            :  */
<span class="lineNum">     712 </span>            : #define BLKPREP_OK              0       /* serve it */
<span class="lineNum">     713 </span>            : #define BLKPREP_KILL            1       /* fatal error, kill */
<span class="lineNum">     714 </span>            : #define BLKPREP_DEFER           2       /* leave on queue */
<span class="lineNum">     715 </span>            : 
<span class="lineNum">     716 </span>            : extern unsigned long blk_max_low_pfn, blk_max_pfn;
<span class="lineNum">     717 </span>            : 
<span class="lineNum">     718 </span>            : /*
<span class="lineNum">     719 </span>            :  * standard bounce addresses:
<span class="lineNum">     720 </span>            :  *
<span class="lineNum">     721 </span>            :  * BLK_BOUNCE_HIGH      : bounce all highmem pages
<span class="lineNum">     722 </span>            :  * BLK_BOUNCE_ANY       : don't bounce anything
<span class="lineNum">     723 </span>            :  * BLK_BOUNCE_ISA       : bounce pages above ISA DMA boundary
<span class="lineNum">     724 </span>            :  */
<span class="lineNum">     725 </span>            : 
<span class="lineNum">     726 </span>            : #if BITS_PER_LONG == 32
<span class="lineNum">     727 </span>            : #define BLK_BOUNCE_HIGH         ((u64)blk_max_low_pfn &lt;&lt; PAGE_SHIFT)
<span class="lineNum">     728 </span>            : #else
<span class="lineNum">     729 </span>            : #define BLK_BOUNCE_HIGH         -1ULL
<span class="lineNum">     730 </span>            : #endif
<span class="lineNum">     731 </span>            : #define BLK_BOUNCE_ANY          (-1ULL)
<span class="lineNum">     732 </span>            : #define BLK_BOUNCE_ISA          (DMA_BIT_MASK(24))
<span class="lineNum">     733 </span>            : 
<span class="lineNum">     734 </span>            : /*
<span class="lineNum">     735 </span>            :  * default timeout for SG_IO if none specified
<span class="lineNum">     736 </span>            :  */
<span class="lineNum">     737 </span>            : #define BLK_DEFAULT_SG_TIMEOUT  (60 * HZ)
<span class="lineNum">     738 </span>            : #define BLK_MIN_SG_TIMEOUT      (7 * HZ)
<span class="lineNum">     739 </span>            : 
<span class="lineNum">     740 </span>            : #ifdef CONFIG_BOUNCE
<span class="lineNum">     741 </span>            : extern int init_emergency_isa_pool(void);
<span class="lineNum">     742 </span>            : extern void blk_queue_bounce(struct request_queue *q, struct bio **bio);
<span class="lineNum">     743 </span>            : #else
<span class="lineNum">     744 </span>            : static inline int init_emergency_isa_pool(void)
<span class="lineNum">     745 </span>            : {
<span class="lineNum">     746 </span>            :         return 0;
<span class="lineNum">     747 </span>            : }
<span class="lineNum">     748 </span>            : static inline void blk_queue_bounce(struct request_queue *q, struct bio **bio)
<span class="lineNum">     749 </span>            : {
<span class="lineNum">     750 </span>            : }
<span class="lineNum">     751 </span>            : #endif /* CONFIG_MMU */
<span class="lineNum">     752 </span>            : 
<span class="lineNum">     753 </span>            : struct rq_map_data {
<span class="lineNum">     754 </span>            :         struct page **pages;
<span class="lineNum">     755 </span>            :         int page_order;
<span class="lineNum">     756 </span>            :         int nr_entries;
<span class="lineNum">     757 </span>            :         unsigned long offset;
<span class="lineNum">     758 </span>            :         int null_mapped;
<span class="lineNum">     759 </span>            :         int from_user;
<span class="lineNum">     760 </span>            : };
<span class="lineNum">     761 </span>            : 
<span class="lineNum">     762 </span>            : struct req_iterator {
<span class="lineNum">     763 </span>            :         struct bvec_iter iter;
<span class="lineNum">     764 </span>            :         struct bio *bio;
<span class="lineNum">     765 </span>            : };
<span class="lineNum">     766 </span>            : 
<span class="lineNum">     767 </span>            : /* This should not be used directly - use rq_for_each_segment */
<span class="lineNum">     768 </span>            : #define for_each_bio(_bio)              \
<span class="lineNum">     769 </span>            :         for (; _bio; _bio = _bio-&gt;bi_next)
<span class="lineNum">     770 </span>            : #define __rq_for_each_bio(_bio, rq)     \
<span class="lineNum">     771 </span>            :         if ((rq-&gt;bio))                       \
<span class="lineNum">     772 </span>            :                 for (_bio = (rq)-&gt;bio; _bio; _bio = _bio-&gt;bi_next)
<span class="lineNum">     773 </span>            : 
<span class="lineNum">     774 </span>            : #define rq_for_each_segment(bvl, _rq, _iter)                    \
<span class="lineNum">     775 </span>            :         __rq_for_each_bio(_iter.bio, _rq)                       \
<span class="lineNum">     776 </span>            :                 bio_for_each_segment(bvl, _iter.bio, _iter.iter)
<span class="lineNum">     777 </span>            : 
<span class="lineNum">     778 </span>            : #define rq_iter_last(bvec, _iter)                               \
<span class="lineNum">     779 </span>            :                 (_iter.bio-&gt;bi_next == NULL &amp;&amp;                       \
<span class="lineNum">     780 </span>            :                  bio_iter_last(bvec, _iter.iter))
<span class="lineNum">     781 </span>            : 
<span class="lineNum">     782 </span>            : #ifndef ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE
<span class="lineNum">     783 </span>            : # error &quot;You should define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE for your platform&quot;
<span class="lineNum">     784 </span>            : #endif
<span class="lineNum">     785 </span>            : #if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE
<span class="lineNum">     786 </span>            : extern void rq_flush_dcache_pages(struct request *rq);
<span class="lineNum">     787 </span>            : #else
<span class="lineNum">     788 </span>            : static inline void rq_flush_dcache_pages(struct request *rq)
<span class="lineNum">     789 </span>            : {
<span class="lineNum">     790 </span>            : }
<span class="lineNum">     791 </span>            : #endif
<span class="lineNum">     792 </span>            : 
<span class="lineNum">     793 </span>            : extern int blk_register_queue(struct gendisk *disk);
<span class="lineNum">     794 </span>            : extern void blk_unregister_queue(struct gendisk *disk);
<span class="lineNum">     795 </span>            : extern void generic_make_request(struct bio *bio);
<span class="lineNum">     796 </span>            : extern void blk_rq_init(struct request_queue *q, struct request *rq);
<span class="lineNum">     797 </span>            : extern void blk_put_request(struct request *);
<span class="lineNum">     798 </span>            : extern void __blk_put_request(struct request_queue *, struct request *);
<span class="lineNum">     799 </span>            : extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
<span class="lineNum">     800 </span>            : extern struct request *blk_make_request(struct request_queue *, struct bio *,
<span class="lineNum">     801 </span>            :                                         gfp_t);
<span class="lineNum">     802 </span>            : extern void blk_rq_set_block_pc(struct request *);
<span class="lineNum">     803 </span>            : extern void blk_requeue_request(struct request_queue *, struct request *);
<span class="lineNum">     804 </span>            : extern void blk_add_request_payload(struct request *rq, struct page *page,
<span class="lineNum">     805 </span>            :                 unsigned int len);
<span class="lineNum">     806 </span>            : extern int blk_rq_check_limits(struct request_queue *q, struct request *rq);
<span class="lineNum">     807 </span>            : extern int blk_lld_busy(struct request_queue *q);
<span class="lineNum">     808 </span>            : extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
<span class="lineNum">     809 </span>            :                              struct bio_set *bs, gfp_t gfp_mask,
<span class="lineNum">     810 </span>            :                              int (*bio_ctr)(struct bio *, struct bio *, void *),
<span class="lineNum">     811 </span>            :                              void *data);
<span class="lineNum">     812 </span>            : extern void blk_rq_unprep_clone(struct request *rq);
<span class="lineNum">     813 </span>            : extern int blk_insert_cloned_request(struct request_queue *q,
<span class="lineNum">     814 </span>            :                                      struct request *rq);
<span class="lineNum">     815 </span>            : extern void blk_delay_queue(struct request_queue *, unsigned long);
<span class="lineNum">     816 </span>            : extern void blk_recount_segments(struct request_queue *, struct bio *);
<span class="lineNum">     817 </span>            : extern int scsi_verify_blk_ioctl(struct block_device *, unsigned int);
<span class="lineNum">     818 </span>            : extern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,
<span class="lineNum">     819 </span>            :                               unsigned int, void __user *);
<span class="lineNum">     820 </span>            : extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
<span class="lineNum">     821 </span>            :                           unsigned int, void __user *);
<span class="lineNum">     822 </span>            : extern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,
<span class="lineNum">     823 </span>            :                          struct scsi_ioctl_command __user *);
<span class="lineNum">     824 </span>            : 
<span class="lineNum">     825 </span>            : extern void blk_queue_bio(struct request_queue *q, struct bio *bio);
<span class="lineNum">     826 </span>            : 
<span class="lineNum">     827 </span>            : /*
<span class="lineNum">     828 </span>            :  * A queue has just exitted congestion.  Note this in the global counter of
<span class="lineNum">     829 </span>            :  * congested queues, and wake up anyone who was waiting for requests to be
<span class="lineNum">     830 </span>            :  * put back.
<span class="lineNum">     831 </span>            :  */
<span class="lineNum">     832 </span>            : static inline void blk_clear_queue_congested(struct request_queue *q, int sync)
<span class="lineNum">     833 </span>            : {
<span class="lineNum">     834 </span>            :         clear_bdi_congested(&amp;q-&gt;backing_dev_info, sync);
<span class="lineNum">     835 </span>            : }
<span class="lineNum">     836 </span>            : 
<span class="lineNum">     837 </span>            : /*
<span class="lineNum">     838 </span>            :  * A queue has just entered congestion.  Flag that in the queue's VM-visible
<span class="lineNum">     839 </span>            :  * state flags and increment the global gounter of congested queues.
<span class="lineNum">     840 </span>            :  */
<span class="lineNum">     841 </span>            : static inline void blk_set_queue_congested(struct request_queue *q, int sync)
<span class="lineNum">     842 </span>            : {
<span class="lineNum">     843 </span>            :         set_bdi_congested(&amp;q-&gt;backing_dev_info, sync);
<span class="lineNum">     844 </span>            : }
<span class="lineNum">     845 </span>            : 
<span class="lineNum">     846 </span>            : extern void blk_start_queue(struct request_queue *q);
<span class="lineNum">     847 </span>            : extern void blk_stop_queue(struct request_queue *q);
<span class="lineNum">     848 </span>            : extern void blk_sync_queue(struct request_queue *q);
<span class="lineNum">     849 </span>            : extern void __blk_stop_queue(struct request_queue *q);
<span class="lineNum">     850 </span>            : extern void __blk_run_queue(struct request_queue *q);
<span class="lineNum">     851 </span>            : extern void blk_run_queue(struct request_queue *);
<span class="lineNum">     852 </span>            : extern void blk_run_queue_async(struct request_queue *q);
<span class="lineNum">     853 </span>            : extern int blk_rq_map_user(struct request_queue *, struct request *,
<span class="lineNum">     854 </span>            :                            struct rq_map_data *, void __user *, unsigned long,
<span class="lineNum">     855 </span>            :                            gfp_t);
<span class="lineNum">     856 </span>            : extern int blk_rq_unmap_user(struct bio *);
<span class="lineNum">     857 </span>            : extern int blk_rq_map_kern(struct request_queue *, struct request *, void *, unsigned int, gfp_t);
<span class="lineNum">     858 </span>            : extern int blk_rq_map_user_iov(struct request_queue *, struct request *,
<span class="lineNum">     859 </span>            :                                struct rq_map_data *, const struct sg_iovec *,
<span class="lineNum">     860 </span>            :                                int, unsigned int, gfp_t);
<span class="lineNum">     861 </span>            : extern int blk_execute_rq(struct request_queue *, struct gendisk *,
<span class="lineNum">     862 </span>            :                           struct request *, int);
<span class="lineNum">     863 </span>            : extern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,
<span class="lineNum">     864 </span>            :                                   struct request *, int, rq_end_io_fn *);
<span class="lineNum">     865 </span>            : 
<span class="lineNum">     866 </span>            : static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
<span class="lineNum">     867 </span>            : {
<span class="lineNum">     868 </span><span class="lineCov">     213418 :         return bdev-&gt;bd_disk-&gt;queue;</span>
<span class="lineNum">     869 </span>            : }
<span class="lineNum">     870 </span>            : 
<span class="lineNum">     871 </span>            : /*
<span class="lineNum">     872 </span>            :  * blk_rq_pos()                 : the current sector
<span class="lineNum">     873 </span>            :  * blk_rq_bytes()               : bytes left in the entire request
<span class="lineNum">     874 </span>            :  * blk_rq_cur_bytes()           : bytes left in the current segment
<span class="lineNum">     875 </span>            :  * blk_rq_err_bytes()           : bytes left till the next error boundary
<span class="lineNum">     876 </span>            :  * blk_rq_sectors()             : sectors left in the entire request
<span class="lineNum">     877 </span>            :  * blk_rq_cur_sectors()         : sectors left in the current segment
<span class="lineNum">     878 </span>            :  */
<span class="lineNum">     879 </span>            : static inline sector_t blk_rq_pos(const struct request *rq)
<span class="lineNum">     880 </span>            : {
<span class="lineNum">     881 </span>            :         return rq-&gt;__sector;
<span class="lineNum">     882 </span>            : }
<span class="lineNum">     883 </span>            : 
<span class="lineNum">     884 </span>            : static inline unsigned int blk_rq_bytes(const struct request *rq)
<span class="lineNum">     885 </span>            : {
<span class="lineNum">     886 </span>            :         return rq-&gt;__data_len;
<span class="lineNum">     887 </span>            : }
<span class="lineNum">     888 </span>            : 
<span class="lineNum">     889 </span>            : static inline int blk_rq_cur_bytes(const struct request *rq)
<span class="lineNum">     890 </span>            : {
<span class="lineNum">     891 </span>            :         return rq-&gt;bio ? bio_cur_bytes(rq-&gt;bio) : 0;
<span class="lineNum">     892 </span>            : }
<span class="lineNum">     893 </span>            : 
<span class="lineNum">     894 </span>            : extern unsigned int blk_rq_err_bytes(const struct request *rq);
<span class="lineNum">     895 </span>            : 
<span class="lineNum">     896 </span>            : static inline unsigned int blk_rq_sectors(const struct request *rq)
<span class="lineNum">     897 </span>            : {
<span class="lineNum">     898 </span>            :         return blk_rq_bytes(rq) &gt;&gt; 9;
<span class="lineNum">     899 </span>            : }
<span class="lineNum">     900 </span>            : 
<span class="lineNum">     901 </span>            : static inline unsigned int blk_rq_cur_sectors(const struct request *rq)
<span class="lineNum">     902 </span>            : {
<span class="lineNum">     903 </span>            :         return blk_rq_cur_bytes(rq) &gt;&gt; 9;
<span class="lineNum">     904 </span>            : }
<span class="lineNum">     905 </span>            : 
<span class="lineNum">     906 </span>            : static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
<span class="lineNum">     907 </span>            :                                                      unsigned int cmd_flags)
<span class="lineNum">     908 </span>            : {
<span class="lineNum">     909 </span>            :         if (unlikely(cmd_flags &amp; REQ_DISCARD))
<span class="lineNum">     910 </span>            :                 return min(q-&gt;limits.max_discard_sectors, UINT_MAX &gt;&gt; 9);
<span class="lineNum">     911 </span>            : 
<span class="lineNum">     912 </span>            :         if (unlikely(cmd_flags &amp; REQ_WRITE_SAME))
<span class="lineNum">     913 </span>            :                 return q-&gt;limits.max_write_same_sectors;
<span class="lineNum">     914 </span>            : 
<span class="lineNum">     915 </span>            :         return q-&gt;limits.max_sectors;
<span class="lineNum">     916 </span>            : }
<span class="lineNum">     917 </span>            : 
<span class="lineNum">     918 </span>            : /*
<span class="lineNum">     919 </span>            :  * Return maximum size of a request at given offset. Only valid for
<span class="lineNum">     920 </span>            :  * file system requests.
<span class="lineNum">     921 </span>            :  */
<span class="lineNum">     922 </span>            : static inline unsigned int blk_max_size_offset(struct request_queue *q,
<span class="lineNum">     923 </span>            :                                                sector_t offset)
<span class="lineNum">     924 </span>            : {
<span class="lineNum">     925 </span>            :         if (!q-&gt;limits.chunk_sectors)
<span class="lineNum">     926 </span>            :                 return q-&gt;limits.max_sectors;
<span class="lineNum">     927 </span>            : 
<span class="lineNum">     928 </span>            :         return q-&gt;limits.chunk_sectors -
<span class="lineNum">     929 </span>            :                         (offset &amp; (q-&gt;limits.chunk_sectors - 1));
<span class="lineNum">     930 </span>            : }
<span class="lineNum">     931 </span>            : 
<span class="lineNum">     932 </span>            : static inline unsigned int blk_rq_get_max_sectors(struct request *rq)
<span class="lineNum">     933 </span>            : {
<span class="lineNum">     934 </span>            :         struct request_queue *q = rq-&gt;q;
<span class="lineNum">     935 </span>            : 
<span class="lineNum">     936 </span>            :         if (unlikely(rq-&gt;cmd_type == REQ_TYPE_BLOCK_PC))
<span class="lineNum">     937 </span>            :                 return q-&gt;limits.max_hw_sectors;
<span class="lineNum">     938 </span>            : 
<span class="lineNum">     939 </span>            :         if (!q-&gt;limits.chunk_sectors)
<span class="lineNum">     940 </span>            :                 return blk_queue_get_max_sectors(q, rq-&gt;cmd_flags);
<span class="lineNum">     941 </span>            : 
<span class="lineNum">     942 </span>            :         return min(blk_max_size_offset(q, blk_rq_pos(rq)),
<span class="lineNum">     943 </span>            :                         blk_queue_get_max_sectors(q, rq-&gt;cmd_flags));
<span class="lineNum">     944 </span>            : }
<span class="lineNum">     945 </span>            : 
<span class="lineNum">     946 </span>            : static inline unsigned int blk_rq_count_bios(struct request *rq)
<span class="lineNum">     947 </span>            : {
<span class="lineNum">     948 </span>            :         unsigned int nr_bios = 0;
<span class="lineNum">     949 </span>            :         struct bio *bio;
<span class="lineNum">     950 </span>            : 
<span class="lineNum">     951 </span>            :         __rq_for_each_bio(bio, rq)
<span class="lineNum">     952 </span>            :                 nr_bios++;
<span class="lineNum">     953 </span>            : 
<span class="lineNum">     954 </span>            :         return nr_bios;
<span class="lineNum">     955 </span>            : }
<span class="lineNum">     956 </span>            : 
<span class="lineNum">     957 </span>            : /*
<span class="lineNum">     958 </span>            :  * Request issue related functions.
<span class="lineNum">     959 </span>            :  */
<span class="lineNum">     960 </span>            : extern struct request *blk_peek_request(struct request_queue *q);
<span class="lineNum">     961 </span>            : extern void blk_start_request(struct request *rq);
<span class="lineNum">     962 </span>            : extern struct request *blk_fetch_request(struct request_queue *q);
<span class="lineNum">     963 </span>            : 
<span class="lineNum">     964 </span>            : /*
<span class="lineNum">     965 </span>            :  * Request completion related functions.
<span class="lineNum">     966 </span>            :  *
<span class="lineNum">     967 </span>            :  * blk_update_request() completes given number of bytes and updates
<span class="lineNum">     968 </span>            :  * the request without completing it.
<span class="lineNum">     969 </span>            :  *
<span class="lineNum">     970 </span>            :  * blk_end_request() and friends.  __blk_end_request() must be called
<span class="lineNum">     971 </span>            :  * with the request queue spinlock acquired.
<span class="lineNum">     972 </span>            :  *
<span class="lineNum">     973 </span>            :  * Several drivers define their own end_request and call
<span class="lineNum">     974 </span>            :  * blk_end_request() for parts of the original function.
<span class="lineNum">     975 </span>            :  * This prevents code duplication in drivers.
<span class="lineNum">     976 </span>            :  */
<span class="lineNum">     977 </span>            : extern bool blk_update_request(struct request *rq, int error,
<span class="lineNum">     978 </span>            :                                unsigned int nr_bytes);
<span class="lineNum">     979 </span>            : extern void blk_finish_request(struct request *rq, int error);
<span class="lineNum">     980 </span>            : extern bool blk_end_request(struct request *rq, int error,
<span class="lineNum">     981 </span>            :                             unsigned int nr_bytes);
<span class="lineNum">     982 </span>            : extern void blk_end_request_all(struct request *rq, int error);
<span class="lineNum">     983 </span>            : extern bool blk_end_request_cur(struct request *rq, int error);
<span class="lineNum">     984 </span>            : extern bool blk_end_request_err(struct request *rq, int error);
<span class="lineNum">     985 </span>            : extern bool __blk_end_request(struct request *rq, int error,
<span class="lineNum">     986 </span>            :                               unsigned int nr_bytes);
<span class="lineNum">     987 </span>            : extern void __blk_end_request_all(struct request *rq, int error);
<span class="lineNum">     988 </span>            : extern bool __blk_end_request_cur(struct request *rq, int error);
<span class="lineNum">     989 </span>            : extern bool __blk_end_request_err(struct request *rq, int error);
<span class="lineNum">     990 </span>            : 
<span class="lineNum">     991 </span>            : extern void blk_complete_request(struct request *);
<span class="lineNum">     992 </span>            : extern void __blk_complete_request(struct request *);
<span class="lineNum">     993 </span>            : extern void blk_abort_request(struct request *);
<span class="lineNum">     994 </span>            : extern void blk_unprep_request(struct request *);
<span class="lineNum">     995 </span>            : 
<span class="lineNum">     996 </span>            : /*
<span class="lineNum">     997 </span>            :  * Access functions for manipulating queue properties
<span class="lineNum">     998 </span>            :  */
<span class="lineNum">     999 </span>            : extern struct request_queue *blk_init_queue_node(request_fn_proc *rfn,
<span class="lineNum">    1000 </span>            :                                         spinlock_t *lock, int node_id);
<span class="lineNum">    1001 </span>            : extern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);
<span class="lineNum">    1002 </span>            : extern struct request_queue *blk_init_allocated_queue(struct request_queue *,
<span class="lineNum">    1003 </span>            :                                                       request_fn_proc *, spinlock_t *);
<span class="lineNum">    1004 </span>            : extern void blk_cleanup_queue(struct request_queue *);
<span class="lineNum">    1005 </span>            : extern void blk_queue_make_request(struct request_queue *, make_request_fn *);
<span class="lineNum">    1006 </span>            : extern void blk_queue_bounce_limit(struct request_queue *, u64);
<span class="lineNum">    1007 </span>            : extern void blk_limits_max_hw_sectors(struct queue_limits *, unsigned int);
<span class="lineNum">    1008 </span>            : extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
<span class="lineNum">    1009 </span>            : extern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);
<span class="lineNum">    1010 </span>            : extern void blk_queue_max_segments(struct request_queue *, unsigned short);
<span class="lineNum">    1011 </span>            : extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
<span class="lineNum">    1012 </span>            : extern void blk_queue_max_discard_sectors(struct request_queue *q,
<span class="lineNum">    1013 </span>            :                 unsigned int max_discard_sectors);
<span class="lineNum">    1014 </span>            : extern void blk_queue_max_write_same_sectors(struct request_queue *q,
<span class="lineNum">    1015 </span>            :                 unsigned int max_write_same_sectors);
<span class="lineNum">    1016 </span>            : extern void blk_queue_logical_block_size(struct request_queue *, unsigned short);
<span class="lineNum">    1017 </span>            : extern void blk_queue_physical_block_size(struct request_queue *, unsigned int);
<span class="lineNum">    1018 </span>            : extern void blk_queue_alignment_offset(struct request_queue *q,
<span class="lineNum">    1019 </span>            :                                        unsigned int alignment);
<span class="lineNum">    1020 </span>            : extern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);
<span class="lineNum">    1021 </span>            : extern void blk_queue_io_min(struct request_queue *q, unsigned int min);
<span class="lineNum">    1022 </span>            : extern void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt);
<span class="lineNum">    1023 </span>            : extern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);
<span class="lineNum">    1024 </span>            : extern void blk_set_default_limits(struct queue_limits *lim);
<span class="lineNum">    1025 </span>            : extern void blk_set_stacking_limits(struct queue_limits *lim);
<span class="lineNum">    1026 </span>            : extern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
<span class="lineNum">    1027 </span>            :                             sector_t offset);
<span class="lineNum">    1028 </span>            : extern int bdev_stack_limits(struct queue_limits *t, struct block_device *bdev,
<span class="lineNum">    1029 </span>            :                             sector_t offset);
<span class="lineNum">    1030 </span>            : extern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,
<span class="lineNum">    1031 </span>            :                               sector_t offset);
<span class="lineNum">    1032 </span>            : extern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);
<span class="lineNum">    1033 </span>            : extern void blk_queue_dma_pad(struct request_queue *, unsigned int);
<span class="lineNum">    1034 </span>            : extern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);
<span class="lineNum">    1035 </span>            : extern int blk_queue_dma_drain(struct request_queue *q,
<span class="lineNum">    1036 </span>            :                                dma_drain_needed_fn *dma_drain_needed,
<span class="lineNum">    1037 </span>            :                                void *buf, unsigned int size);
<span class="lineNum">    1038 </span>            : extern void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn);
<span class="lineNum">    1039 </span>            : extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
<span class="lineNum">    1040 </span>            : extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
<span class="lineNum">    1041 </span>            : extern void blk_queue_unprep_rq(struct request_queue *, unprep_rq_fn *ufn);
<span class="lineNum">    1042 </span>            : extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);
<span class="lineNum">    1043 </span>            : extern void blk_queue_dma_alignment(struct request_queue *, int);
<span class="lineNum">    1044 </span>            : extern void blk_queue_update_dma_alignment(struct request_queue *, int);
<span class="lineNum">    1045 </span>            : extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
<span class="lineNum">    1046 </span>            : extern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);
<span class="lineNum">    1047 </span>            : extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
<span class="lineNum">    1048 </span>            : extern void blk_queue_flush(struct request_queue *q, unsigned int flush);
<span class="lineNum">    1049 </span>            : extern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);
<span class="lineNum">    1050 </span>            : extern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);
<span class="lineNum">    1051 </span>            : 
<span class="lineNum">    1052 </span>            : extern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);
<span class="lineNum">    1053 </span>            : extern int blk_bio_map_sg(struct request_queue *q, struct bio *bio,
<span class="lineNum">    1054 </span>            :                           struct scatterlist *sglist);
<span class="lineNum">    1055 </span>            : extern void blk_dump_rq_flags(struct request *, char *);
<span class="lineNum">    1056 </span>            : extern long nr_blockdev_pages(void);
<span class="lineNum">    1057 </span>            : 
<span class="lineNum">    1058 </span>            : bool __must_check blk_get_queue(struct request_queue *);
<span class="lineNum">    1059 </span>            : struct request_queue *blk_alloc_queue(gfp_t);
<span class="lineNum">    1060 </span>            : struct request_queue *blk_alloc_queue_node(gfp_t, int);
<span class="lineNum">    1061 </span>            : extern void blk_put_queue(struct request_queue *);
<span class="lineNum">    1062 </span>            : 
<span class="lineNum">    1063 </span>            : /*
<span class="lineNum">    1064 </span>            :  * block layer runtime pm functions
<span class="lineNum">    1065 </span>            :  */
<span class="lineNum">    1066 </span>            : #ifdef CONFIG_PM_RUNTIME
<span class="lineNum">    1067 </span>            : extern void blk_pm_runtime_init(struct request_queue *q, struct device *dev);
<span class="lineNum">    1068 </span>            : extern int blk_pre_runtime_suspend(struct request_queue *q);
<span class="lineNum">    1069 </span>            : extern void blk_post_runtime_suspend(struct request_queue *q, int err);
<span class="lineNum">    1070 </span>            : extern void blk_pre_runtime_resume(struct request_queue *q);
<span class="lineNum">    1071 </span>            : extern void blk_post_runtime_resume(struct request_queue *q, int err);
<span class="lineNum">    1072 </span>            : #else
<span class="lineNum">    1073 </span>            : static inline void blk_pm_runtime_init(struct request_queue *q,
<span class="lineNum">    1074 </span>            :         struct device *dev) {}
<span class="lineNum">    1075 </span>            : static inline int blk_pre_runtime_suspend(struct request_queue *q)
<span class="lineNum">    1076 </span>            : {
<span class="lineNum">    1077 </span>            :         return -ENOSYS;
<span class="lineNum">    1078 </span>            : }
<span class="lineNum">    1079 </span>            : static inline void blk_post_runtime_suspend(struct request_queue *q, int err) {}
<span class="lineNum">    1080 </span>            : static inline void blk_pre_runtime_resume(struct request_queue *q) {}
<span class="lineNum">    1081 </span>            : static inline void blk_post_runtime_resume(struct request_queue *q, int err) {}
<span class="lineNum">    1082 </span>            : #endif
<span class="lineNum">    1083 </span>            : 
<span class="lineNum">    1084 </span>            : /*
<span class="lineNum">    1085 </span>            :  * blk_plug permits building a queue of related requests by holding the I/O
<span class="lineNum">    1086 </span>            :  * fragments for a short period. This allows merging of sequential requests
<span class="lineNum">    1087 </span>            :  * into single larger request. As the requests are moved from a per-task list to
<span class="lineNum">    1088 </span>            :  * the device's request_queue in a batch, this results in improved scalability
<span class="lineNum">    1089 </span>            :  * as the lock contention for request_queue lock is reduced.
<span class="lineNum">    1090 </span>            :  *
<span class="lineNum">    1091 </span>            :  * It is ok not to disable preemption when adding the request to the plug list
<span class="lineNum">    1092 </span>            :  * or when attempting a merge, because blk_schedule_flush_list() will only flush
<span class="lineNum">    1093 </span>            :  * the plug list when the task sleeps by itself. For details, please see
<span class="lineNum">    1094 </span>            :  * schedule() where blk_schedule_flush_plug() is called.
<span class="lineNum">    1095 </span>            :  */
<span class="lineNum">    1096 </span>            : struct blk_plug {
<span class="lineNum">    1097 </span>            :         struct list_head list; /* requests */
<span class="lineNum">    1098 </span>            :         struct list_head mq_list; /* blk-mq requests */
<span class="lineNum">    1099 </span>            :         struct list_head cb_list; /* md requires an unplug callback */
<span class="lineNum">    1100 </span>            : };
<span class="lineNum">    1101 </span>            : #define BLK_MAX_REQUEST_COUNT 16
<span class="lineNum">    1102 </span>            : 
<span class="lineNum">    1103 </span>            : struct blk_plug_cb;
<span class="lineNum">    1104 </span>            : typedef void (*blk_plug_cb_fn)(struct blk_plug_cb *, bool);
<span class="lineNum">    1105 </span>            : struct blk_plug_cb {
<span class="lineNum">    1106 </span>            :         struct list_head list;
<span class="lineNum">    1107 </span>            :         blk_plug_cb_fn callback;
<span class="lineNum">    1108 </span>            :         void *data;
<span class="lineNum">    1109 </span>            : };
<span class="lineNum">    1110 </span>            : extern struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug,
<span class="lineNum">    1111 </span>            :                                              void *data, int size);
<span class="lineNum">    1112 </span>            : extern void blk_start_plug(struct blk_plug *);
<span class="lineNum">    1113 </span>            : extern void blk_finish_plug(struct blk_plug *);
<span class="lineNum">    1114 </span>            : extern void blk_flush_plug_list(struct blk_plug *, bool);
<span class="lineNum">    1115 </span>            : 
<span class="lineNum">    1116 </span>            : static inline void blk_flush_plug(struct task_struct *tsk)
<span class="lineNum">    1117 </span>            : {
<span class="lineNum">    1118 </span>            :         struct blk_plug *plug = tsk-&gt;plug;
<span class="lineNum">    1119 </span>            : 
<span class="lineNum">    1120 </span>            :         if (plug)
<span class="lineNum">    1121 </span>            :                 blk_flush_plug_list(plug, false);
<span class="lineNum">    1122 </span>            : }
<span class="lineNum">    1123 </span>            : 
<span class="lineNum">    1124 </span>            : static inline void blk_schedule_flush_plug(struct task_struct *tsk)
<span class="lineNum">    1125 </span>            : {
<span class="lineNum">    1126 </span>            :         struct blk_plug *plug = tsk-&gt;plug;
<span class="lineNum">    1127 </span>            : 
<span class="lineNum">    1128 </span>            :         if (plug)
<span class="lineNum">    1129 </span>            :                 blk_flush_plug_list(plug, true);
<span class="lineNum">    1130 </span>            : }
<span class="lineNum">    1131 </span>            : 
<span class="lineNum">    1132 </span>            : static inline bool blk_needs_flush_plug(struct task_struct *tsk)
<span class="lineNum">    1133 </span>            : {
<span class="lineNum">    1134 </span>            :         struct blk_plug *plug = tsk-&gt;plug;
<span class="lineNum">    1135 </span>            : 
<span class="lineNum">    1136 </span>            :         return plug &amp;&amp;
<span class="lineNum">    1137 </span>            :                 (!list_empty(&amp;plug-&gt;list) ||
<span class="lineNum">    1138 </span>            :                  !list_empty(&amp;plug-&gt;mq_list) ||
<span class="lineNum">    1139 </span>            :                  !list_empty(&amp;plug-&gt;cb_list));
<span class="lineNum">    1140 </span>            : }
<span class="lineNum">    1141 </span>            : 
<span class="lineNum">    1142 </span>            : /*
<span class="lineNum">    1143 </span>            :  * tag stuff
<span class="lineNum">    1144 </span>            :  */
<span class="lineNum">    1145 </span>            : #define blk_rq_tagged(rq) \
<span class="lineNum">    1146 </span>            :         ((rq)-&gt;mq_ctx || ((rq)-&gt;cmd_flags &amp; REQ_QUEUED))
<span class="lineNum">    1147 </span>            : extern int blk_queue_start_tag(struct request_queue *, struct request *);
<span class="lineNum">    1148 </span>            : extern struct request *blk_queue_find_tag(struct request_queue *, int);
<span class="lineNum">    1149 </span>            : extern void blk_queue_end_tag(struct request_queue *, struct request *);
<span class="lineNum">    1150 </span>            : extern int blk_queue_init_tags(struct request_queue *, int, struct blk_queue_tag *);
<span class="lineNum">    1151 </span>            : extern void blk_queue_free_tags(struct request_queue *);
<span class="lineNum">    1152 </span>            : extern int blk_queue_resize_tags(struct request_queue *, int);
<span class="lineNum">    1153 </span>            : extern void blk_queue_invalidate_tags(struct request_queue *);
<span class="lineNum">    1154 </span>            : extern struct blk_queue_tag *blk_init_tags(int);
<span class="lineNum">    1155 </span>            : extern void blk_free_tags(struct blk_queue_tag *);
<span class="lineNum">    1156 </span>            : 
<span class="lineNum">    1157 </span>            : static inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,
<span class="lineNum">    1158 </span>            :                                                 int tag)
<span class="lineNum">    1159 </span>            : {
<span class="lineNum">    1160 </span>            :         if (unlikely(bqt == NULL || tag &gt;= bqt-&gt;real_max_depth))
<span class="lineNum">    1161 </span>            :                 return NULL;
<span class="lineNum">    1162 </span>            :         return bqt-&gt;tag_index[tag];
<span class="lineNum">    1163 </span>            : }
<span class="lineNum">    1164 </span>            : 
<span class="lineNum">    1165 </span>            : #define BLKDEV_DISCARD_SECURE  0x01    /* secure discard */
<span class="lineNum">    1166 </span>            : 
<span class="lineNum">    1167 </span>            : extern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);
<span class="lineNum">    1168 </span>            : extern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
<span class="lineNum">    1169 </span>            :                 sector_t nr_sects, gfp_t gfp_mask, unsigned long flags);
<span class="lineNum">    1170 </span>            : extern int blkdev_issue_write_same(struct block_device *bdev, sector_t sector,
<span class="lineNum">    1171 </span>            :                 sector_t nr_sects, gfp_t gfp_mask, struct page *page);
<span class="lineNum">    1172 </span>            : extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
<span class="lineNum">    1173 </span>            :                         sector_t nr_sects, gfp_t gfp_mask);
<span class="lineNum">    1174 </span>            : static inline int sb_issue_discard(struct super_block *sb, sector_t block,
<span class="lineNum">    1175 </span>            :                 sector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)
<span class="lineNum">    1176 </span>            : {
<span class="lineNum">    1177 </span>            :         return blkdev_issue_discard(sb-&gt;s_bdev, block &lt;&lt; (sb-&gt;s_blocksize_bits - 9),
<span class="lineNum">    1178 </span>            :                                     nr_blocks &lt;&lt; (sb-&gt;s_blocksize_bits - 9),
<span class="lineNum">    1179 </span>            :                                     gfp_mask, flags);
<span class="lineNum">    1180 </span>            : }
<span class="lineNum">    1181 </span>            : static inline int sb_issue_zeroout(struct super_block *sb, sector_t block,
<span class="lineNum">    1182 </span>            :                 sector_t nr_blocks, gfp_t gfp_mask)
<span class="lineNum">    1183 </span>            : {
<span class="lineNum">    1184 </span>            :         return blkdev_issue_zeroout(sb-&gt;s_bdev,
<span class="lineNum">    1185 </span>            :                                     block &lt;&lt; (sb-&gt;s_blocksize_bits - 9),
<span class="lineNum">    1186 </span>            :                                     nr_blocks &lt;&lt; (sb-&gt;s_blocksize_bits - 9),
<span class="lineNum">    1187 </span>            :                                     gfp_mask);
<span class="lineNum">    1188 </span>            : }
<span class="lineNum">    1189 </span>            : 
<span class="lineNum">    1190 </span>            : extern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);
<span class="lineNum">    1191 </span>            : 
<span class="lineNum">    1192 </span>            : enum blk_default_limits {
<span class="lineNum">    1193 </span>            :         BLK_MAX_SEGMENTS        = 128,
<span class="lineNum">    1194 </span>            :         BLK_SAFE_MAX_SECTORS    = 255,
<span class="lineNum">    1195 </span>            :         BLK_DEF_MAX_SECTORS     = 1024,
<span class="lineNum">    1196 </span>            :         BLK_MAX_SEGMENT_SIZE    = 65536,
<span class="lineNum">    1197 </span>            :         BLK_SEG_BOUNDARY_MASK   = 0xFFFFFFFFUL,
<span class="lineNum">    1198 </span>            : };
<span class="lineNum">    1199 </span>            : 
<span class="lineNum">    1200 </span>            : #define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)
<span class="lineNum">    1201 </span>            : 
<span class="lineNum">    1202 </span>            : static inline unsigned long queue_bounce_pfn(struct request_queue *q)
<span class="lineNum">    1203 </span>            : {
<span class="lineNum">    1204 </span>            :         return q-&gt;limits.bounce_pfn;
<span class="lineNum">    1205 </span>            : }
<span class="lineNum">    1206 </span>            : 
<span class="lineNum">    1207 </span>            : static inline unsigned long queue_segment_boundary(struct request_queue *q)
<span class="lineNum">    1208 </span>            : {
<span class="lineNum">    1209 </span>            :         return q-&gt;limits.seg_boundary_mask;
<span class="lineNum">    1210 </span>            : }
<span class="lineNum">    1211 </span>            : 
<span class="lineNum">    1212 </span>            : static inline unsigned int queue_max_sectors(struct request_queue *q)
<span class="lineNum">    1213 </span>            : {
<span class="lineNum">    1214 </span>            :         return q-&gt;limits.max_sectors;
<span class="lineNum">    1215 </span>            : }
<span class="lineNum">    1216 </span>            : 
<span class="lineNum">    1217 </span>            : static inline unsigned int queue_max_hw_sectors(struct request_queue *q)
<span class="lineNum">    1218 </span>            : {
<span class="lineNum">    1219 </span>            :         return q-&gt;limits.max_hw_sectors;
<span class="lineNum">    1220 </span>            : }
<span class="lineNum">    1221 </span>            : 
<span class="lineNum">    1222 </span>            : static inline unsigned short queue_max_segments(struct request_queue *q)
<span class="lineNum">    1223 </span>            : {
<span class="lineNum">    1224 </span>            :         return q-&gt;limits.max_segments;
<span class="lineNum">    1225 </span>            : }
<span class="lineNum">    1226 </span>            : 
<span class="lineNum">    1227 </span>            : static inline unsigned int queue_max_segment_size(struct request_queue *q)
<span class="lineNum">    1228 </span>            : {
<span class="lineNum">    1229 </span>            :         return q-&gt;limits.max_segment_size;
<span class="lineNum">    1230 </span>            : }
<span class="lineNum">    1231 </span>            : 
<span class="lineNum">    1232 </span>            : static inline unsigned short queue_logical_block_size(struct request_queue *q)
<span class="lineNum">    1233 </span>            : {
<span class="lineNum">    1234 </span>            :         int retval = 512;
<span class="lineNum">    1235 </span>            : 
<span class="lineNum">    1236 </span>            :         if (q &amp;&amp; q-&gt;limits.logical_block_size)
<span class="lineNum">    1237 </span>            :                 retval = q-&gt;limits.logical_block_size;
<span class="lineNum">    1238 </span>            : 
<span class="lineNum">    1239 </span>            :         return retval;
<span class="lineNum">    1240 </span>            : }
<span class="lineNum">    1241 </span>            : 
<span class="lineNum">    1242 </span>            : static inline unsigned short bdev_logical_block_size(struct block_device *bdev)
<span class="lineNum">    1243 </span>            : {
<span class="lineNum">    1244 </span>            :         return queue_logical_block_size(bdev_get_queue(bdev));
<span class="lineNum">    1245 </span>            : }
<span class="lineNum">    1246 </span>            : 
<span class="lineNum">    1247 </span>            : static inline unsigned int queue_physical_block_size(struct request_queue *q)
<span class="lineNum">    1248 </span>            : {
<span class="lineNum">    1249 </span>            :         return q-&gt;limits.physical_block_size;
<span class="lineNum">    1250 </span>            : }
<span class="lineNum">    1251 </span>            : 
<span class="lineNum">    1252 </span>            : static inline unsigned int bdev_physical_block_size(struct block_device *bdev)
<span class="lineNum">    1253 </span>            : {
<span class="lineNum">    1254 </span>            :         return queue_physical_block_size(bdev_get_queue(bdev));
<span class="lineNum">    1255 </span>            : }
<span class="lineNum">    1256 </span>            : 
<span class="lineNum">    1257 </span>            : static inline unsigned int queue_io_min(struct request_queue *q)
<span class="lineNum">    1258 </span>            : {
<span class="lineNum">    1259 </span>            :         return q-&gt;limits.io_min;
<span class="lineNum">    1260 </span>            : }
<span class="lineNum">    1261 </span>            : 
<span class="lineNum">    1262 </span>            : static inline int bdev_io_min(struct block_device *bdev)
<span class="lineNum">    1263 </span>            : {
<span class="lineNum">    1264 </span>            :         return queue_io_min(bdev_get_queue(bdev));
<span class="lineNum">    1265 </span>            : }
<span class="lineNum">    1266 </span>            : 
<span class="lineNum">    1267 </span>            : static inline unsigned int queue_io_opt(struct request_queue *q)
<span class="lineNum">    1268 </span>            : {
<span class="lineNum">    1269 </span>            :         return q-&gt;limits.io_opt;
<span class="lineNum">    1270 </span>            : }
<span class="lineNum">    1271 </span>            : 
<span class="lineNum">    1272 </span>            : static inline int bdev_io_opt(struct block_device *bdev)
<span class="lineNum">    1273 </span>            : {
<span class="lineNum">    1274 </span>            :         return queue_io_opt(bdev_get_queue(bdev));
<span class="lineNum">    1275 </span>            : }
<span class="lineNum">    1276 </span>            : 
<span class="lineNum">    1277 </span>            : static inline int queue_alignment_offset(struct request_queue *q)
<span class="lineNum">    1278 </span>            : {
<span class="lineNum">    1279 </span>            :         if (q-&gt;limits.misaligned)
<span class="lineNum">    1280 </span>            :                 return -1;
<span class="lineNum">    1281 </span>            : 
<span class="lineNum">    1282 </span>            :         return q-&gt;limits.alignment_offset;
<span class="lineNum">    1283 </span>            : }
<span class="lineNum">    1284 </span>            : 
<span class="lineNum">    1285 </span>            : static inline int queue_limit_alignment_offset(struct queue_limits *lim, sector_t sector)
<span class="lineNum">    1286 </span>            : {
<span class="lineNum">    1287 </span>            :         unsigned int granularity = max(lim-&gt;physical_block_size, lim-&gt;io_min);
<span class="lineNum">    1288 </span>            :         unsigned int alignment = (sector &lt;&lt; 9) &amp; (granularity - 1);
<span class="lineNum">    1289 </span>            : 
<span class="lineNum">    1290 </span>            :         return (granularity + lim-&gt;alignment_offset - alignment)
<span class="lineNum">    1291 </span>            :                 &amp; (granularity - 1);
<span class="lineNum">    1292 </span>            : }
<span class="lineNum">    1293 </span>            : 
<span class="lineNum">    1294 </span>            : static inline int bdev_alignment_offset(struct block_device *bdev)
<span class="lineNum">    1295 </span>            : {
<span class="lineNum">    1296 </span>            :         struct request_queue *q = bdev_get_queue(bdev);
<span class="lineNum">    1297 </span>            : 
<span class="lineNum">    1298 </span>            :         if (q-&gt;limits.misaligned)
<span class="lineNum">    1299 </span>            :                 return -1;
<span class="lineNum">    1300 </span>            : 
<span class="lineNum">    1301 </span>            :         if (bdev != bdev-&gt;bd_contains)
<span class="lineNum">    1302 </span>            :                 return bdev-&gt;bd_part-&gt;alignment_offset;
<span class="lineNum">    1303 </span>            : 
<span class="lineNum">    1304 </span>            :         return q-&gt;limits.alignment_offset;
<span class="lineNum">    1305 </span>            : }
<span class="lineNum">    1306 </span>            : 
<span class="lineNum">    1307 </span>            : static inline int queue_discard_alignment(struct request_queue *q)
<span class="lineNum">    1308 </span>            : {
<span class="lineNum">    1309 </span>            :         if (q-&gt;limits.discard_misaligned)
<span class="lineNum">    1310 </span>            :                 return -1;
<span class="lineNum">    1311 </span>            : 
<span class="lineNum">    1312 </span>            :         return q-&gt;limits.discard_alignment;
<span class="lineNum">    1313 </span>            : }
<span class="lineNum">    1314 </span>            : 
<span class="lineNum">    1315 </span>            : static inline int queue_limit_discard_alignment(struct queue_limits *lim, sector_t sector)
<span class="lineNum">    1316 </span>            : {
<span class="lineNum">    1317 </span>            :         unsigned int alignment, granularity, offset;
<span class="lineNum">    1318 </span>            : 
<span class="lineNum">    1319 </span>            :         if (!lim-&gt;max_discard_sectors)
<span class="lineNum">    1320 </span>            :                 return 0;
<span class="lineNum">    1321 </span>            : 
<span class="lineNum">    1322 </span>            :         /* Why are these in bytes, not sectors? */
<span class="lineNum">    1323 </span>            :         alignment = lim-&gt;discard_alignment &gt;&gt; 9;
<span class="lineNum">    1324 </span>            :         granularity = lim-&gt;discard_granularity &gt;&gt; 9;
<span class="lineNum">    1325 </span>            :         if (!granularity)
<span class="lineNum">    1326 </span>            :                 return 0;
<span class="lineNum">    1327 </span>            : 
<span class="lineNum">    1328 </span>            :         /* Offset of the partition start in 'granularity' sectors */
<span class="lineNum">    1329 </span>            :         offset = sector_div(sector, granularity);
<span class="lineNum">    1330 </span>            : 
<span class="lineNum">    1331 </span>            :         /* And why do we do this modulus *again* in blkdev_issue_discard()? */
<span class="lineNum">    1332 </span>            :         offset = (granularity + alignment - offset) % granularity;
<span class="lineNum">    1333 </span>            : 
<span class="lineNum">    1334 </span>            :         /* Turn it back into bytes, gaah */
<span class="lineNum">    1335 </span>            :         return offset &lt;&lt; 9;
<span class="lineNum">    1336 </span>            : }
<span class="lineNum">    1337 </span>            : 
<span class="lineNum">    1338 </span>            : static inline int bdev_discard_alignment(struct block_device *bdev)
<span class="lineNum">    1339 </span>            : {
<span class="lineNum">    1340 </span>            :         struct request_queue *q = bdev_get_queue(bdev);
<span class="lineNum">    1341 </span>            : 
<span class="lineNum">    1342 </span>            :         if (bdev != bdev-&gt;bd_contains)
<span class="lineNum">    1343 </span>            :                 return bdev-&gt;bd_part-&gt;discard_alignment;
<span class="lineNum">    1344 </span>            : 
<span class="lineNum">    1345 </span>            :         return q-&gt;limits.discard_alignment;
<span class="lineNum">    1346 </span>            : }
<span class="lineNum">    1347 </span>            : 
<span class="lineNum">    1348 </span>            : static inline unsigned int queue_discard_zeroes_data(struct request_queue *q)
<span class="lineNum">    1349 </span>            : {
<span class="lineNum">    1350 </span>            :         if (q-&gt;limits.max_discard_sectors &amp;&amp; q-&gt;limits.discard_zeroes_data == 1)
<span class="lineNum">    1351 </span>            :                 return 1;
<span class="lineNum">    1352 </span>            : 
<span class="lineNum">    1353 </span>            :         return 0;
<span class="lineNum">    1354 </span>            : }
<span class="lineNum">    1355 </span>            : 
<span class="lineNum">    1356 </span>            : static inline unsigned int bdev_discard_zeroes_data(struct block_device *bdev)
<span class="lineNum">    1357 </span>            : {
<span class="lineNum">    1358 </span>            :         return queue_discard_zeroes_data(bdev_get_queue(bdev));
<span class="lineNum">    1359 </span>            : }
<span class="lineNum">    1360 </span>            : 
<span class="lineNum">    1361 </span>            : static inline unsigned int bdev_write_same(struct block_device *bdev)
<span class="lineNum">    1362 </span>            : {
<span class="lineNum">    1363 </span>            :         struct request_queue *q = bdev_get_queue(bdev);
<span class="lineNum">    1364 </span>            : 
<span class="lineNum">    1365 </span>            :         if (q)
<span class="lineNum">    1366 </span>            :                 return q-&gt;limits.max_write_same_sectors;
<span class="lineNum">    1367 </span>            : 
<span class="lineNum">    1368 </span>            :         return 0;
<span class="lineNum">    1369 </span>            : }
<span class="lineNum">    1370 </span>            : 
<span class="lineNum">    1371 </span>            : static inline int queue_dma_alignment(struct request_queue *q)
<span class="lineNum">    1372 </span>            : {
<span class="lineNum">    1373 </span>            :         return q ? q-&gt;dma_alignment : 511;
<span class="lineNum">    1374 </span>            : }
<span class="lineNum">    1375 </span>            : 
<span class="lineNum">    1376 </span>            : static inline int blk_rq_aligned(struct request_queue *q, unsigned long addr,
<span class="lineNum">    1377 </span>            :                                  unsigned int len)
<span class="lineNum">    1378 </span>            : {
<span class="lineNum">    1379 </span>            :         unsigned int alignment = queue_dma_alignment(q) | q-&gt;dma_pad_mask;
<span class="lineNum">    1380 </span>            :         return !(addr &amp; alignment) &amp;&amp; !(len &amp; alignment);
<span class="lineNum">    1381 </span>            : }
<span class="lineNum">    1382 </span>            : 
<span class="lineNum">    1383 </span>            : /* assumes size &gt; 256 */
<span class="lineNum">    1384 </span>            : static inline unsigned int blksize_bits(unsigned int size)
<span class="lineNum">    1385 </span>            : {
<span class="lineNum">    1386 </span>            :         unsigned int bits = 8;
<span class="lineNum">    1387 </span>            :         do {
<span class="lineNum">    1388 </span><span class="lineCov">       1768 :                 bits++;</span>
<span class="lineNum">    1389 </span><span class="lineCov">       1768 :                 size &gt;&gt;= 1;</span>
<span class="lineNum">    1390 </span><span class="lineCov">       1768 :         } while (size &gt; 256);</span>
<span class="lineNum">    1391 </span>            :         return bits;
<span class="lineNum">    1392 </span>            : }
<span class="lineNum">    1393 </span>            : 
<span class="lineNum">    1394 </span>            : static inline unsigned int block_size(struct block_device *bdev)
<span class="lineNum">    1395 </span>            : {
<span class="lineNum">    1396 </span>            :         return bdev-&gt;bd_block_size;
<span class="lineNum">    1397 </span>            : }
<span class="lineNum">    1398 </span>            : 
<span class="lineNum">    1399 </span>            : static inline bool queue_flush_queueable(struct request_queue *q)
<span class="lineNum">    1400 </span>            : {
<span class="lineNum">    1401 </span>            :         return !q-&gt;flush_not_queueable;
<span class="lineNum">    1402 </span>            : }
<span class="lineNum">    1403 </span>            : 
<span class="lineNum">    1404 </span>            : typedef struct {struct page *v;} Sector;
<span class="lineNum">    1405 </span>            : 
<span class="lineNum">    1406 </span>            : unsigned char *read_dev_sector(struct block_device *, sector_t, Sector *);
<span class="lineNum">    1407 </span>            : 
<span class="lineNum">    1408 </span>            : static inline void put_dev_sector(Sector p)
<span class="lineNum">    1409 </span>            : {
<span class="lineNum">    1410 </span>            :         page_cache_release(p.v);
<span class="lineNum">    1411 </span>            : }
<span class="lineNum">    1412 </span>            : 
<span class="lineNum">    1413 </span>            : struct work_struct;
<span class="lineNum">    1414 </span>            : int kblockd_schedule_work(struct work_struct *work);
<span class="lineNum">    1415 </span>            : int kblockd_schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);
<span class="lineNum">    1416 </span>            : int kblockd_schedule_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
<span class="lineNum">    1417 </span>            : 
<span class="lineNum">    1418 </span>            : #ifdef CONFIG_BLK_CGROUP
<span class="lineNum">    1419 </span>            : /*
<span class="lineNum">    1420 </span>            :  * This should not be using sched_clock(). A real patch is in progress
<span class="lineNum">    1421 </span>            :  * to fix this up, until that is in place we need to disable preemption
<span class="lineNum">    1422 </span>            :  * around sched_clock() in this function and set_io_start_time_ns().
<span class="lineNum">    1423 </span>            :  */
<span class="lineNum">    1424 </span>            : static inline void set_start_time_ns(struct request *req)
<span class="lineNum">    1425 </span>            : {
<span class="lineNum">    1426 </span>            :         preempt_disable();
<span class="lineNum">    1427 </span>            :         req-&gt;start_time_ns = sched_clock();
<span class="lineNum">    1428 </span>            :         preempt_enable();
<span class="lineNum">    1429 </span>            : }
<span class="lineNum">    1430 </span>            : 
<span class="lineNum">    1431 </span>            : static inline void set_io_start_time_ns(struct request *req)
<span class="lineNum">    1432 </span>            : {
<span class="lineNum">    1433 </span>            :         preempt_disable();
<span class="lineNum">    1434 </span>            :         req-&gt;io_start_time_ns = sched_clock();
<span class="lineNum">    1435 </span>            :         preempt_enable();
<span class="lineNum">    1436 </span>            : }
<span class="lineNum">    1437 </span>            : 
<span class="lineNum">    1438 </span>            : static inline uint64_t rq_start_time_ns(struct request *req)
<span class="lineNum">    1439 </span>            : {
<span class="lineNum">    1440 </span>            :         return req-&gt;start_time_ns;
<span class="lineNum">    1441 </span>            : }
<span class="lineNum">    1442 </span>            : 
<span class="lineNum">    1443 </span>            : static inline uint64_t rq_io_start_time_ns(struct request *req)
<span class="lineNum">    1444 </span>            : {
<span class="lineNum">    1445 </span>            :         return req-&gt;io_start_time_ns;
<span class="lineNum">    1446 </span>            : }
<span class="lineNum">    1447 </span>            : #else
<span class="lineNum">    1448 </span>            : static inline void set_start_time_ns(struct request *req) {}
<span class="lineNum">    1449 </span>            : static inline void set_io_start_time_ns(struct request *req) {}
<span class="lineNum">    1450 </span>            : static inline uint64_t rq_start_time_ns(struct request *req)
<span class="lineNum">    1451 </span>            : {
<span class="lineNum">    1452 </span>            :         return 0;
<span class="lineNum">    1453 </span>            : }
<span class="lineNum">    1454 </span>            : static inline uint64_t rq_io_start_time_ns(struct request *req)
<span class="lineNum">    1455 </span>            : {
<span class="lineNum">    1456 </span>            :         return 0;
<span class="lineNum">    1457 </span>            : }
<span class="lineNum">    1458 </span>            : #endif
<span class="lineNum">    1459 </span>            : 
<span class="lineNum">    1460 </span>            : #define MODULE_ALIAS_BLOCKDEV(major,minor) \
<span class="lineNum">    1461 </span>            :         MODULE_ALIAS(&quot;block-major-&quot; __stringify(major) &quot;-&quot; __stringify(minor))
<span class="lineNum">    1462 </span>            : #define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \
<span class="lineNum">    1463 </span>            :         MODULE_ALIAS(&quot;block-major-&quot; __stringify(major) &quot;-*&quot;)
<span class="lineNum">    1464 </span>            : 
<span class="lineNum">    1465 </span>            : #if defined(CONFIG_BLK_DEV_INTEGRITY)
<span class="lineNum">    1466 </span>            : 
<span class="lineNum">    1467 </span>            : #define INTEGRITY_FLAG_READ     2       /* verify data integrity on read */
<span class="lineNum">    1468 </span>            : #define INTEGRITY_FLAG_WRITE    4       /* generate data integrity on write */
<span class="lineNum">    1469 </span>            : 
<span class="lineNum">    1470 </span>            : struct blk_integrity_exchg {
<span class="lineNum">    1471 </span>            :         void                    *prot_buf;
<span class="lineNum">    1472 </span>            :         void                    *data_buf;
<span class="lineNum">    1473 </span>            :         sector_t                sector;
<span class="lineNum">    1474 </span>            :         unsigned int            data_size;
<span class="lineNum">    1475 </span>            :         unsigned short          sector_size;
<span class="lineNum">    1476 </span>            :         const char              *disk_name;
<span class="lineNum">    1477 </span>            : };
<span class="lineNum">    1478 </span>            : 
<span class="lineNum">    1479 </span>            : typedef void (integrity_gen_fn) (struct blk_integrity_exchg *);
<span class="lineNum">    1480 </span>            : typedef int (integrity_vrfy_fn) (struct blk_integrity_exchg *);
<span class="lineNum">    1481 </span>            : typedef void (integrity_set_tag_fn) (void *, void *, unsigned int);
<span class="lineNum">    1482 </span>            : typedef void (integrity_get_tag_fn) (void *, void *, unsigned int);
<span class="lineNum">    1483 </span>            : 
<span class="lineNum">    1484 </span>            : struct blk_integrity {
<span class="lineNum">    1485 </span>            :         integrity_gen_fn        *generate_fn;
<span class="lineNum">    1486 </span>            :         integrity_vrfy_fn       *verify_fn;
<span class="lineNum">    1487 </span>            :         integrity_set_tag_fn    *set_tag_fn;
<span class="lineNum">    1488 </span>            :         integrity_get_tag_fn    *get_tag_fn;
<span class="lineNum">    1489 </span>            : 
<span class="lineNum">    1490 </span>            :         unsigned short          flags;
<span class="lineNum">    1491 </span>            :         unsigned short          tuple_size;
<span class="lineNum">    1492 </span>            :         unsigned short          sector_size;
<span class="lineNum">    1493 </span>            :         unsigned short          tag_size;
<span class="lineNum">    1494 </span>            : 
<span class="lineNum">    1495 </span>            :         const char              *name;
<span class="lineNum">    1496 </span>            : 
<span class="lineNum">    1497 </span>            :         struct kobject          kobj;
<span class="lineNum">    1498 </span>            : };
<span class="lineNum">    1499 </span>            : 
<span class="lineNum">    1500 </span>            : extern bool blk_integrity_is_initialized(struct gendisk *);
<span class="lineNum">    1501 </span>            : extern int blk_integrity_register(struct gendisk *, struct blk_integrity *);
<span class="lineNum">    1502 </span>            : extern void blk_integrity_unregister(struct gendisk *);
<span class="lineNum">    1503 </span>            : extern int blk_integrity_compare(struct gendisk *, struct gendisk *);
<span class="lineNum">    1504 </span>            : extern int blk_rq_map_integrity_sg(struct request_queue *, struct bio *,
<span class="lineNum">    1505 </span>            :                                    struct scatterlist *);
<span class="lineNum">    1506 </span>            : extern int blk_rq_count_integrity_sg(struct request_queue *, struct bio *);
<span class="lineNum">    1507 </span>            : extern int blk_integrity_merge_rq(struct request_queue *, struct request *,
<span class="lineNum">    1508 </span>            :                                   struct request *);
<span class="lineNum">    1509 </span>            : extern int blk_integrity_merge_bio(struct request_queue *, struct request *,
<span class="lineNum">    1510 </span>            :                                    struct bio *);
<span class="lineNum">    1511 </span>            : 
<span class="lineNum">    1512 </span>            : static inline
<span class="lineNum">    1513 </span>            : struct blk_integrity *bdev_get_integrity(struct block_device *bdev)
<span class="lineNum">    1514 </span>            : {
<span class="lineNum">    1515 </span>            :         return bdev-&gt;bd_disk-&gt;integrity;
<span class="lineNum">    1516 </span>            : }
<span class="lineNum">    1517 </span>            : 
<span class="lineNum">    1518 </span>            : static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
<span class="lineNum">    1519 </span>            : {
<span class="lineNum">    1520 </span>            :         return disk-&gt;integrity;
<span class="lineNum">    1521 </span>            : }
<span class="lineNum">    1522 </span>            : 
<span class="lineNum">    1523 </span>            : static inline int blk_integrity_rq(struct request *rq)
<span class="lineNum">    1524 </span>            : {
<span class="lineNum">    1525 </span>            :         if (rq-&gt;bio == NULL)
<span class="lineNum">    1526 </span>            :                 return 0;
<span class="lineNum">    1527 </span>            : 
<span class="lineNum">    1528 </span>            :         return bio_integrity(rq-&gt;bio);
<span class="lineNum">    1529 </span>            : }
<span class="lineNum">    1530 </span>            : 
<span class="lineNum">    1531 </span>            : static inline void blk_queue_max_integrity_segments(struct request_queue *q,
<span class="lineNum">    1532 </span>            :                                                     unsigned int segs)
<span class="lineNum">    1533 </span>            : {
<span class="lineNum">    1534 </span>            :         q-&gt;limits.max_integrity_segments = segs;
<span class="lineNum">    1535 </span>            : }
<span class="lineNum">    1536 </span>            : 
<span class="lineNum">    1537 </span>            : static inline unsigned short
<span class="lineNum">    1538 </span>            : queue_max_integrity_segments(struct request_queue *q)
<span class="lineNum">    1539 </span>            : {
<span class="lineNum">    1540 </span>            :         return q-&gt;limits.max_integrity_segments;
<span class="lineNum">    1541 </span>            : }
<span class="lineNum">    1542 </span>            : 
<span class="lineNum">    1543 </span>            : #else /* CONFIG_BLK_DEV_INTEGRITY */
<span class="lineNum">    1544 </span>            : 
<span class="lineNum">    1545 </span>            : struct bio;
<span class="lineNum">    1546 </span>            : struct block_device;
<span class="lineNum">    1547 </span>            : struct gendisk;
<span class="lineNum">    1548 </span>            : struct blk_integrity;
<span class="lineNum">    1549 </span>            : 
<span class="lineNum">    1550 </span>            : static inline int blk_integrity_rq(struct request *rq)
<span class="lineNum">    1551 </span>            : {
<span class="lineNum">    1552 </span>            :         return 0;
<span class="lineNum">    1553 </span>            : }
<span class="lineNum">    1554 </span>            : static inline int blk_rq_count_integrity_sg(struct request_queue *q,
<span class="lineNum">    1555 </span>            :                                             struct bio *b)
<span class="lineNum">    1556 </span>            : {
<span class="lineNum">    1557 </span>            :         return 0;
<span class="lineNum">    1558 </span>            : }
<span class="lineNum">    1559 </span>            : static inline int blk_rq_map_integrity_sg(struct request_queue *q,
<span class="lineNum">    1560 </span>            :                                           struct bio *b,
<span class="lineNum">    1561 </span>            :                                           struct scatterlist *s)
<span class="lineNum">    1562 </span>            : {
<span class="lineNum">    1563 </span>            :         return 0;
<span class="lineNum">    1564 </span>            : }
<span class="lineNum">    1565 </span>            : static inline struct blk_integrity *bdev_get_integrity(struct block_device *b)
<span class="lineNum">    1566 </span>            : {
<span class="lineNum">    1567 </span>            :         return 0;
<span class="lineNum">    1568 </span>            : }
<span class="lineNum">    1569 </span>            : static inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)
<span class="lineNum">    1570 </span>            : {
<span class="lineNum">    1571 </span>            :         return NULL;
<span class="lineNum">    1572 </span>            : }
<span class="lineNum">    1573 </span>            : static inline int blk_integrity_compare(struct gendisk *a, struct gendisk *b)
<span class="lineNum">    1574 </span>            : {
<span class="lineNum">    1575 </span>            :         return 0;
<span class="lineNum">    1576 </span>            : }
<span class="lineNum">    1577 </span>            : static inline int blk_integrity_register(struct gendisk *d,
<span class="lineNum">    1578 </span>            :                                          struct blk_integrity *b)
<span class="lineNum">    1579 </span>            : {
<span class="lineNum">    1580 </span>            :         return 0;
<span class="lineNum">    1581 </span>            : }
<span class="lineNum">    1582 </span>            : static inline void blk_integrity_unregister(struct gendisk *d)
<span class="lineNum">    1583 </span>            : {
<span class="lineNum">    1584 </span>            : }
<span class="lineNum">    1585 </span>            : static inline void blk_queue_max_integrity_segments(struct request_queue *q,
<span class="lineNum">    1586 </span>            :                                                     unsigned int segs)
<span class="lineNum">    1587 </span>            : {
<span class="lineNum">    1588 </span>            : }
<span class="lineNum">    1589 </span>            : static inline unsigned short queue_max_integrity_segments(struct request_queue *q)
<span class="lineNum">    1590 </span>            : {
<span class="lineNum">    1591 </span>            :         return 0;
<span class="lineNum">    1592 </span>            : }
<span class="lineNum">    1593 </span>            : static inline int blk_integrity_merge_rq(struct request_queue *rq,
<span class="lineNum">    1594 </span>            :                                          struct request *r1,
<span class="lineNum">    1595 </span>            :                                          struct request *r2)
<span class="lineNum">    1596 </span>            : {
<span class="lineNum">    1597 </span>            :         return 0;
<span class="lineNum">    1598 </span>            : }
<span class="lineNum">    1599 </span>            : static inline int blk_integrity_merge_bio(struct request_queue *rq,
<span class="lineNum">    1600 </span>            :                                           struct request *r,
<span class="lineNum">    1601 </span>            :                                           struct bio *b)
<span class="lineNum">    1602 </span>            : {
<span class="lineNum">    1603 </span>            :         return 0;
<span class="lineNum">    1604 </span>            : }
<span class="lineNum">    1605 </span>            : static inline bool blk_integrity_is_initialized(struct gendisk *g)
<span class="lineNum">    1606 </span>            : {
<span class="lineNum">    1607 </span>            :         return 0;
<span class="lineNum">    1608 </span>            : }
<span class="lineNum">    1609 </span>            : 
<span class="lineNum">    1610 </span>            : #endif /* CONFIG_BLK_DEV_INTEGRITY */
<span class="lineNum">    1611 </span>            : 
<span class="lineNum">    1612 </span>            : struct block_device_operations {
<span class="lineNum">    1613 </span>            :         int (*open) (struct block_device *, fmode_t);
<span class="lineNum">    1614 </span>            :         void (*release) (struct gendisk *, fmode_t);
<span class="lineNum">    1615 </span>            :         int (*rw_page)(struct block_device *, sector_t, struct page *, int rw);
<span class="lineNum">    1616 </span>            :         int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
<span class="lineNum">    1617 </span>            :         int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
<span class="lineNum">    1618 </span>            :         int (*direct_access) (struct block_device *, sector_t,
<span class="lineNum">    1619 </span>            :                                                 void **, unsigned long *);
<span class="lineNum">    1620 </span>            :         unsigned int (*check_events) (struct gendisk *disk,
<span class="lineNum">    1621 </span>            :                                       unsigned int clearing);
<span class="lineNum">    1622 </span>            :         /* -&gt;media_changed() is DEPRECATED, use -&gt;check_events() instead */
<span class="lineNum">    1623 </span>            :         int (*media_changed) (struct gendisk *);
<span class="lineNum">    1624 </span>            :         void (*unlock_native_capacity) (struct gendisk *);
<span class="lineNum">    1625 </span>            :         int (*revalidate_disk) (struct gendisk *);
<span class="lineNum">    1626 </span>            :         int (*getgeo)(struct block_device *, struct hd_geometry *);
<span class="lineNum">    1627 </span>            :         /* this callback is with swap_lock and sometimes page table lock held */
<span class="lineNum">    1628 </span>            :         void (*swap_slot_free_notify) (struct block_device *, unsigned long);
<span class="lineNum">    1629 </span>            :         struct module *owner;
<span class="lineNum">    1630 </span>            : };
<span class="lineNum">    1631 </span>            : 
<span class="lineNum">    1632 </span>            : extern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,
<span class="lineNum">    1633 </span>            :                                  unsigned long);
<span class="lineNum">    1634 </span>            : extern int bdev_read_page(struct block_device *, sector_t, struct page *);
<span class="lineNum">    1635 </span>            : extern int bdev_write_page(struct block_device *, sector_t, struct page *,
<span class="lineNum">    1636 </span>            :                                                 struct writeback_control *);
<span class="lineNum">    1637 </span>            : #else /* CONFIG_BLOCK */
<span class="lineNum">    1638 </span>            : 
<span class="lineNum">    1639 </span>            : struct block_device;
<span class="lineNum">    1640 </span>            : 
<span class="lineNum">    1641 </span>            : /*
<span class="lineNum">    1642 </span>            :  * stubs for when the block layer is configured out
<span class="lineNum">    1643 </span>            :  */
<span class="lineNum">    1644 </span>            : #define buffer_heads_over_limit 0
<span class="lineNum">    1645 </span>            : 
<span class="lineNum">    1646 </span>            : static inline long nr_blockdev_pages(void)
<span class="lineNum">    1647 </span>            : {
<span class="lineNum">    1648 </span>            :         return 0;
<span class="lineNum">    1649 </span>            : }
<span class="lineNum">    1650 </span>            : 
<span class="lineNum">    1651 </span>            : struct blk_plug {
<span class="lineNum">    1652 </span>            : };
<span class="lineNum">    1653 </span>            : 
<span class="lineNum">    1654 </span>            : static inline void blk_start_plug(struct blk_plug *plug)
<span class="lineNum">    1655 </span>            : {
<span class="lineNum">    1656 </span>            : }
<span class="lineNum">    1657 </span>            : 
<span class="lineNum">    1658 </span>            : static inline void blk_finish_plug(struct blk_plug *plug)
<span class="lineNum">    1659 </span>            : {
<span class="lineNum">    1660 </span>            : }
<span class="lineNum">    1661 </span>            : 
<span class="lineNum">    1662 </span>            : static inline void blk_flush_plug(struct task_struct *task)
<span class="lineNum">    1663 </span>            : {
<span class="lineNum">    1664 </span>            : }
<span class="lineNum">    1665 </span>            : 
<span class="lineNum">    1666 </span>            : static inline void blk_schedule_flush_plug(struct task_struct *task)
<span class="lineNum">    1667 </span>            : {
<span class="lineNum">    1668 </span>            : }
<span class="lineNum">    1669 </span>            : 
<span class="lineNum">    1670 </span>            : 
<span class="lineNum">    1671 </span>            : static inline bool blk_needs_flush_plug(struct task_struct *tsk)
<span class="lineNum">    1672 </span>            : {
<span class="lineNum">    1673 </span>            :         return false;
<span class="lineNum">    1674 </span>            : }
<span class="lineNum">    1675 </span>            : 
<span class="lineNum">    1676 </span>            : static inline int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
<span class="lineNum">    1677 </span>            :                                      sector_t *error_sector)
<span class="lineNum">    1678 </span>            : {
<span class="lineNum">    1679 </span>            :         return 0;
<span class="lineNum">    1680 </span>            : }
<span class="lineNum">    1681 </span>            : 
<span class="lineNum">    1682 </span>            : #endif /* CONFIG_BLOCK */
<span class="lineNum">    1683 </span>            : 
<span class="lineNum">    1684 </span>            : #endif
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.10</a></td></tr>
  </table>
  <br>

</body>
</html>
